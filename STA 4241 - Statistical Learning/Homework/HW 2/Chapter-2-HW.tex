% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Chapter 2 HW},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Chapter 2 HW}
\author{}
\date{\vspace{-2.5em}}

\begin{document}
\maketitle

Load packages.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{options}\NormalTok{(}\AttributeTok{repos =} \FunctionTok{list}\NormalTok{(}\AttributeTok{CRAN=}\StringTok{"http://cran.rstudio.com/"}\NormalTok{))}
\NormalTok{packages }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}caret\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}ggplot2\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}MASS\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}plyr\textquotesingle{}}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(packages)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Installing packages into 'D:/Users/qcaij/OneDrive - University of Florida/DESKTOP-R7MUAPV/DATA/Users/qcaij/Documents/R/win-library/4.1'
## (as 'lib' is unspecified)
\end{verbatim}

\begin{verbatim}
## package 'caret' successfully unpacked and MD5 sums checked
\end{verbatim}

\begin{verbatim}
## Warning: cannot remove prior installation of package 'caret'
\end{verbatim}

\begin{verbatim}
## Warning in file.copy(savedcopy, lib, recursive = TRUE):
## problem copying D:\Users\qcaij\OneDrive - University of
## Florida\DESKTOP-R7MUAPV\DATA\Users\qcaij\Documents\R\win-
## library\4.1\00LOCK\caret\libs\x64\caret.dll to D:\Users\qcaij\OneDrive -
## University of Florida\DESKTOP-R7MUAPV\DATA\Users\qcaij\Documents\R\win-
## library\4.1\caret\libs\x64\caret.dll: Permission denied
\end{verbatim}

\begin{verbatim}
## Warning: restored 'caret'
\end{verbatim}

\begin{verbatim}
## package 'ggplot2' successfully unpacked and MD5 sums checked
## package 'MASS' successfully unpacked and MD5 sums checked
\end{verbatim}

\begin{verbatim}
## Warning: cannot remove prior installation of package 'MASS'
\end{verbatim}

\begin{verbatim}
## Warning in file.copy(savedcopy, lib, recursive = TRUE):
## problem copying D:\Users\qcaij\OneDrive - University of
## Florida\DESKTOP-R7MUAPV\DATA\Users\qcaij\Documents\R\win-
## library\4.1\00LOCK\MASS\libs\x64\MASS.dll to D:\Users\qcaij\OneDrive -
## University of Florida\DESKTOP-R7MUAPV\DATA\Users\qcaij\Documents\R\win-
## library\4.1\MASS\libs\x64\MASS.dll: Permission denied
\end{verbatim}

\begin{verbatim}
## Warning: restored 'MASS'
\end{verbatim}

\begin{verbatim}
## package 'plyr' successfully unpacked and MD5 sums checked
\end{verbatim}

\begin{verbatim}
## Warning: cannot remove prior installation of package 'plyr'
\end{verbatim}

\begin{verbatim}
## Warning in file.copy(savedcopy, lib, recursive = TRUE):
## problem copying D:\Users\qcaij\OneDrive - University of
## Florida\DESKTOP-R7MUAPV\DATA\Users\qcaij\Documents\R\win-
## library\4.1\00LOCK\plyr\libs\x64\plyr.dll to D:\Users\qcaij\OneDrive -
## University of Florida\DESKTOP-R7MUAPV\DATA\Users\qcaij\Documents\R\win-
## library\4.1\plyr\libs\x64\plyr.dll: Permission denied
\end{verbatim}

\begin{verbatim}
## Warning: restored 'plyr'
\end{verbatim}

\begin{verbatim}
## 
## The downloaded binary packages are in
##  C:\Users\qcaij\AppData\Local\Temp\RtmpqcoU3v\downloaded_packages
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lapply}\NormalTok{(}\AttributeTok{X =}\NormalTok{ packages, }\AttributeTok{FUN =}\NormalTok{ library, }\AttributeTok{character.only =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'caret' was built under R version 4.1.1
\end{verbatim}

\begin{verbatim}
## Loading required package: ggplot2
\end{verbatim}

\begin{verbatim}
## Warning: package 'ggplot2' was built under R version 4.1.1
\end{verbatim}

\begin{verbatim}
## Loading required package: lattice
\end{verbatim}

\begin{verbatim}
## Warning: package 'MASS' was built under R version 4.1.1
\end{verbatim}

\begin{verbatim}
## Warning: package 'plyr' was built under R version 4.1.1
\end{verbatim}

\begin{verbatim}
## [[1]]
##  [1] "caret"     "lattice"   "ggplot2"   "stats"     "graphics"  "grDevices"
##  [7] "utils"     "datasets"  "methods"   "base"     
## 
## [[2]]
##  [1] "caret"     "lattice"   "ggplot2"   "stats"     "graphics"  "grDevices"
##  [7] "utils"     "datasets"  "methods"   "base"     
## 
## [[3]]
##  [1] "MASS"      "caret"     "lattice"   "ggplot2"   "stats"     "graphics" 
##  [7] "grDevices" "utils"     "datasets"  "methods"   "base"     
## 
## [[4]]
##  [1] "plyr"      "MASS"      "caret"     "lattice"   "ggplot2"   "stats"    
##  [7] "graphics"  "grDevices" "utils"     "datasets"  "methods"   "base"
\end{verbatim}

Question 2.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# loading data}
\NormalTok{prob2 }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file =} \StringTok{\textquotesingle{}data/Problem2.csv\textquotesingle{}}\NormalTok{)}

\CommentTok{\# squaring covariates}
\NormalTok{prob2}\SpecialCharTok{$}\NormalTok{X1.squared }\OtherTok{\textless{}{-}}\NormalTok{ prob2}\SpecialCharTok{$}\NormalTok{X1}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{prob2}\SpecialCharTok{$}\NormalTok{X2.squared }\OtherTok{\textless{}{-}}\NormalTok{ prob2}\SpecialCharTok{$}\NormalTok{X2}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{prob2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     Y          X1          X2   X1.squared   X2.squared
## 1   1 -0.84085548 -1.99538697 0.7070379396 3.9815691514
## 2   0  1.38435934  1.13531128 1.9164507919 1.2889317052
## 3   1 -1.25549186  0.67579457 1.5762598171 0.4566982951
## 4   0  0.07014277  0.20848326 0.0049200077 0.0434652710
## 5   1  1.71144087 -0.05784564 2.9290298608 0.0033461183
## 6   0 -0.60290798  0.89381141 0.3634980341 0.7988988440
## 7   1 -0.47216639 -0.22886538 0.2229410953 0.0523793625
## 8   1 -0.63537131 -1.96565265 0.4036967048 3.8637903390
## 9   0 -0.28577363 -0.75351045 0.0816665704 0.5677779920
## 10  1  0.13810822  1.28015162 0.0190738818 1.6387881816
## 11  1  1.22763034 -0.95290496 1.5070762611 0.9080278623
## 12  0 -0.80177945  1.62237939 0.6428502939 2.6321148949
## 13  0 -1.08039260  2.60014202 1.1672481702 6.7607385248
## 14  0 -0.15753436  0.13964851 0.0248170734 0.0195017050
## 15  1 -1.07176004 -1.35071967 1.1486695831 1.8244436354
## 16  0 -0.13898614  0.79893102 0.0193171473 0.6382907714
## 17  1 -0.59731309 -1.55499584 0.3567829331 2.4180120638
## 18  1 -2.18396676  0.46372006 4.7697108092 0.2150362911
## 19  1  0.24081726  0.05242956 0.0579929508 0.0027488592
## 20  1 -0.25935541 -0.20203180 0.0672652270 0.0408168485
## 21  0  0.90051195  1.17085642 0.8109217637 1.3709047610
## 22  0  0.94186939  0.88484486 0.8871179551 0.7829504184
## 23  1  1.46796190 -1.31788860 2.1549121499 1.7368303722
## 24  1  0.70676109 -1.64325094 0.4995112377 2.7002736374
## 25  0  0.81900893  1.05925039 0.6707756278 1.1220113829
## 26  0 -0.29348185  0.29008358 0.0861315955 0.0841484842
## 27  0  1.41858907 -0.40003350 2.0123949566 0.1600268002
## 28  0  1.49877383  1.24309578 2.2463229857 1.5452871134
## 29  0 -0.65708209 -1.36641052 0.4317568789 1.8670777037
## 30  0 -0.85279544 -1.44141330 0.7272600625 2.0776723066
## 31  0  0.31591504  1.34854906 0.0998023115 1.8185845538
## 32  1  1.10969417 -1.97852834 1.2314211457 3.9145743910
## 33  1  2.21546057 -1.24095058 4.9082655447 1.5399583520
## 34  0  1.21710364 -0.10403913 1.4813412680 0.0108241401
## 35  0  1.47922179  0.73297296 2.1880970941 0.5372493581
## 36  1  0.95157383  0.45567962 0.9054927585 0.2076439191
## 37  1 -1.00953265  0.28807955 1.0191561633 0.0829898257
## 38  0 -2.00047274 -1.07369091 4.0018911780 1.1528121717
## 39  0 -1.76218587  0.64874254 3.1052990491 0.4208668825
## 40  1 -0.14260813  0.29916228 0.0203370776 0.0894980689
## 41  1  1.55006037 -0.79599499 2.4026871490 0.6336080289
## 42  0 -0.80242318 -0.02935340 0.6438829626 0.0008616219
## 43  0 -0.07457892  2.18023570 0.0055620153 4.7534277128
## 44  1  1.89566795  0.95741847 3.5935569946 0.9166501241
## 45  0 -0.45656894 -0.30504863 0.2084551978 0.0930546696
## 46  0  0.56222336 -0.41840334 0.3160951095 0.1750613540
## 47  1 -0.88700851  0.09995405 0.7867840995 0.0099908119
## 48  0 -0.46024458 -0.22980962 0.2118250699 0.0528124606
## 49  0 -0.72432849 -1.41521488 0.5246517557 2.0028331458
## 50  1 -0.06921116 -0.39259886 0.0047901841 0.1541338667
## 51  0  1.46324856  0.94608855 2.1410963570 0.8950835443
## 52  0  0.18772610  0.75177087 0.0352410877 0.5651594442
## 53  1  1.02202286 -0.51737685 1.0445307291 0.2676788034
## 54  0 -0.59183483  0.80833598 0.3502684695 0.6534070540
## 55  0 -0.11220066 -0.61453522 0.0125889870 0.3776535407
## 56  0 -0.92495309  1.23825893 0.8555382111 1.5332851734
## 57  0  0.75330480 -0.33809514 0.5674681191 0.1143083254
## 58  0 -0.11260907  1.19636636 0.0126808027 1.4312924747
## 59  1 -0.06409093 -0.44331838 0.0041076471 0.1965311848
## 60  0  0.23327529  0.18611490 0.0544173626 0.0346387550
## 61  0 -1.13658280 -2.62134481 1.2918204684 6.8714486268
## 62  0  0.85483042  2.24625462 0.7307350523 5.0456598172
## 63  1 -0.57837042  0.09343168 0.3345123415 0.0087294790
## 64  0  0.49636154  1.62728009 0.2463747774 2.6480405023
## 65  1 -0.76005793 -0.51091755 0.5776880579 0.2610367420
## 66  1 -0.34138627 -0.65938084 0.1165445856 0.4347830891
## 67  0 -2.10232912 -0.04019016 4.4197877308 0.0016152490
## 68  0 -0.30170228 -0.11869400 0.0910242666 0.0140882661
## 69  0 -1.27238344 -0.01965686 1.6189596239 0.0003863923
## 70  0 -0.27966611 -0.48567849 0.0782131330 0.2358835914
## 71  1 -0.20409732 -1.44014752 0.0416557164 2.0740248917
## 72  0 -0.22561419  0.14376888 0.0509017607 0.0206694902
## 73  1  0.34702845 -1.23458665 0.1204287465 1.5242042069
## 74  1  0.03236784 -1.75250121 0.0010476772 3.0712604908
## 75  1  0.41353129 -0.03549629 0.1710081275 0.0012599864
## 76  0 -0.15534848  0.33203491 0.0241331492 0.1102471805
## 77  0  0.97348539  1.57228826 0.9476738094 2.4720903610
## 78  1  0.12109014 -1.06947057 0.0146628227 1.1437673052
## 79  0  0.18917369  0.91628652 0.0357866855 0.8395809934
## 80  1 -0.56288507 -0.59499290 0.3168396018 0.3540165466
## 81  0  0.49841617  2.18164668 0.2484186735 4.7595822158
## 82  0 -1.74230249 -0.68377329 3.0356179784 0.4675459071
## 83  0  0.97552910  0.75005921 0.9516570195 0.5625888242
## 84  1 -0.02408287  0.97438263 0.0005799848 0.9494215172
## 85  1  0.67568448 -1.26447348 0.4565495102 1.5988931707
## 86  0 -0.71030961 -0.27742142 0.5045397350 0.0769626463
## 87  1  2.38723265 -0.18939869 5.6988797079 0.0358718656
## 88  1 -0.47343201 -0.38402495 0.2241378702 0.1474751592
## 89  1 -0.07577256  0.74058802 0.0057414803 0.5484706201
## 90  0 -0.52184006 -1.16833839 0.2723170445 1.3650145962
## 91  0  0.92604713  0.66753870 0.8575632954 0.4456079135
## 92  1 -1.06241117  0.36623695 1.1287174976 0.1341295033
## 93  1  0.55703387 -0.51494299 0.3102867282 0.2651662870
## 94  1  0.90073058  0.45056824 0.8113155866 0.2030117388
## 95  1  0.98994568 -0.18772038 0.9799924567 0.0352389410
## 96  0  0.38360809  1.33906937 0.1471551649 1.7931067907
## 97  1 -0.34658381  0.81621918 0.1201203399 0.6662137568
## 98  1 -0.54018925  0.08220176 0.2918044258 0.0067571300
## 99  1 -0.18255559 -0.65086272 0.0333265446 0.4236222821
## 100 0 -0.05929965  0.72640902 0.0035164485 0.5276700609
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# creating models}
\NormalTok{logistic.linear }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X1 }\SpecialCharTok{+}\NormalTok{ X2, }\AttributeTok{data =}\NormalTok{ prob2, }\AttributeTok{family =}\NormalTok{ binomial)}
\NormalTok{logistic.squared }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X1.squared }\SpecialCharTok{+}\NormalTok{ X2.squared, }\AttributeTok{data =}\NormalTok{ prob2, }\AttributeTok{family =}\NormalTok{ binomial)}
\NormalTok{lda.model }\OtherTok{\textless{}{-}} \FunctionTok{lda}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X1 }\SpecialCharTok{+}\NormalTok{ X2, }\AttributeTok{data =}\NormalTok{ prob2)}
\NormalTok{qda.model }\OtherTok{\textless{}{-}} \FunctionTok{qda}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X1 }\SpecialCharTok{+}\NormalTok{ X2, }\AttributeTok{data =}\NormalTok{ prob2)}

\CommentTok{\# printing model objects}
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{logistic.linear:}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## logistic.linear:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logistic.linear}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:  glm(formula = Y ~ X1 + X2, family = binomial, data = prob2)
## 
## Coefficients:
## (Intercept)           X1           X2  
##     -0.1859       0.3725      -0.9107  
## 
## Degrees of Freedom: 99 Total (i.e. Null);  97 Residual
## Null Deviance:       138 
## Residual Deviance: 119.9     AIC: 125.9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{logistic.squared:}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## logistic.squared:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logistic.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:  glm(formula = Y ~ X1.squared + X2.squared, family = binomial, 
##     data = prob2)
## 
## Coefficients:
## (Intercept)   X1.squared   X2.squared  
##    -0.05907      0.08636     -0.16821  
## 
## Degrees of Freedom: 99 Total (i.e. Null);  97 Residual
## Null Deviance:       138 
## Residual Deviance: 136.4     AIC: 142.4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{lda.model:}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## lda.model:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lda.model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## lda(Y ~ X1 + X2, data = prob2)
## 
## Prior probabilities of groups:
##    0    1 
## 0.54 0.46 
## 
## Group means:
##            X1         X2
## 0 -0.05292198  0.3830047
## 1  0.13089757 -0.4137219
## 
## Coefficients of linear discriminants:
##           LD1
## X1  0.4109256
## X2 -1.0188356
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{qda.model:}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## qda.model:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{qda.model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## qda(Y ~ X1 + X2, data = prob2)
## 
## Prior probabilities of groups:
##    0    1 
## 0.54 0.46 
## 
## Group means:
##            X1         X2
## 0 -0.05292198  0.3830047
## 1  0.13089757 -0.4137219
\end{verbatim}

Summaries of models.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{Logistic regression with linear covariates:}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Logistic regression with linear covariates:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\AttributeTok{object =}\NormalTok{ logistic.linear)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = Y ~ X1 + X2, family = binomial, data = prob2)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9667  -0.9771  -0.5188   1.0425   1.8100  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)    
## (Intercept)  -0.1859     0.2204  -0.844 0.398921    
## X1            0.3725     0.2438   1.528 0.126511    
## X2           -0.9107     0.2462  -3.699 0.000217 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 137.99  on 99  degrees of freedom
## Residual deviance: 119.91  on 97  degrees of freedom
## AIC: 125.91
## 
## Number of Fisher Scoring iterations: 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{Logistic regression with squared covariates:}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Logistic regression with squared covariates:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\AttributeTok{object =}\NormalTok{ logistic.squared)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = Y ~ X1.squared + X2.squared, family = binomial, 
##     data = prob2)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.3170  -1.1328  -0.8467   1.2035   1.4737  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)
## (Intercept) -0.05907    0.29926  -0.197    0.844
## X1.squared   0.08636    0.16938   0.510    0.610
## X2.squared  -0.16821    0.15196  -1.107    0.268
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 137.99  on 99  degrees of freedom
## Residual deviance: 136.37  on 97  degrees of freedom
## AIC: 142.37
## 
## Number of Fisher Scoring iterations: 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{LDA:}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## LDA:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\AttributeTok{object =}\NormalTok{ lda.model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         Length Class  Mode     
## prior   2      -none- numeric  
## counts  2      -none- numeric  
## means   4      -none- numeric  
## scaling 2      -none- numeric  
## lev     2      -none- character
## svd     1      -none- numeric  
## N       1      -none- numeric  
## call    3      -none- call     
## terms   3      terms  call     
## xlevels 0      -none- list
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{QDA:}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## QDA:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\AttributeTok{object =}\NormalTok{ qda.model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         Length Class  Mode     
## prior   2      -none- numeric  
## counts  2      -none- numeric  
## means   4      -none- numeric  
## scaling 8      -none- numeric  
## ldet    2      -none- numeric  
## lev     2      -none- character
## N       1      -none- numeric  
## call    3      -none- call     
## terms   3      terms  call     
## xlevels 0      -none- list
\end{verbatim}

Question 2i

Setup for plotting predicted outcomes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# spacing}
\NormalTok{linspace\_num }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}
\NormalTok{ticks }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}
\NormalTok{grid }\OtherTok{\textless{}{-}} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{X1 =}\NormalTok{ linspace\_num, }\AttributeTok{X2 =}\NormalTok{ linspace\_num)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logistic.linear.pred }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{*}\NormalTok{ (}\FunctionTok{predict}\NormalTok{(logistic.linear, grid, }\AttributeTok{type =} \StringTok{\textquotesingle{}response\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\textgreater{}} \FloatTok{0.5}\NormalTok{)}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ grid, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ X1, }\AttributeTok{y =}\NormalTok{ X2, }\AttributeTok{color =} \FunctionTok{as.factor}\NormalTok{(logistic.linear.pred))) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =}\NormalTok{ ticks) }\SpecialCharTok{+} 
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{breaks =}\NormalTok{ ticks) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\AttributeTok{label =} \StringTok{\textquotesingle{}Logistic Regression (Linear Covariates)\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}green\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{), }\AttributeTok{guide =} \StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Chapter-2-HW_files/figure-latex/unnamed-chunk-5-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grid}\FloatTok{.2} \OtherTok{\textless{}{-}} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{X1 =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{, }\FloatTok{0.1}\NormalTok{), }\AttributeTok{X2 =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\FloatTok{0.1}\NormalTok{))}
\NormalTok{logistic.squared.pred }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{*}\NormalTok{ (}\FunctionTok{predict}\NormalTok{(}\FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(X1}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(X2}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{), }\AttributeTok{data =}\NormalTok{ prob2, }\AttributeTok{family =}\NormalTok{ binomial), grid}\FloatTok{.2}\NormalTok{, }\AttributeTok{type =} \StringTok{\textquotesingle{}response\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\textgreater{}} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{{-}} \DecValTok{1}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ grid}\FloatTok{.2}\NormalTok{, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ X1, }\AttributeTok{y =}\NormalTok{ X2, }\AttributeTok{color =} \FunctionTok{as.factor}\NormalTok{(logistic.squared.pred))) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =}\NormalTok{ ticks) }\SpecialCharTok{+} 
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{breaks =}\NormalTok{ ticks) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\AttributeTok{label =} \StringTok{\textquotesingle{}Logistic Regression (Squared Covariates)\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}green\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{), }\AttributeTok{guide =} \StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Chapter-2-HW_files/figure-latex/unnamed-chunk-6-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lda.pred }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\AttributeTok{x =} \FunctionTok{predict}\NormalTok{(lda.model, grid)}\SpecialCharTok{$}\NormalTok{class) }\SpecialCharTok{{-}} \DecValTok{1}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ grid, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ X1, }\AttributeTok{y =}\NormalTok{ X2, }\AttributeTok{color =} \FunctionTok{as.factor}\NormalTok{(}\AttributeTok{x =}\NormalTok{ lda.pred))) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =}\NormalTok{ ticks) }\SpecialCharTok{+} 
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{breaks =}\NormalTok{ ticks) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\AttributeTok{label =} \StringTok{\textquotesingle{}LDA\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}green\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{), }\AttributeTok{guide =} \StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Chapter-2-HW_files/figure-latex/unnamed-chunk-7-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{qda.pred }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\AttributeTok{x =} \FunctionTok{predict}\NormalTok{(qda.model, grid)}\SpecialCharTok{$}\NormalTok{class)}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ grid, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ X1, }\AttributeTok{y =}\NormalTok{ X2, }\AttributeTok{color =} \FunctionTok{as.factor}\NormalTok{(qda.pred))) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =}\NormalTok{ ticks) }\SpecialCharTok{+} 
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{breaks =}\NormalTok{ ticks) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\AttributeTok{label =} \StringTok{\textquotesingle{}QDA\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}green\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{), }\AttributeTok{guide =} \StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Chapter-2-HW_files/figure-latex/unnamed-chunk-8-1.pdf}
Question 2ii

The first evident observation is the use of curved boundaries by
logistic regression with squared covariates and QDA. Interestingly, both
the logistic regression with linear terms and LDA seem to agree on the
same boundary. At this stage, it is unclear whether any model exhibits
overfitting just based on visual displays. The two models that use
nonlinear boundaries may be closer to overfitting than the other two
models simply due to curvature of boundaries wrapping around class data
points. Furthermore, QDA is prone to variance, meaning adding new data
points will be very sensitive to the positioning of boundaries.

Question 2iii

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load test dataset}
\NormalTok{prob2.test }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file =} \StringTok{\textquotesingle{}data/Problem2test.csv\textquotesingle{}}\NormalTok{)}
\NormalTok{prob2.test}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Y           X1           X2
## 1    0  1.210146094  0.508630538
## 2    0  0.188657164  0.260949990
## 3    1  1.962498672  1.102686956
## 4    0  0.138711939 -0.023540036
## 5    0 -1.578627354 -0.076654184
## 6    0 -0.797021271  0.809930609
## 7    1  1.224353882 -1.713937928
## 8    0 -0.365333560  1.975068096
## 9    0 -0.162590280 -0.867222960
## 10   1  0.560479179  1.466811326
## 11   0 -0.860725629 -0.278560025
## 12   1  1.238634492  1.803588650
## 13   0  0.767435749  0.288172230
## 14   0 -1.087409106 -0.259131700
## 15   1  0.067503773  0.415262649
## 16   1  1.605140565  0.276235382
## 17   0  1.232229430  0.716427946
## 18   1 -0.379113795 -0.379221662
## 19   0 -1.349866584  1.124960497
## 20   0  0.364918100  0.350475085
## 21   0 -0.363594925  0.185363916
## 22   0  1.374653325  0.673634343
## 23   1  0.291895737  0.716049040
## 24   1  0.710591710  0.941973899
## 25   1 -0.937760920 -2.145285774
## 26   0 -1.114063127  1.358675249
## 27   0  0.634371955  1.930555389
## 28   1 -0.231192902 -2.366205929
## 29   0 -1.368194204 -0.516412695
## 30   1 -0.754907446 -2.076942075
## 31   0 -1.125596649  1.035753928
## 32   1 -0.219359247 -0.506279133
## 33   0 -0.134307952  0.123793618
## 34   0 -0.818020611 -0.767850447
## 35   1  0.472334157  0.190999226
## 36   1 -0.869256130  0.283432350
## 37   0 -1.332288341  0.615096443
## 38   0  0.070562870  0.009051521
## 39   1  0.464093186 -1.865811875
## 40   1  0.289158499  0.314708871
## 41   0 -2.884941083 -0.237993971
## 42   0 -2.334691775 -0.963559475
## 43   1 -1.730891047  0.852140008
## 44   1  0.825009586 -0.653566616
## 45   0 -1.045039551  0.465528320
## 46   1 -0.877193374  1.273151063
## 47   1 -0.400389842 -0.930120589
## 48   0 -1.268188972  1.632379226
## 49   1  0.138586391  0.208970674
## 50   1  1.183571604 -0.668749226
## 51   0 -2.110555073 -1.492592001
## 52   1  0.260676110 -1.443982207
## 53   1  0.945566826  0.007750606
## 54   1 -0.619960620  1.189554014
## 55   0 -0.009100739  1.508630715
## 56   0  0.520225423  1.088447140
## 57   1  1.806258708  0.836477133
## 58   1 -1.912519898 -0.692637371
## 59   1  0.199282075  0.202121029
## 60   0  0.276484953  2.354131990
## 61   0 -0.836276427 -0.118000685
## 62   1  2.029597415  0.172908616
## 63   1  0.429105987 -0.581460287
## 64   1  1.063947866 -0.292173437
## 65   0 -0.605823053 -0.031352277
## 66   1  1.074373046  0.142689242
## 67   1 -0.712347640 -1.066501548
## 68   0 -0.178860133  0.198233494
## 69   1  0.499760039  0.572825722
## 70   0 -0.199571141  0.414215202
## 71   1  0.089894805 -0.525333970
## 72   0  1.004910058  0.481253090
## 73   1 -1.872941673 -3.353001479
## 74   0  0.524784536  0.774893597
## 75   1 -0.514627337 -0.918469978
## 76   1  1.210932570  0.596621535
## 77   0 -0.512795377  0.688705144
## 78   1  1.090625652  0.137121747
## 79   1  0.491844896  0.961974154
## 80   0 -0.242628972 -0.892975152
## 81   1  2.116047248 -1.541205679
## 82   0  1.188639573  0.606202786
## 83   0  0.895261191  2.030999409
## 84   1  0.868540637 -1.049324143
## 85   0  1.656177965 -0.140524362
## 86   1  1.445635992 -1.161760955
## 87   0  0.744600427 -1.090386511
## 88   0 -0.690190218  0.963635008
## 89   1 -0.791411761 -1.175166079
## 90   1 -0.262081191 -2.664690479
## 91   1 -0.407917108  0.105614016
## 92   0  0.201311056 -0.004874413
## 93   1 -0.731947483  0.552347713
## 94   1  0.729135717 -0.454247446
## 95   1  0.326648604 -0.293124351
## 96   0 -2.707877887  1.952503960
## 97   1 -0.592138910 -1.231156499
## 98   1  0.504543697 -1.446009644
## 99   0 -1.522697081  0.475657988
## 100  1 -0.358875198  0.062018035
## 101  0 -0.470126447  1.649233106
## 102  1  0.469322389 -1.697478259
## 103  0 -1.475340129  0.134364931
## 104  0 -0.915954714  1.667660043
## 105  1  0.457975057  0.409593760
## 106  0 -0.712502460  0.185181757
## 107  0 -0.378438329  0.760072066
## 108  0  2.006860933  0.498652820
## 109  0 -0.550168354 -0.219360918
## 110  0 -1.980786166  1.914846700
## 111  1  1.034598131 -0.236518020
## 112  0 -0.844839716  0.660723750
## 113  1 -1.016949117  0.070503275
## 114  0  0.674755829  0.745024642
## 115  1 -0.033226776  0.271449498
## 116  1  2.245491408 -0.041946815
## 117  0 -0.454146942 -0.131586505
## 118  1  0.194874583 -0.457089002
## 119  0  0.031498073 -0.048566356
## 120  0 -1.789330989 -0.324215951
## 121  1 -0.070490915  0.667192965
## 122  1  1.804141755 -0.145122520
## 123  0  0.444637125  1.278782702
## 124  1 -1.190940100  0.414307729
## 125  0  0.248229103  2.873133341
## 126  1 -0.118978160  0.073524920
## 127  0  0.686988980  2.069828503
## 128  1 -1.407777792  0.253816382
## 129  1  0.575322041 -0.849717459
## 130  1  0.637701530  0.949571973
## 131  0 -0.948123686  1.340665179
## 132  0 -1.515531972 -0.553127106
## 133  0  0.266538188  0.418923488
## 134  1  0.291532260 -1.568498520
## 135  0  1.708922438 -0.757474330
## 136  1  0.476701848 -1.045436349
## 137  1  1.307551804  0.176874048
## 138  1  1.806012686 -0.096858813
## 139  0  1.035385990  1.353019752
## 140  1  0.516518297 -0.462969939
## 141  1 -0.394103521  0.417196608
## 142  0 -1.510240230  1.325475491
## 143  1  0.005989837  0.924859134
## 144  1  0.855006991  0.350624573
## 145  1  0.729851453  0.997016937
## 146  0  0.348758873  1.481643255
## 147  1  1.361367470  0.035370462
## 148  1 -0.288662129 -0.812982660
## 149  0  1.888403664  0.619304906
## 150  0 -0.017375770 -0.223086733
## 151  1  0.541082912  0.838647604
## 152  0 -0.540041112  0.647781775
## 153  0 -0.080849396 -0.119997799
## 154  1 -0.029633958 -0.238070949
## 155  0 -0.145250484  0.706267091
## 156  1 -0.010862596 -0.639519514
## 157  0 -1.378718765  0.724004601
## 158  0  0.671535677  0.189735206
## 159  1 -1.154205453 -0.681391923
## 160  1  0.422597786  0.427678544
## 161  0 -0.386989726  0.141984264
## 162  0 -1.240525985  1.688949596
## 163  0  0.456962145  1.807531105
## 164  0 -1.410793353  0.059182818
## 165  1  1.933907528 -0.007233756
## 166  0  0.569560653 -0.698554574
## 167  0  0.932779752  0.422568257
## 168  1 -1.545347337 -0.081840567
## 169  1  0.601618386 -0.841075964
## 170  1 -0.950766156 -1.250590149
## 171  1  1.360365938 -0.330713780
## 172  1 -0.266523590 -2.564661071
## 173  0 -3.034945771 -1.141495171
## 174  1 -0.238819914 -0.461586801
## 175  0 -0.272186719  0.471210538
## 176  0  1.190396803  1.050471407
## 177  1  0.051196093  0.556810703
## 178  0  0.331048034 -0.257207098
## 179  1  0.531619097 -0.066234532
## 180  1  0.614031469  1.007486669
## 181  0  0.076715890  2.223533377
## 182  0 -0.260879256 -0.876482111
## 183  0 -0.921870406 -0.656145261
## 184  0  0.117606159  0.807712656
## 185  0 -2.138410219  0.150083946
## 186  1 -0.319419595 -0.068531365
## 187  0 -0.740694413 -0.082166561
## 188  0 -0.925791427  0.581116512
## 189  1  0.757933789 -1.019987683
## 190  0  0.359560263  0.431353445
## 191  1 -0.829235675  0.434808368
## 192  0 -0.445752122 -0.110162106
## 193  0 -0.670912047  0.870229453
## 194  0 -0.800916902  0.343812289
## 195  1  1.610946699 -0.141178048
## 196  1  0.018518334 -0.104213136
## 197  0 -1.083399341  0.945030339
## 198  0  1.289449757 -0.090380203
## 199  0 -0.143912736  0.015595256
## 200  0 -2.288008838 -0.206724968
## 201  0 -0.084716818  1.554022103
## 202  0  0.199059827 -0.661741975
## 203  1  1.229825830  0.104093164
## 204  1  0.004552531 -0.856410620
## 205  1 -1.294607637 -1.330315345
## 206  1 -0.397161380 -1.597554281
## 207  1  1.739481988  0.524215129
## 208  0  1.023020955  0.887065030
## 209  0 -0.527567252  0.724333960
## 210  0  0.845996179  0.157465769
## 211  1  0.821549398 -1.472779004
## 212  0 -1.335769387  2.279989974
## 213  0  1.043118044  1.637916390
## 214  0 -1.122990920 -0.275630691
## 215  0  0.690923293 -0.940113326
## 216  0  0.368605572  1.029474677
## 217  1  0.648513362  0.257033937
## 218  0 -0.439830314  0.472016617
## 219  0 -0.866420855 -0.291185611
## 220  1  0.865300603  0.441616656
## 221  0 -0.509815203 -0.776092906
## 222  0 -0.758776324 -1.886963749
## 223  1  3.401872032 -0.723581692
## 224  0 -0.450415990 -0.382221060
## 225  1  1.729198147 -1.392873232
## 226  0 -0.718488291  1.991607540
## 227  0  0.533619596  0.739821576
## 228  0 -0.070186932  0.009938563
## 229  0 -1.550563512  1.986069781
## 230  0  0.273427523  0.859987159
## 231  1  1.885234977  0.455585832
## 232  0  0.843198997  0.647142042
## 233  1  0.334533920  1.146357837
## 234  1  0.020385466 -1.331540242
## 235  0 -1.007291538  0.498854140
## 236  1  0.412693957  0.087379596
## 237  1 -0.767032339 -0.316633119
## 238  1  0.924991878 -0.174084418
## 239  0  0.309149448 -0.367704902
## 240  1  1.966845363 -0.029638860
## 241  0 -0.937537800 -0.407802066
## 242  1  0.420473976  0.348674492
## 243  1 -0.539202237  0.174400595
## 244  1  0.868593878  0.043728212
## 245  1  1.150157519  0.266735961
## 246  1  0.017696000 -0.054640685
## 247  1 -0.927844335 -0.985521072
## 248  0 -0.505731945  0.149978204
## 249  1  1.225733179  0.790481312
## 250  1  1.210377729 -0.810981933
## 251  0 -1.698273701 -0.277590550
## 252  1  0.094696370  0.016570946
## 253  0  0.762139340  0.570492061
## 254  1  2.367203589  0.035124778
## 255  1 -0.043702820 -0.173344764
## 256  0  0.067098045  1.199663939
## 257  1  0.155296921 -0.588189186
## 258  1 -0.532872886 -0.418259830
## 259  1 -1.828973544 -2.585112566
## 260  0  0.600554626  0.959071914
## 261  1  0.085340083 -0.241897949
## 262  0 -0.396573561  1.445139499
## 263  0 -1.617414314  2.275838639
## 264  0 -0.022905254  1.445887811
## 265  1  0.406281025 -1.484872620
## 266  0  0.509045870  1.156776234
## 267  1  1.586671077  2.755396562
## 268  1  1.046311776 -0.897281157
## 269  0 -1.047938308  1.032451378
## 270  0 -0.790600527 -0.985315416
## 271  0  0.467298191 -1.276372725
## 272  1  0.553826861 -0.829764724
## 273  1 -0.680978405 -2.209462978
## 274  0 -2.794959603  0.982818539
## 275  0 -1.084586126  1.503928380
## 276  0 -0.657097799  0.085675395
## 277  0 -0.969290748 -0.033931598
## 278  1  0.412763202 -1.861062896
## 279  0 -0.915410286  0.358931935
## 280  1 -0.448315999 -0.585992188
## 281  1 -0.798502012  0.018388841
## 282  1 -1.017287856 -0.513812710
## 283  1  1.014988922 -1.362798418
## 284  0 -0.651507685 -0.165079554
## 285  1  0.038215849  1.287398103
## 286  0  0.184078667  1.074861548
## 287  1  0.451722586  0.155539400
## 288  0  0.307880065  1.047669081
## 289  0  0.284858233 -0.407005503
## 290  1  0.664551039 -0.935437359
## 291  1 -0.330974788 -0.066807528
## 292  1  1.491598133 -0.004349923
## 293  0  0.172090210 -0.427797277
## 294  0  0.191160283  0.937585939
## 295  1 -0.279615008 -2.060012495
## 296  1  0.775019562 -0.423060064
## 297  1 -0.335845291 -1.269024734
## 298  1 -0.010529493 -0.525269728
## 299  0 -0.139135003  1.363412615
## 300  0 -0.053465947 -0.685394741
## 301  0  1.329496165 -0.684215613
## 302  1  0.989769380 -0.857690099
## 303  1  1.664565787  0.541591839
## 304  0 -2.119883648 -0.506126499
## 305  1  0.141480817 -1.453589208
## 306  0 -0.177806203  1.031680349
## 307  0  0.297883930  0.783754061
## 308  0  1.139433348 -0.128606369
## 309  0  0.126737533  2.122182602
## 310  0 -0.485950257  1.076740252
## 311  0 -1.003831691 -0.417382824
## 312  0  1.784203198  3.603545460
## 313  0 -0.867144559  1.173221669
## 314  1 -0.970574785  0.051589270
## 315  0 -1.202874783  0.118247582
## 316  0 -0.367313496  0.532868640
## 317  1  0.013773615  0.244012362
## 318  0  1.320088710 -1.112361669
## 319  0  0.489273674 -0.688699260
## 320  0 -1.202380268 -0.173611017
## 321  1 -0.873557167 -0.565188380
## 322  1  0.111954035  0.161828947
## 323  0  1.514231380  2.679819974
## 324  0  0.376559837  0.009708917
## 325  1  1.297281442  0.109394523
## 326  1  2.562356562 -1.080802614
## 327  0 -1.002024655 -0.221295208
## 328  1  0.800667328 -1.674835850
## 329  0 -1.035988169  0.822133367
## 330  1  1.365877489  0.226805224
## 331  1 -0.229221054 -2.164292490
## 332  0 -0.267813094 -0.589030932
## 333  1  0.373691037 -1.569437096
## 334  0  0.980535553  0.162205150
## 335  0  2.281291420  1.095458782
## 336  0  0.532324966  2.416092046
## 337  1  0.498830789 -2.387473334
## 338  1  0.154100500  0.480043031
## 339  1  0.082773252 -0.290633927
## 340  1 -1.585977289 -0.734818417
## 341  0 -1.529555425 -0.723605685
## 342  1  1.724155508 -0.483712505
## 343  0  0.204569302  0.197494226
## 344  0  1.237809010  0.985843770
## 345  1  0.242963732  0.007469590
## 346  1  1.013543653 -0.217846271
## 347  1  1.282489650 -0.913381962
## 348  0  0.289209163  0.049581827
## 349  1 -2.403685955  0.089344357
## 350  1  2.268593688 -0.390378947
## 351  0 -0.611040029 -0.140763080
## 352  1 -0.463479824  0.392414258
## 353  0 -1.481711447 -0.007980416
## 354  0 -0.304140955  1.708531707
## 355  1  0.313265182 -1.213384034
## 356  0 -0.265018542  0.985772858
## 357  0 -1.360338469  0.171611841
## 358  0  0.394405151  1.459585653
## 359  1  0.538715766 -1.599321040
## 360  1  2.137291965  0.307924132
## 361  1  0.207298220 -0.380841078
## 362  0 -1.276639279 -1.134305168
## 363  0 -0.515669447  0.670601382
## 364  1 -0.078895644  0.068813981
## 365  1  0.638348474 -0.277228224
## 366  0  0.812925028  1.083959444
## 367  0 -0.893184781  0.395295520
## 368  0 -0.401595576  0.184258489
## 369  0  0.824127950  0.658349041
## 370  0 -1.304010707 -0.009850306
## 371  0  0.873266146  0.968147812
## 372  1  0.689705268  0.455570098
## 373  0 -0.853394495 -0.158636173
## 374  0  0.654166469  1.462806595
## 375  1  1.400008806  0.675793050
## 376  0  1.002432289  0.373958503
## 377  0 -3.042110324  1.146755355
## 378  1 -0.242274127 -0.840590232
## 379  1  1.851809894 -0.383195691
## 380  0 -1.676781597 -0.521971407
## 381  1 -0.628017639 -1.692571888
## 382  0  0.253333621 -0.743482174
## 383  1 -0.025188786  0.912448831
## 384  0 -0.186620692  0.747481409
## 385  1 -0.233379734 -0.942136344
## 386  0  2.724207048 -0.406483070
## 387  0 -2.792593010 -0.606126466
## 388  0  1.651417134  1.242553644
## 389  0  2.609546715  0.525319711
## 390  1 -0.603246880  1.360055241
## 391  1  0.410431203 -0.293803154
## 392  0  0.551867593  0.622637790
## 393  0 -2.122146718  0.721552478
## 394  1 -0.113899397 -0.694183379
## 395  0 -0.273671467 -0.416374984
## 396  0  0.208377054  0.122534908
## 397  1  2.364867966 -1.155978151
## 398  1  1.605565418  0.936390689
## 399  1  1.653888842 -2.278958233
## 400  0  2.214271624  2.284395953
## 401  0 -0.441119911  1.051021847
## 402  1  0.563341173 -0.138919762
## 403  0  1.180089437  0.505009254
## 404  0 -0.933773326  0.129718064
## 405  0 -0.022007094  0.881542962
## 406  1 -0.233246562 -0.905255139
## 407  1 -1.224146118 -1.869912406
## 408  0  0.564314143  0.789415727
## 409  1  0.718121619  0.621935568
## 410  0 -0.771993338  0.612084810
## 411  1  0.434233333 -0.559441450
## 412  0  0.296161234  1.132143341
## 413  0 -0.971647706  0.626009534
## 414  0 -0.640902192  1.205961283
## 415  0  0.922922098  1.189682173
## 416  0 -1.086292574  1.488944385
## 417  1  0.460090646 -0.875746846
## 418  0 -0.218147958  0.488645912
## 419  1  1.323011066 -0.416130806
## 420  0  0.285302764 -0.865519664
## 421  0 -0.894471366  0.419549416
## 422  0 -0.017760113  0.415672062
## 423  1  0.422302972 -0.711531784
## 424  0  0.123370711  0.703778931
## 425  0 -1.658477230  0.103066706
## 426  1 -0.127865246 -0.318783223
## 427  0 -0.890580307  1.968486238
## 428  1  0.896985420  0.366705048
## 429  1  1.350929367 -0.901666225
## 430  0 -1.032052945 -1.138932313
## 431  1  0.513654664 -2.693146956
## 432  0  0.339709435  1.632403762
## 433  1 -0.264107864 -1.240707247
## 434  1  0.445102515  0.895702538
## 435  1  1.368019332 -0.792517359
## 436  0 -0.821511978 -0.593716034
## 437  0  0.191318638 -0.420116573
## 438  0  1.446818116 -1.288855570
## 439  0 -1.003428771 -0.364524421
## 440  0  0.615373217  0.879254628
## 441  1  0.244585662  1.178008137
## 442  0 -0.705352734  2.064641985
## 443  0 -1.561219030 -0.798758426
## 444  0 -0.757714098 -1.021013006
## 445  1  1.542098510 -1.531885844
## 446  1 -0.064669899 -0.459860476
## 447  0 -0.966578842  0.551831631
## 448  1  0.375867015  0.638220991
## 449  0 -2.020810170  2.175026361
## 450  0  0.244792596  0.323031392
## 451  1  0.024357435  0.858978892
## 452  1 -0.937410840  1.145256025
## 453  1  0.009875067 -0.372352405
## 454  0 -1.218941756  0.357980159
## 455  0  0.690627111  1.691587563
## 456  0  0.461008072  1.460092549
## 457  0  0.565864491  1.541083955
## 458  1 -0.042363881 -0.365822424
## 459  1  0.494492116  0.488531097
## 460  1 -0.773181892  0.065523275
## 461  1  1.580863257  1.610498739
## 462  0 -1.825163581  0.277979023
## 463  1  0.326875191 -0.715519070
## 464  0 -0.280689346 -1.015735355
## 465  1  0.898097174 -0.580649150
## 466  1 -0.641526810 -0.255376394
## 467  1  0.141200765 -1.245255630
## 468  1 -1.654511931 -0.217731356
## 469  0 -1.984728500 -0.006920912
## 470  1 -0.514325863  0.006000098
## 471  0 -1.963281412  0.012664981
## 472  1  0.303169082  0.718981872
## 473  1  0.613763724  1.278577825
## 474  0  0.748677558  0.054320425
## 475  1 -0.025737487 -0.581146123
## 476  1  0.716948265 -0.295601521
## 477  1  1.079118359 -0.600663916
## 478  0 -0.123602821 -1.005329068
## 479  0  0.729638423  0.198177644
## 480  0  0.209945564  1.304254459
## 481  0  0.987313403  2.794082833
## 482  0 -1.516439193  1.436968319
## 483  1 -0.018359785 -1.451490925
## 484  1 -0.589378277 -0.011476470
## 485  1  0.206255891 -1.010565850
## 486  0  0.452142564  0.506202560
## 487  1  0.666052622 -0.966300478
## 488  0 -0.526977088 -0.007907213
## 489  1  0.025128410 -1.100401437
## 490  1  0.551995466  0.021242955
## 491  0  0.415004841  0.962112084
## 492  1  0.354026642 -0.302059720
## 493  0 -0.764599960  1.536122832
## 494  0 -0.509099170 -2.005516654
## 495  0 -1.797437543  0.173053278
## 496  1 -0.299965592 -0.988044429
## 497  1 -0.450220045 -0.498609475
## 498  1  0.926355622 -0.677149307
## 499  0 -1.401341444  0.659553229
## 500  1  1.266077770  0.733122426
## 501  1 -0.237565973  1.141446486
## 502  1 -0.914303594 -2.233582973
## 503  0  1.443364326 -0.977243589
## 504  0 -0.013701583  0.311668207
## 505  0 -0.530397350  0.767689607
## 506  0  0.809799747  0.521980435
## 507  0 -0.335226669  0.542733856
## 508  1 -1.439145352 -0.161274186
## 509  1  1.861293268  0.617064537
## 510  1  0.053185761  0.269779231
## 511  1  0.048363719  0.692872456
## 512  1 -1.296305892  0.213469717
## 513  0 -1.072000114  0.142682751
## 514  0 -0.020532250  0.630919663
## 515  0 -0.708490788 -0.276418411
## 516  1  1.023947792 -0.632507867
## 517  1  1.082354080 -0.392765084
## 518  1  0.554768126  0.103911038
## 519  0  1.586779961  1.450392837
## 520  1  0.258860946 -0.086321000
## 521  1  1.265192476  0.188944324
## 522  1  1.155243220  1.001440874
## 523  1  1.673429858  0.040553310
## 524  1  0.629657208  1.569530142
## 525  1 -1.174445080 -0.297981949
## 526  0  0.798921975  0.710880885
## 527  1  1.209961784  0.310069771
## 528  1 -0.544853699 -0.391071585
## 529  1  1.182837038  1.318537432
## 530  1 -0.556169120 -2.160609088
## 531  0  0.710042474 -0.496883390
## 532  0  2.074379205  0.253919996
## 533  1 -0.082676737 -0.572338116
## 534  0 -1.191441020  0.560342827
## 535  0 -2.604796109  0.096001430
## 536  0  0.417891219  0.891333097
## 537  0  0.131969280  1.844585338
## 538  1  0.498007496 -0.707610605
## 539  1  0.217052361  0.363615412
## 540  0  0.053708139 -0.169973723
## 541  1 -0.063578111 -0.995072658
## 542  1 -0.732268200 -0.584369350
## 543  0  1.500058600  2.391007016
## 544  1 -0.302830859 -2.230767357
## 545  1  0.853625205 -1.388703704
## 546  1 -0.401748757 -0.595604212
## 547  0 -0.713585913 -0.089875619
## 548  0 -2.131410157  0.500001957
## 549  1  0.545294625  0.026158667
## 550  0  0.217441462  1.340687260
## 551  1  0.155194094 -0.264199299
## 552  1  1.249536805 -0.810328800
## 553  0  0.496885618  0.796597508
## 554  0 -0.309076753 -0.169387490
## 555  1 -0.549228358  0.247323711
## 556  1  0.357551854 -0.757174720
## 557  1  1.083390186 -1.323021673
## 558  1  0.841140475 -0.366621031
## 559  1 -0.264479924 -0.234235878
## 560  0 -0.844749345 -0.561785936
## 561  1 -0.423713730  0.550680887
## 562  1  0.267596434 -1.437685920
## 563  0  0.313787615  0.377184447
## 564  0  0.968423223  1.277589695
## 565  0 -0.520841792  0.132861177
## 566  1 -0.005744297 -0.390143708
## 567  1  0.952890749 -2.233374262
## 568  0  0.715788002  0.375173770
## 569  1 -0.521479412  0.215375117
## 570  1 -0.347252939 -0.682139520
## 571  1 -1.193615928 -0.801724955
## 572  1  0.512965867 -0.316928021
## 573  0  0.040136878 -1.003672274
## 574  1  0.694734255 -0.161991349
## 575  0  0.840251703 -1.578781285
## 576  0 -0.655040366  0.695592309
## 577  1  0.762725635 -0.784083313
## 578  1  1.197467874 -1.084594802
## 579  0 -2.178007433 -0.135630472
## 580  0 -0.149109739 -0.347055413
## 581  1  0.245035641  0.570961187
## 582  1 -0.210997712 -0.944480245
## 583  0  0.306578190  1.142012026
## 584  0  0.216769528 -0.699447695
## 585  1 -0.666721393 -1.423524610
## 586  0 -0.963746394  1.029427250
## 587  0 -1.292509946  0.399286569
## 588  1  0.690055943 -0.144459891
## 589  0  2.197315218  1.452992238
## 590  0 -0.099265099  1.082097805
## 591  0 -0.573025062 -0.417850407
## 592  1 -0.876832408 -0.014523885
## 593  1  1.107525961  0.879671165
## 594  0 -0.913394120 -0.139272067
## 595  1  1.225612645 -0.253841087
## 596  0 -1.153224135  1.975041611
## 597  0 -0.802089264  1.003351398
## 598  0 -1.301246728 -0.010941204
## 599  0 -0.544186494 -0.630351880
## 600  0  0.862495688  0.827314923
## 601  0  0.772252831  1.722324394
## 602  1 -0.994521916 -1.644023859
## 603  1  0.340337126 -1.079051165
## 604  0 -0.436641507  2.024901103
## 605  1  0.195052346 -0.859549131
## 606  1 -0.081217956  0.174027710
## 607  1  1.404255045 -0.303658670
## 608  1  0.661418176  0.895086699
## 609  0  0.529903349  0.090716878
## 610  1  1.661989328 -0.096783980
## 611  1 -0.362518124 -2.223350411
## 612  1 -1.441137171 -0.291140158
## 613  1  1.638578200 -1.190064692
## 614  0 -1.923365406  0.286512204
## 615  0  0.309752322 -0.368306156
## 616  0  0.828795139  0.249581507
## 617  0 -1.461986811  0.841324252
## 618  0 -0.103458524  0.329463376
## 619  1  1.131949229  0.071518692
## 620  1 -0.605596672 -1.561895110
## 621  0 -0.453984276 -0.004965868
## 622  0 -0.097612023  0.531243694
## 623  1 -0.337073721 -0.944605121
## 624  1  0.250194071 -0.865758033
## 625  0  0.725096500  0.669895078
## 626  0 -1.683939192 -0.250545458
## 627  0 -0.757340698 -0.185988199
## 628  0  0.024224152 -2.167683786
## 629  0 -1.931125994  2.895047472
## 630  0  0.331462341  0.720788772
## 631  1 -0.376044710 -0.393912755
## 632  1  0.735922250 -0.775918919
## 633  1 -0.833266269  0.083205099
## 634  1 -0.480124639  0.030774061
## 635  0 -2.406617995  0.800218616
## 636  0  0.412893079 -1.916704069
## 637  0 -1.174399317  0.502061264
## 638  0  0.140732763  1.595868344
## 639  0 -0.962333201  1.435551215
## 640  0 -0.504185433  1.400301025
## 641  1  0.501277268 -1.376125673
## 642  0  1.030017995 -0.381751926
## 643  0 -0.778220009  0.450254879
## 644  0  0.605401603 -0.092665698
## 645  0 -1.199201070  0.016703280
## 646  1 -1.558755372 -2.556803166
## 647  0  0.325345236  1.410020310
## 648  0  0.542851569  1.077045941
## 649  1  0.304423991  0.881381933
## 650  0 -0.683751074  1.343348955
## 651  1  0.100700772  0.059820911
## 652  0  0.854286054  0.511695495
## 653  1 -0.381366982 -0.167586626
## 654  0 -0.575194401  0.779625608
## 655  0 -0.915962477 -0.734401000
## 656  1 -0.702735139  0.587040418
## 657  0  0.331287193  0.515721102
## 658  1  1.525374375 -0.268272777
## 659  1 -0.729146945 -0.520092867
## 660  1  2.235763240 -0.567126097
## 661  1  0.473862759  0.022812234
## 662  1  1.614032276  0.342185368
## 663  0  0.225151401 -0.300705524
## 664  1 -0.772273011  0.100057004
## 665  1  0.203790162 -2.161434449
## 666  0  1.097421972  0.750324141
## 667  1 -0.268394986 -1.110316435
## 668  1 -1.453388268  0.839715987
## 669  0  0.238079585 -0.556615315
## 670  0  0.643059708  0.006616981
## 671  0 -0.708430058  0.970465522
## 672  0 -0.613042679  0.902759135
## 673  1  0.138329545 -1.309451630
## 674  1  0.671275175 -1.847728664
## 675  1  0.560490291 -0.078202777
## 676  0  1.300914345  1.262139696
## 677  0 -1.929557606 -1.644798489
## 678  0 -2.280682875  2.219877638
## 679  0 -1.236646517 -0.599402531
## 680  0  0.204415725 -0.359234450
## 681  0  0.892131228  0.835710363
## 682  1  0.546645875 -0.600643758
## 683  0 -2.125499379  0.095536004
## 684  0  0.266252241  0.699332192
## 685  1 -0.116534396  0.322558625
## 686  0 -0.544277213  0.960425776
## 687  1 -0.301727269 -0.371600306
## 688  0 -0.042183728  0.948674376
## 689  1  0.685970311  1.324718693
## 690  0 -1.106915269 -0.813540216
## 691  1  2.174463737 -0.089120436
## 692  0 -0.312277225 -0.361866373
## 693  0  0.393352643 -0.148153035
## 694  1 -0.394369188 -1.418036648
## 695  1  1.839546457 -1.113868580
## 696  0 -2.029448421 -0.675521625
## 697  0 -1.557903194  0.393878562
## 698  1  1.515593816 -0.579531921
## 699  0 -1.010074852  0.166598997
## 700  0  1.089721604 -0.135807603
## 701  1 -1.455054580 -1.353495916
## 702  0  1.244562895  2.043784537
## 703  1 -0.431947066 -0.380268284
## 704  1  0.006869276  0.725128868
## 705  0  0.124551049  1.292948021
## 706  0 -0.409628630  1.307282018
## 707  1  0.563413643 -0.703566719
## 708  0  1.606958115  1.062578565
## 709  0 -1.001951673  1.328677772
## 710  1  0.509325898 -0.946270302
## 711  0  0.353716735  2.082297641
## 712  0  0.196861225 -0.657776161
## 713  1  1.024085291  0.772393379
## 714  1 -0.953975347 -2.116683314
## 715  0  0.469299524 -0.545721107
## 716  1 -0.084159999 -0.472244940
## 717  0 -1.130436861 -0.305554308
## 718  1 -0.230487489  0.272381535
## 719  1 -0.033262088 -0.437943835
## 720  1 -1.003035348  0.713383406
## 721  0  0.071542363 -0.439832935
## 722  0  1.587421191  1.490961771
## 723  1  0.427751930 -0.477461175
## 724  0  0.308248594  0.648756399
## 725  1  0.121656262 -1.921977828
## 726  0 -1.317024038  0.826029651
## 727  1 -0.909648055  0.242676071
## 728  0 -0.547850929  0.701404486
## 729  0 -0.786164875  0.887870619
## 730  1  0.361859808 -0.089689657
## 731  0 -0.319370903 -0.107268745
## 732  0  0.453330251  0.837351518
## 733  1  0.187373442  0.476303934
## 734  0  0.813470637  1.427132048
## 735  1 -1.149198291 -1.631496478
## 736  0 -1.002334635  0.789646237
## 737  1  1.270341536 -1.451908044
## 738  0  0.225266278 -0.502022581
## 739  1 -1.428880811  0.742943992
## 740  0 -0.636583088 -0.755720501
## 741  0  0.024270472  1.531832928
## 742  0  1.639986708  0.349634772
## 743  1  2.537115060 -0.783306324
## 744  1  0.030887259 -1.355886353
## 745  1  0.162896180  1.255891398
## 746  0 -0.696807446 -0.152657957
## 747  1  1.862484867 -0.003788717
## 748  1  0.394783951 -1.041303062
## 749  1 -0.831239637 -0.795321101
## 750  0 -1.576717825  0.096877467
## 751  1  0.510324080 -0.220363623
## 752  0 -1.446372164 -0.058418866
## 753  0 -1.149373739  0.477687933
## 754  0 -1.452195854 -0.790867560
## 755  0 -0.416498908  0.813556924
## 756  1  1.729261335 -1.393503838
## 757  0 -0.536868667  1.298248102
## 758  0  1.072530736 -1.008093554
## 759  0 -0.428423860 -0.111401485
## 760  1  1.008122124 -1.087993084
## 761  1 -1.051405262  1.347546323
## 762  1  0.416472333 -0.848862367
## 763  1  0.795446589 -0.719797671
## 764  1  1.468349517  0.538965006
## 765  0  0.156620959 -1.467794283
## 766  0 -0.006058787  1.980890027
## 767  1  0.517898064 -1.679535795
## 768  0 -0.812061260  1.860440239
## 769  1  0.518137819  0.082515656
## 770  1 -0.537654786 -1.628200917
## 771  1  0.709858634  0.311188221
## 772  1  2.139179003 -0.703144086
## 773  1  0.705982523 -0.951131526
## 774  1  2.402835621 -0.345279135
## 775  1 -0.459171862  0.728514356
## 776  1 -0.276606408 -1.159353639
## 777  0 -0.911957040  0.226530493
## 778  1 -0.253164099  0.563149482
## 779  0 -1.344950684 -0.728902035
## 780  1 -0.757648366 -0.953710154
## 781  0 -0.059597470 -0.415798748
## 782  0  0.092574056  0.800734671
## 783  0 -0.409401084 -0.290525963
## 784  0  0.064001440  0.753926966
## 785  0 -0.810362702  2.151810901
## 786  0 -0.086159538  0.525332009
## 787  0  0.263963327 -0.418887023
## 788  0 -1.307831848  0.527156083
## 789  1  1.086186095  1.221676587
## 790  0  1.025168447  0.375483139
## 791  0 -0.608179574 -1.296155645
## 792  1  0.087294475 -0.628876464
## 793  0  1.475828983 -0.464297814
## 794  1  2.341336651 -1.246258677
## 795  0 -2.008020221 -0.602133427
## 796  1 -0.623211550 -0.397179917
## 797  0  0.454940521  0.335091951
## 798  0 -0.391792934  0.437860505
## 799  0  0.011643538  0.377421191
## 800  1 -0.223239230  0.310310453
## 801  0 -1.188197712  0.161463680
## 802  1 -0.112982326  0.533957313
## 803  1 -0.568207654 -0.614330015
## 804  1 -2.675884273 -0.717383418
## 805  0 -0.193132378 -0.833708801
## 806  0  0.687832009 -1.504367054
## 807  1  1.604100973  0.011763209
## 808  0  1.206515412  1.115363084
## 809  1  0.740838465 -0.175155077
## 810  1  0.241231270 -0.431126894
## 811  0  1.052772853  0.538772767
## 812  1  1.792342750 -0.607323975
## 813  1  1.381249690  0.010255331
## 814  0  0.654784573  0.546224445
## 815  1  1.895738988 -0.495604673
## 816  0  0.260448860  0.759649086
## 817  0 -1.127138650 -0.037300030
## 818  1 -1.105393970 -1.531137261
## 819  0 -0.620768282  1.546483463
## 820  1 -1.529789145 -1.025675665
## 821  1  1.088979377 -1.960601091
## 822  0 -0.152919969  1.680592178
## 823  0  0.261863462  0.292212286
## 824  0  0.816179505 -0.796066803
## 825  0 -2.265878963  0.414439562
## 826  0 -1.040512706  0.542415397
## 827  0 -0.942165280  1.805691603
## 828  0  1.112934360 -0.533304900
## 829  1  0.623065651  1.658638711
## 830  1  2.011536316  0.499301781
## 831  0 -1.852254527  1.093322235
## 832  0  0.178869388  2.203419181
## 833  0 -0.335349450  0.622686596
## 834  1  0.174933182 -0.035226497
## 835  1 -0.301957509  1.272148589
## 836  1  0.400901479 -1.555697259
## 837  0 -0.201540137  0.320672439
## 838  1 -0.650374346  0.320314493
## 839  0 -0.031518016  0.850663408
## 840  1  1.253388469 -1.323009636
## 841  1  0.903504908 -0.439923513
## 842  0 -0.682817803 -1.129462235
## 843  0 -1.542089399  0.236773923
## 844  1  0.570471556 -0.712582179
## 845  0 -1.187921011  0.587746382
## 846  1 -0.385609866  0.676864683
## 847  0 -1.325109771  0.846674994
## 848  0 -0.300708236  1.222108496
## 849  1 -0.256499073 -0.053853649
## 850  0 -0.932719968  1.496051329
## 851  0  1.110121139  0.492405164
## 852  1 -0.768894842 -0.778952355
## 853  1  0.510829746 -0.008639899
## 854  1  2.015061051  0.023112453
## 855  1  2.037684493 -0.061809106
## 856  1  1.036774747 -1.143575669
## 857  1  0.113197509 -0.595795692
## 858  1  0.398034195  1.235107584
## 859  1  0.043746515  0.379159578
## 860  0 -0.792166372 -0.226725540
## 861  0 -0.530615115  1.060378474
## 862  1  1.501391552 -0.109535018
## 863  1  0.526542305 -0.955039866
## 864  0  0.264948123  1.418234443
## 865  1  1.907293776  0.167433134
## 866  1  0.696559222 -1.045443545
## 867  0 -0.307992316 -0.241798354
## 868  0 -0.467975563 -0.192812800
## 869  0  0.188763951  1.937748109
## 870  0  0.633411649  0.374377823
## 871  0 -0.979443771  0.759108499
## 872  0 -2.449393423 -1.014518835
## 873  1  0.082242655  0.337114011
## 874  1  1.044555253 -0.770995509
## 875  0 -0.020514081  0.612589870
## 876  1 -0.046649420 -0.201836334
## 877  1 -0.235566106 -3.210021416
## 878  0  1.051434438  1.910698656
## 879  1 -0.028177875  0.348973591
## 880  1  0.652416950 -0.663767236
## 881  1  1.246870921 -0.988080405
## 882  1  0.946435329 -1.045229173
## 883  0  0.032504989  0.357058996
## 884  1  0.777086529 -1.945206921
## 885  1  0.619647218 -0.703314701
## 886  1 -0.422533182 -0.403430500
## 887  0  0.459620010 -0.693661043
## 888  1  0.041750066 -0.106874951
## 889  1  0.394537137 -1.478028648
## 890  1  0.189219041 -0.046839980
## 891  1  0.502739092 -1.311699549
## 892  1 -0.154894220 -2.372930129
## 893  0  0.064097674  1.386212107
## 894  1 -0.382393641 -0.462457130
## 895  0  0.220930714  1.063690536
## 896  1  0.618741070 -0.670779759
## 897  0 -0.124456778 -0.519952865
## 898  0 -1.312567090  0.359934199
## 899  0  0.539828535  0.922068988
## 900  1 -0.496620524 -0.553119802
## 901  0  0.499270593  1.105014645
## 902  1  1.541335533 -1.779543040
## 903  1  0.333229475 -3.643902053
## 904  1  0.612103532 -1.060323578
## 905  0  1.431689500  0.121290745
## 906  1  0.425080847 -1.323355880
## 907  0 -0.344395854 -0.641175169
## 908  0 -1.460153987 -0.599392951
## 909  0 -0.349028168 -1.329703964
## 910  0  0.559910513  2.153807406
## 911  1  1.810500295  0.315567353
## 912  0  0.443172974 -0.208571135
## 913  0  0.291101894  0.098086616
## 914  1  0.126211724  1.741251322
## 915  1  1.382377910 -1.359532516
## 916  0  0.839162174  0.657973258
## 917  0 -0.316456631  0.290679853
## 918  0  1.485974737  0.606560947
## 919  1  0.204183307  0.820018526
## 920  0 -1.529815269  0.792391187
## 921  1  1.731778978 -0.811793314
## 922  1  0.945493993  0.859695437
## 923  0 -1.104654760 -0.549487545
## 924  1 -0.674460959 -0.554991094
## 925  0 -0.464451419  0.120961419
## 926  1 -0.403208193 -0.298577451
## 927  1  0.819332013 -0.310334714
## 928  0 -0.050672704  1.666625040
## 929  0  0.417984593  0.582446652
## 930  1  1.294385982 -0.622435522
## 931  0 -0.725631912 -0.340180692
## 932  1 -0.615910886 -1.255117326
## 933  1  2.165306688 -2.603414394
## 934  1  0.272846999  1.441941030
## 935  0  1.154335345  0.323079354
## 936  1 -0.730481452 -0.675501701
## 937  1 -0.420735205  0.380980255
## 938  1  0.696688040 -0.717311695
## 939  1 -0.202744803 -0.503300275
## 940  1 -0.423028864  0.438457451
## 941  0  0.165060896  1.408851460
## 942  0 -0.022007582  2.737326853
## 943  1  0.152354857  1.690053010
## 944  0 -0.871148134 -0.756809655
## 945  0 -0.708574014 -1.469509867
## 946  0  1.530183785  1.208860149
## 947  0  0.021380277 -0.841533716
## 948  1  1.536321879 -1.481898377
## 949  0  1.282022465  1.098479393
## 950  0 -1.423256649  1.488755222
## 951  1  0.963116729 -0.195498693
## 952  1 -0.167454450 -1.322295502
## 953  1 -0.016288520 -0.942765861
## 954  1 -1.018256987 -0.944239563
## 955  1  0.940713417  0.329372966
## 956  1  1.237238771 -0.377488130
## 957  1  2.151646245 -0.257743131
## 958  1  1.714155125 -0.308416814
## 959  0 -1.292846566  0.617800377
## 960  1 -0.010518574  0.138645235
## 961  0 -0.527235029  1.478842376
## 962  1  0.137208484 -0.634699788
## 963  0  0.828722950  1.600288763
## 964  1 -1.480202195 -0.520252058
## 965  1  0.224707208 -1.058638136
## 966  1  2.197572915 -0.790273216
## 967  1 -1.316198745 -0.508611316
## 968  1  0.547361886 -1.998458837
## 969  1  0.496877318 -1.750455425
## 970  1 -1.010962147 -1.013801028
## 971  0  0.106146259  0.787629994
## 972  1  0.699002654  0.851625712
## 973  1 -0.295719655 -0.523312639
## 974  0 -0.177922226  0.955604796
## 975  0  1.114934111  0.596376733
## 976  1 -0.397766909 -0.071134680
## 977  1  0.573207521 -0.459513255
## 978  0 -0.112141269  1.305281863
## 979  0 -0.033574078  0.936511069
## 980  0 -0.649523303 -0.695330626
## 981  1  0.716907527 -0.860701954
## 982  0 -1.474859851  1.770863176
## 983  0  1.226159173 -0.911141456
## 984  0 -0.531220646  0.259526283
## 985  0 -0.470629773 -0.475190057
## 986  1  0.029390220 -0.645843124
## 987  0  0.872806805 -0.877583379
## 988  1  0.172179666 -0.632409079
## 989  0  1.054999017  0.515174039
## 990  0  0.392255225 -0.762958708
## 991  1  0.025937217  0.401785013
## 992  1  2.185152528 -0.601859262
## 993  0  0.059551116 -0.060491754
## 994  1  0.457302383 -1.596512716
## 995  1  0.719371042  1.030271661
## 996  0 -0.867438813  0.666611974
## 997  1 -1.176691588  0.528917690
## 998  0 -1.585033863 -1.922516278
## 999  0 -0.670673990  0.194597100
## 1000 0  0.501634859 -1.062005179
## 1001 0  1.756936350  0.195992927
## 1002 1  0.329390867 -0.287934681
## 1003 0 -0.252497029  0.045217302
## 1004 0 -1.916858726 -0.442735850
## 1005 1 -1.638734415 -2.968713440
## 1006 1  2.351887766 -0.614924714
## 1007 1  1.028210024 -0.402836543
## 1008 1  1.070775546 -1.293493844
## 1009 0  0.622174496  1.648720627
## 1010 1  0.338994694 -0.395383104
## 1011 0  0.231815777 -0.456458653
## 1012 1 -1.490874283  0.140340188
## 1013 0 -0.649039036 -0.238996486
## 1014 0 -1.109819364  1.359543182
## 1015 0  0.414603184  0.945465284
## 1016 0  0.254311002 -0.300896856
## 1017 1 -0.136076942 -0.869096579
## 1018 0  0.096188938  1.728716925
## 1019 1  0.629162172  0.847604173
## 1020 0  0.210539339 -0.971371559
## 1021 0 -1.420308139 -0.183907919
## 1022 0  0.597378606 -0.262247251
## 1023 0 -0.899988710  0.923900783
## 1024 0 -0.857427107  0.944028978
## 1025 1  0.238711712 -0.201833799
## 1026 1 -0.197582289  0.302277681
## 1027 1  0.435429670 -0.641994225
## 1028 1 -0.916711119 -0.469133043
## 1029 0  0.262182246  1.234834823
## 1030 0 -0.025872988  1.169301137
## 1031 0 -1.013178624 -0.240112626
## 1032 0 -0.819297479  0.588084542
## 1033 0 -2.264689690  0.722960239
## 1034 1 -0.466258433  0.196648905
## 1035 1  0.786733846 -1.491432971
## 1036 0 -1.277368898 -1.386458554
## 1037 0  0.360082401  1.515610919
## 1038 0 -0.599489389  0.108890734
## 1039 1  0.948126670  0.515248503
## 1040 1  0.255304751  0.862043754
## 1041 0  0.090691396  2.442411167
## 1042 0 -1.946112926  0.902738167
## 1043 0 -1.503987219  0.416709500
## 1044 1 -0.818189431 -0.506232896
## 1045 0  0.050010879 -0.618468171
## 1046 1  1.181985835  0.062058673
## 1047 1 -0.943772319 -2.956211536
## 1048 1  1.404891694 -1.130464373
## 1049 1  0.322940616  1.645071431
## 1050 0 -1.280844233 -0.458062773
## 1051 0  1.295279671  0.534345015
## 1052 0 -0.367550002  1.335352312
## 1053 1  0.500951010  0.995713380
## 1054 0  0.768588051  0.425727392
## 1055 1 -0.160639424 -0.415155088
## 1056 1  1.102169914 -1.708519326
## 1057 1  1.185563914 -0.486239700
## 1058 1  2.710714106 -1.023452944
## 1059 0 -0.828944091  1.156173720
## 1060 1  1.645981879 -0.968189902
## 1061 0  0.195401549  1.096291829
## 1062 1 -1.167422145 -0.485064403
## 1063 1 -0.053919723 -2.080964686
## 1064 1  0.445264746  0.112264775
## 1065 1 -0.199812140 -0.427983972
## 1066 1  2.087502291  0.991076509
## 1067 1  0.567683733  0.793692910
## 1068 0  0.978335872 -0.366273603
## 1069 1  1.069552829 -0.867612319
## 1070 1 -1.338647528  0.088861938
## 1071 1  0.928771849 -0.281518372
## 1072 1  1.337570677  1.268682294
## 1073 0  1.368323831  0.717332539
## 1074 1  0.285606045 -2.000544397
## 1075 1 -0.931673593  0.286594438
## 1076 0 -1.879886078 -1.906781709
## 1077 0 -1.127520789 -0.548055652
## 1078 1 -0.563505636 -1.876648729
## 1079 1  1.537225955 -1.198253982
## 1080 1  2.265773297  0.608399108
## 1081 1 -1.295720894 -1.211310936
## 1082 1 -0.123721848 -0.021649432
## 1083 0 -0.311332769  0.824409776
## 1084 0 -0.887650963  1.144827518
## 1085 1  0.363612813  0.232442475
## 1086 0 -0.998048067  1.298094801
## 1087 0  0.198865677  1.155686123
## 1088 1  0.773685128 -1.727670132
## 1089 0 -0.937573952  0.458124127
## 1090 0  0.659165149  1.686315857
## 1091 0 -0.123710388  1.638552537
## 1092 1  0.937118046 -0.256491886
## 1093 0 -0.290186935  1.833676551
## 1094 0 -0.347511093 -0.540781630
## 1095 0 -2.083746318  0.719434787
## 1096 0  0.049958774  0.840825107
## 1097 1  1.050933181  0.837728820
## 1098 1 -0.532143459 -1.143215548
## 1099 1  0.434398863 -1.413925860
## 1100 1  0.577426376 -0.467508552
## 1101 1  0.334084671  0.504343872
## 1102 1 -1.093748381 -1.332188372
## 1103 1  0.526700823  0.555147358
## 1104 1  1.459679263 -1.497363900
## 1105 0 -1.571544374 -1.396123662
## 1106 0 -0.320774318 -1.350356616
## 1107 1 -0.131960939 -0.233222234
## 1108 0 -1.167453126 -2.130174962
## 1109 1  0.707135049 -0.062340393
## 1110 1  2.106504411  0.660256326
## 1111 0 -0.478397326 -1.017190060
## 1112 1 -1.676428800 -1.326369244
## 1113 1 -0.712610457 -1.518242118
## 1114 1 -2.122648991 -0.703157467
## 1115 0 -0.284610993  0.865201153
## 1116 0  0.312421871 -0.318809262
## 1117 1  1.474761416 -1.620895021
## 1118 0 -1.797786253 -0.069226911
## 1119 1  0.045353465 -1.864298055
## 1120 1 -0.098977425 -0.233078869
## 1121 0  0.062361970  0.442934481
## 1122 1 -1.187390958 -0.853498927
## 1123 0  1.107737823  0.014003746
## 1124 0 -0.591649590  0.305225480
## 1125 0  0.013069869  0.535830427
## 1126 0  0.006429538  1.168192942
## 1127 0  0.495695987  1.809495365
## 1128 0 -1.669791719  1.264410777
## 1129 1  0.313628301  1.104380292
## 1130 0 -1.207543332  1.173946626
## 1131 0 -2.744668106  1.230144432
## 1132 0 -0.756870368  0.810095023
## 1133 1  2.403081411  0.443239967
## 1134 0  0.206946259 -0.514791411
## 1135 1 -0.428032989 -0.797364583
## 1136 1  0.294559096  1.207590990
## 1137 1 -0.362820984  0.461427631
## 1138 1  0.401774828 -0.711583734
## 1139 1 -0.679268592 -1.793827696
## 1140 1 -0.663418214 -0.208333228
## 1141 0  0.525902238  2.139666253
## 1142 1 -1.147759233 -0.438674515
## 1143 0 -0.619500551 -0.091885450
## 1144 1  0.116341672 -1.609483279
## 1145 0 -0.102478278  2.486132043
## 1146 0 -0.050459892 -1.090243328
## 1147 1  0.803963121 -0.365024078
## 1148 0 -0.206498688 -0.444513758
## 1149 1  0.351070702  0.898641938
## 1150 0  1.313390516  0.640928295
## 1151 1 -0.455748320 -1.202913718
## 1152 0 -0.482840630  2.182032054
## 1153 1  0.325126009 -0.801712678
## 1154 1 -0.415286281 -1.196692028
## 1155 0 -2.088269076 -0.789044266
## 1156 0 -1.276997569 -0.235436238
## 1157 0  0.705057989 -1.041161189
## 1158 1  0.761163346 -0.089886288
## 1159 1 -0.008795716  1.324728907
## 1160 0  0.751957318 -0.538518082
## 1161 1  0.357618334  0.960859529
## 1162 0 -0.100729006 -0.266769784
## 1163 0 -3.107155080 -0.428311054
## 1164 1 -0.591181710  1.151131880
## 1165 0 -0.472771969  0.398410386
## 1166 1  1.575780220  0.529908943
## 1167 1  1.081939164 -1.526784033
## 1168 0 -0.532635887  1.631272322
## 1169 0 -0.349591662  2.049079291
## 1170 0  0.780217438 -0.028155521
## 1171 0 -1.747420062 -0.420132859
## 1172 0 -0.675244377  1.028415767
## 1173 0 -0.882188888 -0.324336907
## 1174 0  0.548273198  0.305456525
## 1175 0  0.307815862 -0.386387424
## 1176 0  0.344033846  0.570238572
## 1177 1 -0.298661356  0.412336419
## 1178 1  0.115194385 -0.284116562
## 1179 0  0.876682635  1.096960220
## 1180 1 -0.614034508 -1.565008877
## 1181 1 -0.129016446 -0.383652030
## 1182 1  0.575422090 -2.343101006
## 1183 0  1.444802897  0.325090011
## 1184 1 -0.787907524 -0.749224625
## 1185 1  2.395240648 -2.024385369
## 1186 0  0.462729143  0.359420795
## 1187 0 -0.579490767 -0.317630286
## 1188 1  1.841756811  0.702170760
## 1189 1  0.364754221 -0.795643949
## 1190 1  0.773150496  1.256152928
## 1191 0 -0.544881466  1.521280358
## 1192 1 -0.423406884 -2.838585015
## 1193 1  0.472536759 -2.191951616
## 1194 0 -0.557954322  0.369971314
## 1195 1 -1.144447712 -0.526779710
## 1196 0 -1.122646907 -0.535666212
## 1197 0 -2.976619054  0.150165326
## 1198 1  0.825234010  0.227695918
## 1199 0 -1.224313848  0.365321567
## 1200 0 -0.159762490  1.661539325
## 1201 1  1.490089694  0.474347102
## 1202 1  0.170279110  0.401679597
## 1203 1  2.434169611 -0.814850725
## 1204 1  1.531445635 -0.513752366
## 1205 1  0.935765477 -0.138244619
## 1206 0 -0.318391705  1.169469472
## 1207 0  0.223856185  0.878760234
## 1208 0  0.176365307  1.946229636
## 1209 1  0.254664180 -0.358683329
## 1210 1 -0.035500151 -0.158043900
## 1211 0 -0.150530234 -0.145218455
## 1212 1 -0.035174589  0.220886578
## 1213 0  2.242435763  0.824213999
## 1214 0 -0.812261806 -0.052925676
## 1215 1  0.839125044 -2.101954722
## 1216 1 -0.281076079 -0.931784967
## 1217 1  0.554108508 -0.103991801
## 1218 0 -0.993481393  0.545138913
## 1219 1 -0.350748118 -2.341284668
## 1220 0 -1.927624878 -0.133193218
## 1221 0 -0.291835008  2.641730555
## 1222 0 -1.783033685 -1.672371587
## 1223 0 -0.571731742  1.643220087
## 1224 1  0.894384018  1.150335078
## 1225 1  1.272348672 -0.995529353
## 1226 0 -0.156118725  0.872445857
## 1227 0  0.012452102  0.070896984
## 1228 1  1.671718107 -0.375779062
## 1229 1  1.271839891 -1.104746179
## 1230 1  1.816939349 -0.636501543
## 1231 1  0.773147509  0.481115440
## 1232 1  0.833048722 -0.554852402
## 1233 0  0.282131583 -0.017141581
## 1234 0  0.663762807 -0.564312271
## 1235 1  0.563454490  1.031349512
## 1236 0  0.049470854 -0.302837440
## 1237 0  0.264153927  2.269038597
## 1238 1 -0.210368774  0.115891182
## 1239 1 -0.932255276 -0.438914614
## 1240 1  0.961792968 -0.323707509
## 1241 0 -0.723796503 -1.080140042
## 1242 1 -0.176633769 -0.864751112
## 1243 0 -1.334401084  0.641291396
## 1244 1  0.733215450 -0.510609241
## 1245 1  0.118554499  0.673396086
## 1246 1  0.574744302 -0.495221531
## 1247 0 -1.395537910 -0.015934584
## 1248 1  0.645210989 -1.579217268
## 1249 1  2.138648250 -0.905133520
## 1250 1 -0.956184902 -1.663066164
## 1251 0  0.857357671  0.962174886
## 1252 1  2.358525005 -0.008956933
## 1253 0 -0.478318118 -0.011856495
## 1254 1 -1.189807856 -0.439305202
## 1255 0  0.259574899  0.502037295
## 1256 0  0.245235097  0.655299972
## 1257 1  1.331641290 -0.726963815
## 1258 1 -1.334931391  0.202898308
## 1259 1  0.441971954 -1.760346846
## 1260 0  0.337286055  1.656021828
## 1261 1  1.298870513 -0.669697409
## 1262 0 -0.765619889 -1.641061786
## 1263 1  0.354274617  0.927980204
## 1264 1 -1.145104108 -1.082303355
## 1265 1 -0.318634815  0.487023692
## 1266 1  0.327977407 -0.041615015
## 1267 1  0.073575607  0.120212746
## 1268 0 -0.401417013  0.593889038
## 1269 0  0.383597121  0.607325372
## 1270 1 -0.156022250  0.030503080
## 1271 0  0.146371043  0.761514940
## 1272 1 -1.272679390 -0.742312921
## 1273 0 -1.007139828  0.211008890
## 1274 1 -0.263166389 -0.909994281
## 1275 0 -0.927214835  0.354364244
## 1276 1  1.207671193  0.290365435
## 1277 1 -0.363828473 -0.112660188
## 1278 0 -0.125310595  1.040137468
## 1279 0 -1.917878846  0.832868832
## 1280 0  1.046806031  0.295218079
## 1281 1 -1.348484750  1.071072980
## 1282 0 -1.225576148 -1.873486984
## 1283 1 -0.708955114 -1.559967994
## 1284 1 -0.348634770 -0.420130020
## 1285 1  1.845184897  1.020932667
## 1286 0 -1.586558462  0.466066666
## 1287 0 -0.391671077  0.900678918
## 1288 1 -1.716972378 -2.601160198
## 1289 0  0.989956199  0.193057376
## 1290 0 -0.057120059  1.195876785
## 1291 1 -1.185764226 -2.809628454
## 1292 1  1.849573401  0.070731978
## 1293 1  1.535086867 -0.627806191
## 1294 0  0.046279481  0.714373420
## 1295 1  2.095062986 -1.211080698
## 1296 0 -0.108037156 -0.094487215
## 1297 0 -1.766555186 -0.516811586
## 1298 1  1.038872399 -0.972802974
## 1299 1  0.418680340 -0.193452233
## 1300 0 -1.053397074  0.939630988
## 1301 0 -0.807175717  0.726205359
## 1302 1  2.058937471 -0.012660804
## 1303 1  1.169221271  0.862650332
## 1304 0 -0.929634241  0.943870973
## 1305 0 -0.063523431  1.607554897
## 1306 0  1.488265529  1.069351747
## 1307 1 -0.970410224  0.381993013
## 1308 1 -1.870200367 -0.253177577
## 1309 0  0.139381711  0.140289258
## 1310 0 -0.597774947 -0.998911767
## 1311 1 -0.955950937 -0.663079253
## 1312 0 -1.366546261  0.121859710
## 1313 1 -1.700201994 -0.820289181
## 1314 0 -0.718096478 -0.183001576
## 1315 1  1.485184854 -1.012509179
## 1316 0  0.864015196 -1.106410688
## 1317 1  0.913327211 -1.113192596
## 1318 0 -0.304567855 -1.569840179
## 1319 0  0.219482874  0.553442115
## 1320 1  0.959247533 -0.151995622
## 1321 1 -0.133938885  0.318451168
## 1322 0 -0.289207186 -0.127812883
## 1323 1 -0.527267240 -1.660585935
## 1324 1  0.974171831 -2.423380715
## 1325 0  0.926903853 -0.471023215
## 1326 0 -0.119080310  0.905711002
## 1327 1 -0.386145825 -0.752359783
## 1328 0  0.294503108 -0.421251520
## 1329 0 -1.108727634  0.551711436
## 1330 0 -0.719633648 -0.184266562
## 1331 1 -0.072503399 -0.693932431
## 1332 0  0.049655208  1.558905516
## 1333 0 -0.773547258 -0.093246218
## 1334 0  0.597075316  1.533410823
## 1335 0  0.112537880  0.930873065
## 1336 1  2.297051519  0.653469964
## 1337 0  1.314805008 -0.958951567
## 1338 1 -0.326483302  1.077062899
## 1339 1 -0.313629201 -0.301348572
## 1340 1  0.641516392  0.546032200
## 1341 0 -0.768424115 -0.322953751
## 1342 0  1.119417678  1.427853083
## 1343 1 -0.544973308 -1.142224537
## 1344 1  0.399547377  0.008813333
## 1345 1  1.321233465  0.522228661
## 1346 0 -0.188280950  0.402504235
## 1347 0 -0.335640616 -0.279915775
## 1348 1  1.431497784  0.145288429
## 1349 0 -0.663517548  0.498217185
## 1350 1 -0.888575811  0.227439198
## 1351 1  1.501803644 -0.609444967
## 1352 0  0.896808991  1.203254186
## 1353 1  0.718069436 -0.656841746
## 1354 0  0.678239529  0.886966387
## 1355 1 -0.049576668 -0.780321714
## 1356 1  0.675645550  0.988803434
## 1357 1  0.837254118  0.941269514
## 1358 0 -0.764631451  0.513911655
## 1359 0  0.321087028  0.137614924
## 1360 1 -0.599274770  0.826401490
## 1361 1  1.754995580 -0.899795167
## 1362 1 -0.593884892 -0.137488172
## 1363 1  0.769462006 -0.393847508
## 1364 1  0.300262176 -0.732843627
## 1365 0 -1.556247958 -0.735829909
## 1366 1 -0.365241189 -1.037504362
## 1367 0 -0.495795090 -0.786653850
## 1368 0 -0.332460889  1.776423177
## 1369 1 -0.611666301  0.609762912
## 1370 0 -1.600224265  0.493944892
## 1371 1  1.435602738  0.640566474
## 1372 1  1.144267275 -0.538675598
## 1373 0 -1.437025550  1.416649451
## 1374 0 -0.681166074 -0.032064932
## 1375 0  1.815978487  2.077807574
## 1376 0 -0.699143474  1.213299991
## 1377 1  1.592403508 -1.065478478
## 1378 0  0.582775470 -0.465382126
## 1379 1 -1.109300916  0.136290403
## 1380 1 -0.189906646 -0.044346804
## 1381 1  0.674280872 -0.717453900
## 1382 1  0.237049193  1.854050512
## 1383 0 -0.194418360  1.421094200
## 1384 1  0.435435718 -0.747334435
## 1385 0  0.126220203 -0.580063569
## 1386 0 -0.555229961 -0.994416050
## 1387 1 -1.188714934  0.473222081
## 1388 0 -0.045864396  0.505909894
## 1389 1  0.094255229 -0.329180288
## 1390 0 -0.049168394 -0.186432389
## 1391 0  0.645569971  2.498105730
## 1392 1  1.477098554  0.491697872
## 1393 1  0.651136713 -0.217966874
## 1394 1  1.623367136 -1.300007822
## 1395 1 -0.317413739 -1.902816930
## 1396 0 -0.080233157  0.959871610
## 1397 1 -0.575288230 -0.531226393
## 1398 1  0.780288518 -0.583965287
## 1399 1 -0.209378147 -0.252278824
## 1400 0  0.122665805  0.643486163
## 1401 0 -1.514643209 -0.846898021
## 1402 0  0.258171144  1.037450798
## 1403 0  1.465170495  2.070217542
## 1404 0 -0.823043321  0.515797589
## 1405 0  0.600981145  0.025251426
## 1406 1  0.439021887  1.018898340
## 1407 0 -0.777284336  1.027406485
## 1408 1  0.241683922 -0.939344296
## 1409 0  0.759610744  1.006052751
## 1410 0  0.207507445  0.243105299
## 1411 0 -1.306560940 -0.701973255
## 1412 0  0.499090617  1.843474002
## 1413 1  1.374544187 -0.983673478
## 1414 1  0.146658781  0.129221506
## 1415 0 -1.091778610  1.277056203
## 1416 1  0.427369638 -0.799581514
## 1417 0 -0.279569014  1.582404477
## 1418 1  1.355678522 -0.840560185
## 1419 1  1.310002592  0.006046839
## 1420 1  1.919444147  0.931006913
## 1421 0 -0.804586342  0.032094886
## 1422 0  0.390790126  0.711323265
## 1423 0  1.495335512  0.466080635
## 1424 1 -0.268825289 -2.132199128
## 1425 1  0.254075942 -0.394125295
## 1426 0 -1.118841219 -0.595832994
## 1427 0  1.562331905 -0.251125321
## 1428 0  0.221799701 -0.221572453
## 1429 1 -0.477241978  0.503753914
## 1430 0 -0.252819395 -0.483029266
## 1431 0 -0.943403851  0.188300302
## 1432 0  0.193786042  0.538931353
## 1433 0 -0.731160232  2.892388215
## 1434 1  1.424512668  1.837228606
## 1435 1  0.407930755  0.557382947
## 1436 0 -1.097423686  0.801603420
## 1437 1 -1.290152790  0.404272672
## 1438 1 -0.778580244 -1.321217231
## 1439 1  1.877102972 -0.322864977
## 1440 0 -0.412572942 -0.613584823
## 1441 0  1.384118822  1.174886379
## 1442 1 -0.111065230 -0.074428045
## 1443 1 -1.484421507 -3.129607478
## 1444 0  0.769149986 -0.159928501
## 1445 0 -0.377178355  0.094683465
## 1446 1 -0.556908920 -1.989831413
## 1447 1 -0.715389994 -0.022883382
## 1448 1  1.466423421  0.725740955
## 1449 0 -1.718981120 -0.570241736
## 1450 0 -1.434697888  0.246382098
## 1451 1  1.379700921 -1.867212010
## 1452 0  0.491796961  2.104041024
## 1453 0  1.353045348 -0.414421580
## 1454 1  0.740556151 -0.950309886
## 1455 1  1.212243185 -1.381285887
## 1456 0 -2.120284205  0.414919390
## 1457 0  0.861990596  1.854984464
## 1458 0 -1.398536722  1.096966335
## 1459 0  0.154810976  1.756817944
## 1460 0 -0.375968693 -1.033489546
## 1461 0  0.449896000  0.499065115
## 1462 1  2.133996612 -1.426205879
## 1463 1 -0.374195225 -0.114882871
## 1464 0 -1.108735163 -0.241106013
## 1465 1 -0.316773012  0.491610170
## 1466 0 -1.824135790  2.194726592
## 1467 1  0.787594321 -0.354155137
## 1468 1 -0.617771483 -0.074076257
## 1469 0 -0.890510920 -0.365997193
## 1470 0 -1.703658974 -1.203496568
## 1471 0  0.198191797 -0.460820212
## 1472 1  0.692715256 -1.426393720
## 1473 0  0.549446279 -0.260986493
## 1474 1  2.561675598 -1.179202425
## 1475 0  1.523763993  2.187456136
## 1476 0 -1.761000043 -0.348936803
## 1477 1  1.805481119 -0.687535508
## 1478 0  0.303214825  1.873565855
## 1479 1 -0.058753135  0.288351022
## 1480 0 -0.135415818  1.981164017
## 1481 1  0.715992707  0.083572744
## 1482 0 -0.625804288  1.119815573
## 1483 1 -0.185160435 -0.417588768
## 1484 0  0.234939613 -1.234107955
## 1485 0 -0.401820354  1.625704749
## 1486 1  0.860870413 -0.642145505
## 1487 0  1.326124284  0.152062486
## 1488 1 -0.705288085 -0.153237190
## 1489 0  0.435378179 -1.133625738
## 1490 1 -0.129798062 -0.701920953
## 1491 0  0.055710683 -0.031116762
## 1492 0 -0.298940896  1.067684835
## 1493 1 -0.471418958 -0.507261030
## 1494 1  0.101775233 -0.839119751
## 1495 1  0.305412604  0.188783033
## 1496 1 -0.328705465 -0.423568749
## 1497 0 -0.749061442 -1.052728765
## 1498 0  0.484284906 -1.020222344
## 1499 0 -0.377812475  0.627821422
## 1500 0 -0.251709035 -0.938044358
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{err.rate.logistic.func }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(model, data\_test) \{}
\NormalTok{  pred }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{*}\NormalTok{ (}\FunctionTok{predict}\NormalTok{(model,data\_test, }\AttributeTok{type =} \StringTok{\textquotesingle{}response\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\textgreater{}} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{{-}} \DecValTok{1}
\NormalTok{  ct  }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(data\_test}\SpecialCharTok{$}\NormalTok{Y, pred)}
  \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(}\FunctionTok{prop.table}\NormalTok{(ct)))}
\NormalTok{\}}

\NormalTok{err.rate.discriminant.func }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(model, data\_test) \{}
\NormalTok{  pred }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{predict}\NormalTok{(model,data\_test)}\SpecialCharTok{$}\NormalTok{class) }\SpecialCharTok{{-}} \DecValTok{1}
\NormalTok{  ct  }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(data\_test}\SpecialCharTok{$}\NormalTok{Y, pred)}
  \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(}\FunctionTok{prop.table}\NormalTok{(ct)))}
\NormalTok{\}}

\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{Error rates per model:}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Error rates per model:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\FunctionTok{err.rate.logistic.func}\NormalTok{(}\AttributeTok{model =}\NormalTok{ logistic.linear, }\AttributeTok{data\_test =}\NormalTok{ prob2.test))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.296
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\FunctionTok{err.rate.logistic.func}\NormalTok{(}\AttributeTok{model =} \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(X1}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(X2}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{), }\AttributeTok{data =}\NormalTok{ prob2, }\AttributeTok{family =}\NormalTok{ binomial), }\AttributeTok{data\_test =}\NormalTok{ prob2.test))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4933333
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\FunctionTok{err.rate.discriminant.func}\NormalTok{(}\AttributeTok{model =}\NormalTok{ lda.model, }\AttributeTok{data\_test =}\NormalTok{ prob2.test))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2966667
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\FunctionTok{err.rate.discriminant.func}\NormalTok{(}\AttributeTok{model =}\NormalTok{ qda.model, }\AttributeTok{data\_test =}\NormalTok{ prob2.test))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3133333
\end{verbatim}

The models ranked from lowest to highest in error rates are logistic
regression w/ linear covariates, LDA, QDA, and logistic regression w/
squared covariates. The top 2 models are very close in error rate and
together suggest that the two classes are quite linearly separable.

Question 3

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# loading data}
\NormalTok{prob3 }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file =} \StringTok{\textquotesingle{}data/Problem3.csv\textquotesingle{}}\NormalTok{)}
\NormalTok{prob3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Y          X1          X2
## 1  2  0.55432694  0.27312919
## 2  2 -0.28027194  0.75552507
## 3  0  1.77516337 -0.34901841
## 4  3  0.18732012 -0.54619076
## 5  3  1.14252615  0.23436199
## 6  1  0.41552613 -0.29782822
## 7  2  1.22950656 -0.84047613
## 8  2  0.23667967  0.82651036
## 9  0 -0.36538277  1.48369123
## 10 2  1.10514427  0.69967564
## 11 1 -1.09359397 -1.26157415
## 12 2  0.46187091  0.29827174
## 13 0 -1.36098453 -0.14780711
## 14 1 -1.85602715 -0.88892233
## 15 0 -0.43985541  1.01306586
## 16 2 -0.19394690 -0.92052508
## 17 2  1.39643151 -0.57389450
## 18 1  0.10066325  1.15036548
## 19 2 -0.11443881  1.14382456
## 20 1  0.70222523 -0.23944276
## 21 2  0.26254267 -1.08680215
## 22 3  1.83616330 -0.06144699
## 23 2  0.35740242 -0.51669734
## 24 3 -1.04541013 -1.90767369
## 25 1  0.62018413  0.10715648
## 26 3  0.14935453 -1.17737519
## 27 0 -1.45931685  1.74542691
## 28 0 -2.02704380 -0.39869853
## 29 2 -1.05695776  0.44243942
## 30 0 -0.72814372  0.45027946
## 31 1 -0.00821067 -0.07606216
## 32 2  0.84779738  0.29751322
## 33 1 -0.38349150 -1.19435471
## 34 3 -0.52651151 -1.99687548
## 35 1 -0.27322596  1.38851305
## 36 1 -0.60574161 -0.08248357
## 37 0 -0.33286731  0.39251449
## 38 2 -0.24153755 -1.08276971
## 39 1 -0.86277540  1.60212039
## 40 0 -0.84697075  1.00406897
## 41 2  0.10034035  0.37989570
## 42 3  1.59003353 -0.56550536
## 43 2  0.56649488 -1.21377810
## 44 1  1.61447949 -1.36430159
## 45 2 -0.46865016 -1.41613295
## 46 1 -0.72610140 -0.25557803
## 47 1 -1.02333900 -1.22542595
## 48 1 -1.93781553  0.21383426
## 49 0  0.27714729  0.06722356
## 50 0  1.40835367  0.85663511
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# creating models}
\NormalTok{lda.model.prob3 }\OtherTok{\textless{}{-}} \FunctionTok{lda}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X1 }\SpecialCharTok{+}\NormalTok{ X2, }\AttributeTok{data =}\NormalTok{ prob3)}
\NormalTok{qda.model.prob3 }\OtherTok{\textless{}{-}} \FunctionTok{qda}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X1 }\SpecialCharTok{+}\NormalTok{ X2, }\AttributeTok{data =}\NormalTok{ prob3)}

\CommentTok{\# printing model objects}
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{lda.model.prob3:}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## lda.model.prob3:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lda.model.prob3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## lda(Y ~ X1 + X2, data = prob3)
## 
## Prior probabilities of groups:
##    0    1    2    3 
## 0.22 0.30 0.34 0.14 
## 
## Group means:
##           X1         X2
## 0 -0.3727183  0.5561256
## 1 -0.3544829 -0.1615989
## 2  0.2801608 -0.1490759
## 3  0.4762109 -0.8601008
## 
## Coefficients of linear discriminants:
##           LD1       LD2
## X1  0.6624955 0.8433123
## X2 -0.9769546 0.6415818
## 
## Proportion of trace:
##    LD1    LD2 
## 0.8736 0.1264
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{qda.model.prob3:}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## qda.model.prob3:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{qda.model.prob3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## qda(Y ~ X1 + X2, data = prob3)
## 
## Prior probabilities of groups:
##    0    1    2    3 
## 0.22 0.30 0.34 0.14 
## 
## Group means:
##           X1         X2
## 0 -0.3727183  0.5561256
## 1 -0.3544829 -0.1615989
## 2  0.2801608 -0.1490759
## 3  0.4762109 -0.8601008
\end{verbatim}

Summaries of models.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{LDA:}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## LDA:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\AttributeTok{object =}\NormalTok{ lda.model.prob3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         Length Class  Mode     
## prior   4      -none- numeric  
## counts  4      -none- numeric  
## means   8      -none- numeric  
## scaling 4      -none- numeric  
## lev     4      -none- character
## svd     2      -none- numeric  
## N       1      -none- numeric  
## call    3      -none- call     
## terms   3      terms  call     
## xlevels 0      -none- list
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{QDA:}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## QDA:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\AttributeTok{object =}\NormalTok{ qda.model.prob3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         Length Class  Mode     
## prior    4     -none- numeric  
## counts   4     -none- numeric  
## means    8     -none- numeric  
## scaling 16     -none- numeric  
## ldet     4     -none- numeric  
## lev      4     -none- character
## N        1     -none- numeric  
## call     3     -none- call     
## terms    3     terms  call     
## xlevels  0     -none- list
\end{verbatim}

Question 3i.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lda.pred.prob3 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\AttributeTok{x =} \FunctionTok{predict}\NormalTok{(lda.model.prob3, grid)}\SpecialCharTok{$}\NormalTok{class) }\SpecialCharTok{{-}} \DecValTok{1}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ grid, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ X1, }\AttributeTok{y =}\NormalTok{ X2, }\AttributeTok{color =} \FunctionTok{as.factor}\NormalTok{(}\AttributeTok{x =}\NormalTok{ lda.pred.prob3))) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =}\NormalTok{ ticks) }\SpecialCharTok{+} 
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{breaks =}\NormalTok{ ticks) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\AttributeTok{label =} \StringTok{\textquotesingle{}LDA\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Chapter-2-HW_files/figure-latex/unnamed-chunk-13-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{qda.pred.prob3 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\AttributeTok{x =} \FunctionTok{predict}\NormalTok{(qda.model.prob3, grid)}\SpecialCharTok{$}\NormalTok{class) }\SpecialCharTok{{-}} \DecValTok{1}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ grid, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ X1, }\AttributeTok{y =}\NormalTok{ X2, }\AttributeTok{color =} \FunctionTok{as.factor}\NormalTok{(}\AttributeTok{x =}\NormalTok{ qda.pred.prob3))) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =}\NormalTok{ ticks) }\SpecialCharTok{+} 
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{breaks =}\NormalTok{ ticks) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\AttributeTok{label =} \StringTok{\textquotesingle{}QDA\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Chapter-2-HW_files/figure-latex/unnamed-chunk-14-1.pdf}

Question 3ii

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load test dataset}
\NormalTok{prob3.test }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file =} \StringTok{\textquotesingle{}data/Problem3test.csv\textquotesingle{}}\NormalTok{)}
\NormalTok{prob3.test}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Y           X1            X2
## 1    1  0.613784261  1.2103550739
## 2    2  0.412614524 -0.4789165684
## 3    0 -0.458893669  1.1261809006
## 4    2 -0.673178501 -1.4614845064
## 5    0  0.193054297  1.2084881405
## 6    3  1.382929057 -1.1286067035
## 7    2  0.064967339 -0.4450009431
## 8    3  2.302243701 -0.3447496159
## 9    1 -2.083677787  0.2347292455
## 10   1 -1.157149225 -0.3969460868
## 11   2  0.623805645 -1.1623811460
## 12   1 -1.522859085 -0.7384102010
## 13   0 -2.611055995  0.5251862467
## 14   3  2.526573820 -2.1093928340
## 15   1  0.377890535  0.8657146543
## 16   2  0.028446625  1.3190771749
## 17   0 -0.706028546  0.2236660676
## 18   3  0.634799654 -0.1693998503
## 19   3  1.168828062 -0.6033511233
## 20   0 -0.308182664  0.8841719271
## 21   0 -0.703780854 -0.7457286750
## 22   1 -1.269993476  0.9685560849
## 23   1  0.095576981  1.9422320886
## 24   2 -0.446568926 -1.0059540893
## 25   0 -1.709381636  1.3543028097
## 26   0 -1.442223218 -0.1494106063
## 27   3  1.273168425 -0.3180851065
## 28   2  0.765854837 -0.2409480614
## 29   1 -0.930395608 -0.5564778573
## 30   2  1.448039911 -0.0534663848
## 31   3 -0.512486889 -1.2536296028
## 32   2  2.336057430  1.3223083720
## 33   2  0.654727978  0.2860471717
## 34   1 -0.139395433  0.4023530274
## 35   0 -0.503982378  0.8053874655
## 36   1  0.057283958  0.3208678931
## 37   1 -1.439879686 -0.4167730068
## 38   3  1.254845652 -0.8834786876
## 39   2  0.419987880 -0.3256439079
## 40   2  0.105962081 -0.5614074731
## 41   1  0.089930522  1.8152623023
## 42   0 -2.381103931 -0.0014443777
## 43   1  2.905688567  0.6864864516
## 44   1  0.256785279  0.1357275366
## 45   3  0.156277425  0.0429864295
## 46   0 -1.284913957  2.0407595428
## 47   1 -0.448597574  0.8172656676
## 48   0 -1.589604816  2.0119240343
## 49   3 -1.256015801 -2.2270638491
## 50   1 -0.106843606 -0.7571038761
## 51   1 -1.274831133  0.1664907403
## 52   1 -1.103806687 -1.1436888252
## 53   2  0.135233017  0.6319085843
## 54   1  0.333342621 -0.1862531044
## 55   0 -1.038405067  0.1766040864
## 56   3  0.219711940 -0.4485194242
## 57   1  0.397292797 -0.4472816943
## 58   1  1.459550621  0.8198669250
## 59   0 -1.163745825 -0.2482137789
## 60   2 -1.354997839 -0.4512304499
## 61   3  1.072274835 -1.1499507724
## 62   3  1.070002491 -2.8862684729
## 63   0 -0.166770621  1.6767867614
## 64   1  1.114057628  0.3196450451
## 65   1 -2.231594579 -1.2928473647
## 66   2  1.584155849  0.0909684952
## 67   2 -1.091625729 -0.8377274610
## 68   1  0.476942318  1.4718979535
## 69   3  0.334858131  0.1165126186
## 70   1  0.165884569 -1.7166025832
## 71   2  0.338407299 -0.0096585099
## 72   3  0.653596619 -0.6477558964
## 73   0 -0.902883750  1.4545098392
## 74   1 -0.028594982  0.0941659749
## 75   0 -0.728241539  0.4144833897
## 76   1 -0.133624610  0.1758367670
## 77   1  0.788151939 -1.1207758682
## 78   2 -0.918335867 -1.0606093695
## 79   1  1.351959179  2.8423710107
## 80   3  0.561280864 -2.2813223790
## 81   3  1.322193987 -2.3589769608
## 82   1  0.954265071  0.1914314149
## 83   3  0.876563156 -1.0685986970
## 84   2  0.681676146 -0.0131145493
## 85   3  0.433397662 -0.8683113217
## 86   2  1.529153350 -0.8120936045
## 87   0 -2.066457117  1.0035252232
## 88   1  0.877878056  0.1858114554
## 89   1  1.433081607  1.4407003555
## 90   3  0.096524620 -2.9356620867
## 91   0 -2.318630145 -0.8602755607
## 92   1 -0.460478915  0.2687285592
## 93   0 -1.015635163  0.5786744250
## 94   2 -0.941725972 -0.5163637357
## 95   0  1.374929255 -0.3769604850
## 96   2  0.142114051 -0.7870576992
## 97   1  0.474430604 -0.5081415260
## 98   3 -1.481425557 -0.8284689585
## 99   2  1.511090786  1.5742255205
## 100  3  0.104603461 -1.4670705155
## 101  2  2.107815210  1.1581839830
## 102  3  1.577381220 -0.0743208252
## 103  2  0.061495037  1.5370403018
## 104  1 -0.237928991  0.2018475041
## 105  3 -0.177306880 -1.8123656409
## 106  3 -1.173930250  0.6893459114
## 107  3  3.588863620  0.0013457613
## 108  3  1.228347792 -0.7433407589
## 109  3  0.962649782 -0.7064042245
## 110  3  0.180371258 -0.2612517895
## 111  1  0.150124761  0.0345982071
## 112  1  0.411332810  0.7586974571
## 113  3  1.991414339 -1.3300327955
## 114  0  1.371634993  2.0798342118
## 115  1  0.020864792  0.2404726316
## 116  0 -1.871299781  1.3818801858
## 117  0  0.126354954  0.2507755139
## 118  1 -1.010615462 -0.2910730697
## 119  1  0.381258560  0.1804508747
## 120  1  0.752840952  1.1078881745
## 121  2  0.364333619  0.0384162327
## 122  0 -0.761696922  0.3812794428
## 123  0 -0.202280007  0.4180369964
## 124  1  1.648818143 -1.1148163219
## 125  1  1.462882090  0.6943157137
## 126  1  1.100018171  1.0133840561
## 127  1  0.119542189  0.1605084855
## 128  0  0.643362023  0.6868052245
## 129  1 -0.573969319 -2.3341781004
## 130  2  0.117342463 -0.6020887829
## 131  0  0.928531810  0.9675174899
## 132  0  0.200974251 -0.1906277041
## 133  0 -0.114127492  1.6514133180
## 134  0 -0.556948871  1.6034342998
## 135  2 -0.049907559 -0.0511056535
## 136  1 -1.290680650  1.1109700579
## 137  0  0.341245762  0.1372385277
## 138  3  1.691552886 -1.6733998862
## 139  2 -0.209424213 -0.5414727978
## 140  1  0.139773990  0.0245084977
## 141  1 -1.813238132  0.1289869070
## 142  2  0.313815693  0.3457636860
## 143  3  0.925729730 -0.7398364180
## 144  2 -0.272497294 -0.3675826843
## 145  2  0.295403893 -1.6412058428
## 146  3  1.598596731 -1.6590507805
## 147  2 -0.810997210 -0.0144685403
## 148  2  1.164225335 -0.3978919311
## 149  1 -0.027070198  0.4388917973
## 150  2 -0.736605669 -0.2426368844
## 151  0  0.211748906  0.5361508145
## 152  3  0.165769380 -0.2691957989
## 153  2  0.633404379 -0.1871651036
## 154  2 -0.952997751  0.1208084781
## 155  1 -0.591738429 -0.5686464457
## 156  2 -0.001054059 -1.3025553586
## 157  1 -0.044081368 -0.2645799165
## 158  2  0.066236432  0.3331088253
## 159  1 -0.389770559  0.1646337295
## 160  2  0.395706267 -0.2267665116
## 161  1 -0.146017270 -1.0230701913
## 162  2  1.650655865  0.4313800187
## 163  2  1.769587065  0.6370931578
## 164  1  0.084518056 -0.2249036890
## 165  2  0.742528878 -0.4879677206
## 166  2  0.237715550 -0.2126991773
## 167  2  0.101607289  1.1304505053
## 168  2  0.347338566 -1.2807180334
## 169  2  0.538617823 -0.6218756577
## 170  2 -0.202315624 -0.0467691540
## 171  3 -0.571915749 -2.2706122170
## 172  0 -1.718857792  1.1541763293
## 173  1 -1.532665236  0.0630502203
## 174  3  1.185474067 -0.0499948435
## 175  1 -1.087051634 -1.0182247557
## 176  0 -1.488662846  1.2276775010
## 177  0  0.540122680  2.1761754925
## 178  3  1.544643798 -0.1797161222
## 179  1 -0.472596287  0.2926195635
## 180  1  0.684270615  1.7089978858
## 181  3  0.935778426 -2.0936682036
## 182  1  0.783239496  0.3639423571
## 183  1 -0.601157977  0.2225911747
## 184  1 -0.258836888 -0.8634953435
## 185  1  0.709447295 -0.1078303013
## 186  1  0.203060669  0.2235897745
## 187  3  1.000924608 -0.3272079438
## 188  2  0.692991032 -1.0773107761
## 189  1  1.083609483  0.8516170628
## 190  0  1.039306422  0.7750640190
## 191  3  0.510516916 -0.5186636842
## 192  1  1.355168933  0.5891240785
## 193  2 -1.027144382 -0.4834804009
## 194  2  0.246106681 -0.4110153001
## 195  1 -1.891132460 -0.9128976462
## 196  1 -1.220067278  1.0847565653
## 197  1 -0.371108690  1.9830493858
## 198  2  1.265776720 -0.1143667696
## 199  2 -0.552173869 -0.1904703379
## 200  1 -0.255705293 -0.4962745406
## 201  1 -1.126517125 -0.4014863473
## 202  1 -2.344596199 -0.1982957922
## 203  2 -0.626160661  0.7589008397
## 204  1 -0.679150879  1.6546709752
## 205  2 -0.066219987  0.9393701742
## 206  0 -0.147113996  1.6746540078
## 207  2  1.495052638 -0.0527085370
## 208  1  0.128032494  0.5220619870
## 209  1 -1.021653863  0.8530051127
## 210  3  2.242174318 -0.6001719429
## 211  1 -0.520155709 -0.2228470886
## 212  2 -1.044579604 -0.3107994241
## 213  2  1.650230049 -0.1912014918
## 214  1  0.367623561 -0.0022777662
## 215  1 -1.225658720  1.2457482687
## 216  2  0.311369386 -0.3873731141
## 217  2  0.663149689 -0.7384071156
## 218  2  0.140088283 -0.0504444780
## 219  1 -0.316826386  0.2462239988
## 220  0  1.431859658  1.0246022439
## 221  1 -1.101002525  0.2306828909
## 222  2  0.570653993 -0.6475681266
## 223  1  2.371032575  0.8790763529
## 224  2 -0.946765883 -0.2971783448
## 225  1  1.242373062 -1.8345017515
## 226  0 -2.403822363  0.7724122701
## 227  2  0.611163004 -0.4387997065
## 228  2  1.512478019  0.1082290685
## 229  1  1.138495699  1.3095907994
## 230  2  0.392135593 -0.8637992478
## 231  1 -0.358206431 -0.1248412020
## 232  3  1.467920645 -0.1827177267
## 233  2 -0.789002682 -1.3900967601
## 234  1  0.581266541  0.6651547216
## 235  1 -0.203285960 -0.5173754943
## 236  2  0.197207481  0.0342964731
## 237  1 -0.525734777 -0.5685190819
## 238  1  0.221721448  0.9030708521
## 239  2 -0.266085633 -0.0006366187
## 240  3 -0.423359965 -1.4636295223
## 241  1 -1.651769306  0.9680956611
## 242  0 -1.088323505  0.9128491837
## 243  1 -0.417967384  2.8166845342
## 244  0  0.546927533  1.6083465260
## 245  1 -1.141264233 -0.6387380218
## 246  1 -0.482196989  0.9084137883
## 247  1  0.126966426  1.2155906989
## 248  2  0.223426731  1.0615898853
## 249  0 -1.084927947 -0.6956682186
## 250  3  0.497683320 -1.3326605409
## 251  2  0.916816165  0.3918370303
## 252  0 -1.682629756  0.9470407206
## 253  2 -0.061595979 -1.0260742674
## 254  1  1.837832619  1.0478033291
## 255  1 -1.241511176 -0.1815797281
## 256  1  1.303841546  1.1981343005
## 257  1 -0.865477256 -0.1450188749
## 258  2 -0.848477596  0.1517656006
## 259  1 -1.362924908  0.9888682478
## 260  2 -0.959521317 -1.5005609075
## 261  1 -1.234465045  0.0462969369
## 262  0 -0.324802292  0.7220434573
## 263  3  1.454729648  0.2146973179
## 264  1 -0.201024365  3.1294775494
## 265  1  0.189317112 -1.4297843589
## 266  2 -0.426470244  0.5148981737
## 267  1 -0.696107831  0.3650361601
## 268  1  0.507062603 -0.1559844043
## 269  2  1.388364799 -0.6766828427
## 270  3  0.277258980 -1.7475245660
## 271  1 -1.057322099  1.0996944139
## 272  2 -0.002027694 -0.3990561178
## 273  2  0.679518674 -0.0323688662
## 274  1  0.174794866  1.2861445585
## 275  1 -1.891113439 -0.1035455873
## 276  2 -0.701134774 -0.7528467267
## 277  1 -1.354322643  0.5545736245
## 278  2  0.145935791 -0.3441961668
## 279  2  0.966425238 -1.2685545759
## 280  0 -1.729407348 -0.4426044206
## 281  3 -0.511609604 -1.3668741377
## 282  2 -0.897319433  0.3385750757
## 283  2  0.178128154 -0.8534785085
## 284  2 -1.318083401  0.1229668062
## 285  2  1.123172464 -0.2830647968
## 286  1  0.866385977 -0.1702938815
## 287  2 -0.584307117 -0.2731080493
## 288  2  1.309773700 -0.7988456562
## 289  0  0.188778220  0.4967566586
## 290  3  1.640781629 -0.8348391427
## 291  2 -1.562401201 -0.3787161360
## 292  1 -0.039035998  1.7029331425
## 293  1 -0.491398394  1.1484027387
## 294  2  1.497166848  1.1301851302
## 295  0  1.000994615  2.1058589234
## 296  2  1.121642301  0.2212350174
## 297  3  1.002034201 -0.6185810392
## 298  2  0.225411791 -0.3976644236
## 299  1 -1.434944540  0.2407873739
## 300  0  0.230324977 -0.1618788733
## 301  0 -1.383747877 -0.2914657083
## 302  2 -0.069401742 -1.0625051084
## 303  1  0.344476669 -0.0722446508
## 304  2  0.656036457  0.3012495921
## 305  0 -0.524358751  0.2220284421
## 306  2 -0.330639126 -0.3321396250
## 307  2 -0.664357012 -0.1384882742
## 308  3  2.596619832  0.6524230327
## 309  2  1.415170747  0.1330598132
## 310  2 -1.448122285  0.4414066723
## 311  3  0.154242726 -1.1716034221
## 312  2  0.596145133 -0.2257391598
## 313  3  1.085261130 -0.2176974296
## 314  1  0.406754218  1.1374709873
## 315  3  0.763383022 -2.3212354727
## 316  3  1.090659505 -0.6102739529
## 317  2  0.929214778 -0.2046252815
## 318  1  0.055559121  0.7830786656
## 319  0  0.368043419  1.2971808434
## 320  0 -0.029599691  1.2925907150
## 321  2  0.233738005  0.9966640243
## 322  2 -0.588377280 -0.2904604908
## 323  1 -0.727678045 -0.4079342019
## 324  2  0.011433459 -1.2292039919
## 325  2  0.493265817 -0.2889819522
## 326  2 -0.308421979  0.7661425002
## 327  3  0.179415112 -0.2561331419
## 328  0  0.060401603  2.0792886560
## 329  3  0.129756463 -1.9336536692
## 330  0 -0.772296784  1.4914941777
## 331  2  0.786894269  0.6318792541
## 332  0 -0.369021776  1.1295296155
## 333  1 -0.252862384 -0.1982282792
## 334  0 -0.308473566  0.2899468748
## 335  0 -0.884947542  1.2478162379
## 336  3 -0.163547374 -1.1508025842
## 337  1 -0.195671120 -1.5809618777
## 338  2 -0.035855237  0.4424676913
## 339  1 -0.458136963  0.1509924723
## 340  1 -1.085347624 -1.1790346957
## 341  2 -0.521112963 -0.8172783585
## 342  1 -0.762024256  0.0620021077
## 343  2  0.247243512 -0.9395419623
## 344  2  0.005889943 -0.4387890901
## 345  2 -2.002508280  0.1275953778
## 346  1  0.516109384  0.3122484931
## 347  2  1.201271507  1.1447083574
## 348  1  1.372272805 -0.5290991118
## 349  0 -1.800707286  2.1103127597
## 350  3  0.996206764  0.5378313184
## 351  1  0.957241550  0.8682172361
## 352  2  0.427685928 -0.9429723774
## 353  2  0.161682429  0.2258050492
## 354  2 -0.948137594 -1.3698842084
## 355  1  1.670565505  1.0423630646
## 356  3  0.207088914 -1.4042803289
## 357  2 -0.734102751  0.3722356844
## 358  1 -2.043354765  0.0344591255
## 359  1  1.315370344  0.7474260137
## 360  1  0.611482197  0.7420816206
## 361  3  1.176504577 -0.2991004047
## 362  1 -1.588547574 -0.5374548734
## 363  3  1.147730735 -0.5815223422
## 364  2 -0.982543971 -0.6822769638
## 365  1  0.716812474  1.4144375868
## 366  3  1.361762769  0.0123138380
## 367  2  0.657769378 -0.1842769958
## 368  2  0.768366266 -0.3524672034
## 369  0 -0.593686262  0.3070302370
## 370  2  0.099812973  0.1396188841
## 371  1  1.197102556  0.8760085862
## 372  2  0.838406075  1.3835206849
## 373  3 -0.019004602 -2.4030858801
## 374  3  0.852191012 -1.3657782929
## 375  1 -1.780276834 -1.1291317622
## 376  2  0.838113485  1.8420890397
## 377  2  0.219027938 -1.1637936334
## 378  1  0.316191805  0.0377891594
## 379  2 -0.011150070 -0.7629717657
## 380  1  0.994944794  0.4016396489
## 381  2 -0.382667705 -0.0924243164
## 382  3  0.330521742 -0.8696242818
## 383  2  1.646419990  0.0233220354
## 384  3  1.037809576 -0.8031871813
## 385  0 -1.115759947  0.2902240203
## 386  2 -0.064651688  0.2528567047
## 387  1  0.542419506  0.4644182806
## 388  1 -1.169729088 -0.7085774942
## 389  1 -0.508749156 -0.9890692292
## 390  1  0.323401366  0.5607592706
## 391  2 -0.315807688 -0.6903107092
## 392  2 -0.962032554  0.4171439175
## 393  2 -0.327324323 -0.6725054507
## 394  2  0.339754939 -0.5055969362
## 395  1  1.425655534  0.4208856947
## 396  1 -0.244432278 -0.1884356869
## 397  0 -0.430711415 -0.3257052386
## 398  0 -1.131643431  1.0129443477
## 399  1  1.174496003  1.5313665931
## 400  3 -0.678011370 -1.5295476562
## 401  2  0.760897568  0.2372825958
## 402  2 -0.694724193 -0.6664928109
## 403  3  0.563951420 -3.1373975592
## 404  2  0.706009473 -0.2045496053
## 405  1  0.418489925  1.3903357850
## 406  2  0.907279893  1.6011365310
## 407  1 -1.912632501  0.2709225131
## 408  1 -0.036184130  0.3164548242
## 409  0 -1.183625875  0.4236535882
## 410  1 -2.493075871 -1.0203512893
## 411  3 -0.625920939 -0.1492783357
## 412  2 -0.713340451 -0.2087001877
## 413  0 -0.675138597 -0.0250119012
## 414  1 -1.640296819  1.0658733258
## 415  0  0.994698799  0.0075030480
## 416  1  0.199175001 -0.1342626452
## 417  3 -0.666639958 -3.0766468216
## 418  2 -1.998617476 -1.3379059481
## 419  1  0.372550970 -0.0393467663
## 420  1 -0.587443654 -0.9673344718
## 421  3  1.826319037 -0.6476545241
## 422  3  1.564313462 -1.5447880552
## 423  1 -0.303319087 -0.4561402512
## 424  2 -0.111510312 -0.2500316719
## 425  1 -0.109880801  1.5037069500
## 426  1 -0.152978069  0.3079068300
## 427  0 -0.155910618  1.1514774964
## 428  1 -1.109802987 -0.2372747290
## 429  3 -1.186931948 -1.0114879654
## 430  0 -0.829960125  1.2028121957
## 431  2  0.902516626  0.4216035680
## 432  3  1.107167251  0.0403694516
## 433  1 -0.569376949  1.0446123891
## 434  3  0.865444707 -1.0959308326
## 435  3  0.781761225 -0.8518476471
## 436  1  0.595352225  0.4423591949
## 437  1 -1.349808568  1.3558171306
## 438  1 -0.243977880  1.4749154906
## 439  1 -1.111429839  0.4466717983
## 440  0 -1.363609430  0.8381723406
## 441  2 -1.126224207  0.3399234672
## 442  1 -0.621020470  0.4320511517
## 443  2  0.546406766 -0.7553190747
## 444  0  0.460426776  1.5505241517
## 445  1 -1.451258437  1.7004143604
## 446  3  1.212232412  0.1593573102
## 447  3 -0.147771941 -1.3331776875
## 448  3  0.044185801 -0.0801151490
## 449  1  0.143687430  0.3433882127
## 450  2 -0.595676682 -1.1817283842
## 451  1  0.142798986  0.2360416067
## 452  0 -0.960733691  1.7263631646
## 453  3  0.038148670 -1.6383479993
## 454  0 -0.307455234  0.8796551324
## 455  1  0.464777608  0.1049204057
## 456  3  0.525145781 -1.6181629750
## 457  3  0.683763602 -0.6927334574
## 458  3  1.015386484 -1.9596750449
## 459  3  0.731253993 -1.1895125642
## 460  2 -1.839042001 -1.7823744129
## 461  0  0.139866692  0.3415450064
## 462  3  1.107623268 -2.3508392208
## 463  1 -0.778075964  0.3041747042
## 464  1  1.031920416  0.4157431602
## 465  3  0.955775187 -0.0990427511
## 466  2 -0.622836087 -0.2744353052
## 467  2 -1.230100275  0.2218667609
## 468  2  0.551003074  1.0960383128
## 469  2 -0.133156032  0.4378493491
## 470  3 -0.024506994 -2.0948389905
## 471  1 -0.548861738 -1.2349624277
## 472  2 -1.106527009  0.2665336473
## 473  3  1.921444715  0.4180429301
## 474  0 -0.433422074  0.2654674464
## 475  3  1.527757751  0.3185388347
## 476  3  0.707038244 -0.5273059970
## 477  0 -1.374071743  1.2782806186
## 478  3 -0.343112443 -0.5199721089
## 479  2  0.675230454  0.2540597373
## 480  1 -1.236977229 -1.2452814307
## 481  3  1.326901895  0.3255359173
## 482  1  1.522870664  1.4042606126
## 483  1 -0.771390928  0.3305792052
## 484  2  0.910521739  0.1586269729
## 485  1 -0.157208027  0.9341790637
## 486  1 -0.850168902  0.0917690735
## 487  0 -1.013922116  2.3172744690
## 488  2 -0.315774785  0.1955507744
## 489  2 -0.404785210  0.3381478942
## 490  3  0.529830411 -1.0720777139
## 491  0 -1.888347092  0.8706856947
## 492  1 -1.476027061 -1.1410932554
## 493  3  0.586269916 -0.3692248415
## 494  1  0.533632329  1.4592296057
## 495  3  0.197778910  0.6096695091
## 496  3  0.350178906 -0.8530746742
## 497  1 -1.161435512 -1.0603275415
## 498  1 -0.383764974  1.0078615233
## 499  2 -0.286049436  0.0512463750
## 500  2  2.066974994  0.2665618745
## 501  1 -0.312800686  0.0098667075
## 502  0 -0.942445109  0.1970982740
## 503  3  0.039916735 -1.5429334874
## 504  2  1.962572520  0.4708707368
## 505  2 -0.250574995  0.4563302997
## 506  2  0.722590203  0.3951421921
## 507  2  0.809795728 -0.5578475190
## 508  1  0.342792559  0.7398180435
## 509  2  0.634972571 -0.1791015658
## 510  0 -1.010903357  0.1476444688
## 511  0 -0.324569745  0.2811973610
## 512  2 -1.783488832 -1.0096247735
## 513  2 -0.323493726 -0.1117925344
## 514  2 -0.152603101 -0.7160975207
## 515  2  1.646619302  0.0117893437
## 516  3  3.321750006 -0.0158612925
## 517  2  1.778849682  0.3416271171
## 518  0 -0.571036855 -0.3131671271
## 519  3 -1.070831346 -1.7026060088
## 520  0 -0.255244095  0.3254998769
## 521  2  0.969769510  0.3356158915
## 522  2  0.834679346  1.0097558636
## 523  2 -0.106177936 -1.6563106909
## 524  1 -0.587770087  0.2069798869
## 525  2 -0.347421319 -0.2445164043
## 526  1 -1.442279496  0.6901196145
## 527  1  0.465210541  0.2496020945
## 528  3 -0.596846669 -1.0538326704
## 529  2 -0.779662463 -0.7594309763
## 530  3 -0.139174614 -0.4477550436
## 531  2  0.984620722 -0.6584037315
## 532  2  0.784975379 -0.6618499398
## 533  1  1.391583370 -0.6733439043
## 534  0 -1.174369334 -0.1554386937
## 535  1 -0.527162835 -0.4763126122
## 536  1 -1.428169056  1.8872931057
## 537  3  0.995261349 -1.1678820576
## 538  2 -0.006026548 -0.7399035192
## 539  1 -0.252939096 -0.5824428227
## 540  2  0.875745095 -0.0994590434
## 541  2  1.908966603  0.3156493456
## 542  2 -1.055267412 -0.5139303122
## 543  3  0.576483312 -0.4868125041
## 544  2  0.300493413 -0.1651078578
## 545  0 -1.423919257  1.2313531063
## 546  3  0.270897698 -1.7183930697
## 547  0 -2.419993826  0.7544897945
## 548  2 -1.919506371 -0.8944597772
## 549  2  0.605242343  0.3927772620
## 550  1 -0.526807018 -0.8245702946
## 551  2  1.466711073 -1.7660204125
## 552  2  0.088940098  0.4313471018
## 553  0 -1.998373408  1.1952101726
## 554  1  1.418267235  0.9619694188
## 555  2  1.064937847 -0.0802636552
## 556  3  0.152235137 -1.4476457646
## 557  0  0.028487935  0.7219713060
## 558  2  1.148838070  0.0015415896
## 559  0 -0.152388251  0.5642084496
## 560  0 -0.813823255  0.7514069488
## 561  3  1.525772153 -1.1092631434
## 562  2  1.025362294 -0.3932092388
## 563  1  0.002484232  0.1072902574
## 564  2 -0.140110212 -1.1747769331
## 565  1 -0.778833593 -0.3246136917
## 566  2  0.802121233  0.2964386763
## 567  1 -0.633587580 -0.1016264350
## 568  3 -0.451925804 -0.3285187468
## 569  0  0.286050680  0.3011922521
## 570  0 -1.246935715 -0.1728108584
## 571  0  0.079394639  1.0252294855
## 572  1  0.312538363  0.1214451181
## 573  2  1.394593550  0.3383660960
## 574  3  0.560256838 -2.7493024884
## 575  2 -0.232556467  1.4703236099
## 576  1 -1.023195541  0.5471066953
## 577  2 -0.324271786 -0.4764997173
## 578  3  2.444461423  0.5426422488
## 579  1 -0.248644192 -1.0614531847
## 580  2  1.093262524 -1.6449469418
## 581  2 -0.102702995  0.4871174499
## 582  0 -0.547248776 -0.7463700722
## 583  3 -0.046932245 -0.0610250193
## 584  2  0.385097054 -1.7092156902
## 585  1  0.735896944  1.6435517271
## 586  2  1.127058288 -0.0869363873
## 587  3  0.775161202  0.7614775797
## 588  3 -0.658158633 -0.0250671541
## 589  2  0.942451736 -0.2349041208
## 590  0 -0.969773679  2.0639657283
## 591  3  1.252283024 -0.7113871126
## 592  1  1.261747097  0.1436103966
## 593  0 -0.509775809  0.7876595332
## 594  1 -0.450529916  1.1544180122
## 595  0 -1.245120786  0.0984620640
## 596  1 -0.087556537  0.1830859467
## 597  1 -0.059972643  0.9871375290
## 598  1  0.007846240  0.0485836403
## 599  1  0.545948813  1.6900250635
## 600  2  0.748513567  1.0601875391
## 601  3 -0.952233980 -2.1562633411
## 602  2 -0.106843117 -1.2176273822
## 603  0 -1.147043799  2.2171303645
## 604  1 -0.744372049  0.0884592306
## 605  1 -0.068259959 -0.4139664939
## 606  3  0.585033845  0.3274532251
## 607  3  0.484726347 -0.7520305742
## 608  2 -0.741019216 -1.5380951864
## 609  2  0.940228165 -0.3247523130
## 610  2  1.442164809 -0.6138807524
## 611  2 -0.713069460  0.0433017079
## 612  1 -0.829679519 -1.1051560842
## 613  1 -0.756559363 -0.3204780454
## 614  1  1.368440989 -0.0678529060
## 615  2 -0.699962273 -0.3344114044
## 616  2  0.918545573 -0.5266180399
## 617  0  0.111579902 -0.1731102007
## 618  0 -0.442233361  0.1424826831
## 619  0 -0.552346392 -1.0413303570
## 620  2  0.229161720  0.6404344031
## 621  3  0.487007408  0.5045272321
## 622  1 -0.452275066 -0.7333201639
## 623  1 -0.178254750  1.0275915931
## 624  2 -0.802921866 -0.3572766789
## 625  0 -2.580746860  0.6613454132
## 626  2 -0.136129855 -0.5529633158
## 627  1  0.428418133  1.1581065283
## 628  2  0.607351322 -0.5720486011
## 629  2 -0.226911469 -0.3679590516
## 630  2  0.032140582  0.5341914534
## 631  3  1.841294232 -0.2688584570
## 632  3  1.500559887 -2.4559267466
## 633  2  0.236769735 -0.2087047681
## 634  1  0.288313446  0.2456963685
## 635  2  0.852209142 -1.4830235764
## 636  1  0.212781606 -0.3631437086
## 637  1 -0.445802941 -0.9367348056
## 638  2 -0.165943582 -0.5930475420
## 639  1 -0.749561150  0.0586114150
## 640  1 -1.067320558  0.4218223418
## 641  3  1.346636043 -1.1813532109
## 642  0  0.244022415  0.3131880208
## 643  0 -1.693149584 -0.0348897685
## 644  1 -0.188276504  0.0280114673
## 645  1  0.080890898  1.7129049278
## 646  1  0.851024128 -1.4290945772
## 647  1  1.404213994  1.8742806196
## 648  0 -0.890486649  0.5553507014
## 649  2  0.231345893  1.5728868058
## 650  1  0.845617941  1.2516763104
## 651  2 -0.348565148 -0.3948772940
## 652  2 -0.903204349 -1.0983071256
## 653  3  2.287994469 -0.5617109266
## 654  0 -0.612165676  2.5422369645
## 655  1 -0.214087708  0.6854910904
## 656  2  0.523385492  1.1821764464
## 657  1 -2.660847391  0.8886135564
## 658  1 -0.593229269  1.0298018992
## 659  2 -0.751117037 -1.1018169184
## 660  3 -0.215971765 -1.1284218891
## 661  2  0.667086103  0.2208826084
## 662  0 -0.163820252  0.7950206699
## 663  0 -0.807213449 -0.1824759571
## 664  1 -0.046751226  0.0402530331
## 665  3  1.112053011 -0.3290092812
## 666  0  0.328961289  1.7436648426
## 667  3  1.594732986 -0.0934645341
## 668  3  1.752821410 -0.4487711975
## 669  2 -0.662089215 -0.2686525614
## 670  2 -1.776066438 -1.9264210371
## 671  2  0.807321469  0.5776376298
## 672  0 -0.250598409  0.2079121989
## 673  2 -0.827607251 -0.0531276029
## 674  2 -0.916105363 -0.9583290385
## 675  3 -0.135495056 -3.2389693345
## 676  0 -0.492071634  1.8488697731
## 677  1  0.105267128  0.8815750487
## 678  2  0.102391410 -0.1201438523
## 679  1 -1.209502354  0.2477179284
## 680  1 -0.822299447  0.5667137861
## 681  3  1.292325276 -1.3619057791
## 682  1 -0.052539101 -0.6594943615
## 683  0 -0.462243753  1.3899172457
## 684  1  0.352380837  1.2541618787
## 685  0  1.107975180  1.1009015038
## 686  2 -0.074165599 -0.2921588849
## 687  1  0.280355150  0.3180723851
## 688  0 -0.356295271  1.1339654474
## 689  2 -0.136800843 -1.3498238815
## 690  1 -0.316614728 -0.6925728874
## 691  1 -0.890544519  0.3143646247
## 692  2  1.804005596 -0.4101050774
## 693  0 -1.025921080  0.7772179142
## 694  2 -0.468385305 -0.3244348904
## 695  2  0.294345063 -0.3691218547
## 696  3  2.460680881 -2.1681163555
## 697  2  0.529003870  1.4305424024
## 698  1 -1.180418305  0.6350535305
## 699  0 -0.907425403  1.4248426597
## 700  1 -0.004536711  0.0780453827
## 701  1 -0.850661624  0.8045016673
## 702  3  0.941861898  0.2091101408
## 703  2 -0.216050254 -0.5970589855
## 704  3  1.234129203  0.1840117150
## 705  0 -0.558343392 -0.8231636381
## 706  3  1.142361246 -1.9658943948
## 707  1  0.510516604  1.0529581432
## 708  0 -0.587162523 -0.1340075853
## 709  3 -1.270717392 -0.7658317089
## 710  2 -0.105789883 -0.2672859147
## 711  2 -0.597206206 -0.9645907984
## 712  2  1.117962012  0.9275110404
## 713  0  0.975340197  2.6098083340
## 714  1 -0.516857624  0.1674058844
## 715  2  0.360589058  0.6860729861
## 716  2 -1.360920497 -0.9970799443
## 717  2 -0.590928726 -0.9939514988
## 718  3 -1.360113585 -0.5030276609
## 719  3  0.893248402  0.5933670941
## 720  1 -1.689700112  0.4442880866
## 721  1 -0.233519419  0.9846295597
## 722  1  0.842171738  0.6597493854
## 723  0 -1.094595307  1.6301515003
## 724  3 -1.117283417 -1.5349839894
## 725  1 -2.320957864  0.1346737142
## 726  3  1.145094243 -1.7756057219
## 727  2  0.576815914 -1.6077107062
## 728  2 -0.243267457 -0.4961363482
## 729  1 -0.871059340  1.4421577602
## 730  2 -0.748624485 -1.1586595401
## 731  0  0.108822907  1.8937624534
## 732  0 -1.747265048  0.5639525600
## 733  3 -0.069382068 -2.0309049727
## 734  2 -0.439184290  0.1971404061
## 735  1 -0.628587181  0.4452841693
## 736  2 -0.474203636  0.0067974012
## 737  0  0.095332274 -0.0135037712
## 738  2  0.017804010 -0.7752036327
## 739  1  0.170516930  0.5915166747
## 740  0 -0.489743583  2.0977124809
## 741  1 -0.252234588  0.1926983330
## 742  1 -0.724823363 -0.2919388362
## 743  2 -0.033007548 -1.0608449989
## 744  1  0.295448628  0.7902191504
## 745  2 -0.688058910  0.0759074664
## 746  0 -0.012930684  1.4598334949
## 747  2 -0.386598432  0.0363959467
## 748  1  0.767308744  2.5746014963
## 749  1  1.999494743 -0.2082200337
## 750  3  1.662275592 -1.1345461692
## 751  2 -0.586875951 -0.1477238387
## 752  2  1.115374633 -0.0345628906
## 753  1  1.249663437  1.2412991963
## 754  1 -0.050483515  0.9808238106
## 755  1  0.376956216  0.1588433672
## 756  1 -1.216135840  0.9129321741
## 757  1 -0.206590138 -0.1903209437
## 758  0 -0.529013255  0.9863334646
## 759  2  0.295819267  0.1747854154
## 760  3  1.555287474 -0.6756236084
## 761  3  0.360758487 -1.3612746401
## 762  2 -1.008177603 -0.5675964772
## 763  1  0.628285578  0.0983732388
## 764  0 -0.468843870  1.3680186497
## 765  2  0.057167928  0.2248556076
## 766  1  0.084723998  1.8046044521
## 767  0 -0.329834618  2.0435240685
## 768  3 -0.587851633 -0.7573732932
## 769  2  0.654707527 -0.0663373578
## 770  2  0.287011332 -0.7514858319
## 771  0 -1.363675352  1.3785523444
## 772  2  0.818613127  0.4010679579
## 773  1  0.686317863 -1.4883876641
## 774  2 -1.453312432  0.0336641816
## 775  2 -0.219859925  1.0844565008
## 776  1 -0.436085344 -0.8356391757
## 777  2 -0.255180703 -1.2787050386
## 778  0 -1.725691266  1.3447993140
## 779  2 -0.766394295 -0.3026916748
## 780  2 -0.261961456 -0.2229514581
## 781  1  1.725335427  0.9567038886
## 782  2  0.410601623 -0.3042290380
## 783  1 -0.796530451  1.2160704629
## 784  1 -1.808243480  1.5733148729
## 785  1 -0.501286387 -0.4453004569
## 786  1 -0.411386546  0.3643145017
## 787  2  0.614857077 -1.0511191865
## 788  1 -1.465352432 -0.3175399336
## 789  3  1.650989600 -0.1771652957
## 790  2  0.421686019 -0.7302273469
## 791  0 -0.879760220  0.0764822167
## 792  2 -0.215226190  0.1251928504
## 793  1  0.139387438  0.6877229045
## 794  3  1.319305391 -1.2435724124
## 795  2 -0.871352538 -0.4047977456
## 796  2 -1.000242131 -0.2536741553
## 797  3  2.267846694 -0.9443317571
## 798  3 -0.328791193 -2.1645787138
## 799  0 -0.942189139  0.6262126184
## 800  3  1.308024659  0.1744636307
## 801  1 -1.186833339 -0.6367815978
## 802  1  1.350556502  0.8002121234
## 803  3  0.002322956 -1.6017315522
## 804  1 -0.371995402  0.7034315082
## 805  0 -0.141542329  1.5354563224
## 806  0 -0.574274253  0.7120058818
## 807  3  1.799121157  0.2196725926
## 808  2  0.932121433 -1.6803393329
## 809  2 -0.494160415  0.4362485075
## 810  2 -1.867165194  0.0349293036
## 811  1 -0.592175503 -1.3246547332
## 812  3 -0.757040846 -1.5502174264
## 813  1 -2.535257363 -0.4353477108
## 814  1  1.334201493  1.2472322874
## 815  2  0.315902254 -0.8225862974
## 816  1  0.260620426  1.4518184452
## 817  3 -0.160487515 -0.9972706744
## 818  3  0.565198703 -2.0298036261
## 819  0  0.305010052  0.9338361215
## 820  2 -0.794151464 -0.7158858566
## 821  1  1.219844365  0.5506032993
## 822  3  0.531524861 -0.5754472911
## 823  2  2.851518453  0.1919298566
## 824  1 -0.012267677  0.0945574742
## 825  2  0.335686134 -0.0483419628
## 826  0 -0.429486198  0.1401653200
## 827  2  1.638294930  0.0342900028
## 828  0 -1.882266891  1.5821003605
## 829  2  2.058540917  0.5473289808
## 830  2  1.659255127 -1.3853535916
## 831  1 -1.634321140 -0.1938720402
## 832  3  2.795432601 -0.5140703350
## 833  3  1.995869722 -1.8026660712
## 834  3  1.181293484 -0.8503275250
## 835  1  1.339830311  0.8665795637
## 836  0 -0.394481949  1.1893588351
## 837  2 -0.223852604 -0.0836753277
## 838  1  2.009282530  0.4452384818
## 839  2  1.209962868  0.6519492196
## 840  3  0.231532738 -2.0663415294
## 841  1 -2.020551822  0.1377655908
## 842  1 -0.809984433 -0.2700229157
## 843  2  0.218042731  1.0116171702
## 844  1  0.703605986  1.0436292615
## 845  1 -1.569498743 -1.0368727572
## 846  1  1.595781001  1.3893188293
## 847  1 -1.414214544 -0.2854557504
## 848  2  0.987814288 -0.2138928521
## 849  2 -0.561368358 -0.2303858019
## 850  1  0.447739724  1.5794312688
## 851  2 -0.456439369  0.2679445063
## 852  1 -0.477335231  0.8893381410
## 853  2  0.541696888 -1.8039241594
## 854  1 -0.501260190  0.3049326773
## 855  3  1.735918161 -0.2841266602
## 856  3 -0.501299570 -1.0094838026
## 857  1 -1.148285928 -0.7030768970
## 858  3  1.592996867 -1.1879651729
## 859  2  1.345995204 -0.4101306939
## 860  0  1.258078921  1.0809560160
## 861  2  0.119402948 -0.8598137631
## 862  0 -0.602398821  2.9977168834
## 863  3  1.082012714 -0.5294040437
## 864  2 -1.008510239  0.0485075151
## 865  2 -0.132434600  0.4916625017
## 866  1  0.923687611  1.9677668544
## 867  0 -0.616536184  0.6040503757
## 868  0 -1.261224098  2.3006119088
## 869  1 -2.844671063  1.8314191575
## 870  2  0.155773177 -0.3304632796
## 871  1 -1.411668113 -1.4459612008
## 872  2  0.462410319  0.0886244163
## 873  1 -0.182025512 -0.5163367467
## 874  3 -0.581396433 -0.6054633379
## 875  0  0.589398471  2.1535829547
## 876  3  1.003791810 -0.8855718112
## 877  1  0.517907970 -0.9340994224
## 878  1  0.661740652  1.2342985865
## 879  3 -1.686704171 -0.6330612931
## 880  1 -0.079639795  0.0147299675
## 881  3  3.430144729 -0.4135930405
## 882  3  1.302405429 -0.6996461861
## 883  3  1.486930873  0.7297672086
## 884  1 -1.591626033  1.9708417395
## 885  3  1.135743822 -1.9836970404
## 886  2 -0.737530509  0.1362759048
## 887  3 -0.580765044  0.1753156605
## 888  1  0.315057854  1.3907287974
## 889  1  0.958657562  1.1189770627
## 890  1 -1.752213191 -0.4425888532
## 891  0 -0.611674159  0.2001298802
## 892  1  1.407442369 -0.4095511552
## 893  1 -1.153638002 -0.0957331807
## 894  2 -0.197327989 -1.3369252247
## 895  2 -0.683143075  0.3305498885
## 896  3 -0.161494951 -0.7965335437
## 897  0 -0.615041347 -0.1513595919
## 898  1  0.428165201  0.1406639098
## 899  0 -0.712704749  0.8008218288
## 900  1 -0.282043199  0.4783555664
## 901  2 -0.064586063 -0.1145154027
## 902  2 -0.185593725 -0.3640704727
## 903  3  1.298882694 -0.3741696177
## 904  2 -1.061604429 -1.9924000632
## 905  1 -2.065479648 -0.2843855235
## 906  2 -0.374614292  0.3566535380
## 907  2  1.338757572  0.4667078404
## 908  3  1.535673973 -1.4314471865
## 909  2 -1.449242407 -0.1137155069
## 910  2 -0.929706636 -0.3589016409
## 911  3  0.350713240  0.2182636414
## 912  0  1.413194568  1.8615751485
## 913  0  0.241756096  1.4513816247
## 914  0 -1.152139478  0.8946529098
## 915  2 -0.109465884 -1.2044255708
## 916  3  1.784095517  0.3154356398
## 917  3  0.847520280  0.9052007871
## 918  2 -0.940638876 -1.1734280566
## 919  2  0.054060762 -0.7700044565
## 920  1  0.204923713 -0.1718635750
## 921  1 -0.939125182 -0.0428822994
## 922  1 -1.111033964 -0.7594143278
## 923  0  0.068885587  0.6093922449
## 924  0  1.097538088  0.8658383664
## 925  1 -1.174269633  1.7277634732
## 926  1 -1.193280309  0.3169555039
## 927  3  0.542531332 -1.2080948273
## 928  1  0.340818919  1.3976906073
## 929  1  0.465076920  0.6577810619
## 930  0  0.415052076  1.1494950228
## 931  2  0.286412058 -0.5699033626
## 932  1 -0.117279818 -0.1259573302
## 933  2 -0.186260008 -1.9881919142
## 934  2 -0.396659857 -0.4857411567
## 935  1  1.095753384 -0.2147107234
## 936  2  0.587195160 -0.9207670710
## 937  2  0.896287755  0.7029408709
## 938  2  1.118489242  0.4060426454
## 939  1  0.407589400  0.5732846132
## 940  2 -1.305141450 -0.8592974570
## 941  2  0.267413779  0.4762560957
## 942  3  3.349483174  0.9287221546
## 943  3 -0.071239873 -1.5107083666
## 944  3  0.009537593  0.1124141823
## 945  0 -0.220732549  1.2349861123
## 946  2 -0.231897777 -0.3615572097
## 947  1 -0.620613410  0.6653022652
## 948  1 -0.214155637 -0.3388460939
## 949  2 -0.506184664 -0.5908993636
## 950  3  0.930275226 -0.7447347658
## 951  1 -0.913096390  0.8824021940
## 952  1 -0.225339847  0.8877323079
## 953  2  0.649313377 -2.2263662797
## 954  2  1.792458795  1.9786034846
## 955  0 -1.083192910  0.0588150954
## 956  0  0.202523207  1.2762613467
## 957  1 -0.479489174  1.5103217861
## 958  3  1.689483122 -0.2446991080
## 959  1  0.200450237 -0.1296412029
## 960  1 -1.780013303 -0.2226023994
## 961  2  1.553315945  0.0883712535
## 962  3  1.457373955  0.2616095111
## 963  1  1.204432022  1.2262990606
## 964  0 -1.682335905  1.6744767780
## 965  2  1.103290671  0.3213736902
## 966  0 -1.342641023  0.7237467027
## 967  1 -0.949769787  0.8645409072
## 968  2 -0.178438277 -0.9590012214
## 969  2  0.595154778 -0.6129009671
## 970  1 -0.705927623 -0.6252545713
## 971  1  1.071136396  0.9327501962
## 972  3  1.282629868 -0.0773608868
## 973  1  1.006988832 -0.6052582305
## 974  2 -0.207164043 -0.6734737628
## 975  0 -0.554378101 -0.8376752495
## 976  2  1.660817490  0.4172316542
## 977  1  1.159371115 -0.2817719253
## 978  1 -0.326536753  0.9506041497
## 979  0  0.068789456 -0.9039746450
## 980  3 -0.963176574 -2.1718547837
## 981  0 -1.087669916  2.4349317504
## 982  3 -0.411097438 -1.7447195550
## 983  0 -0.719904445 -0.3539232506
## 984  2  1.303931305 -0.4757480922
## 985  2 -0.274320897 -0.4813934966
## 986  2  0.399319649 -0.8929762209
## 987  0 -1.012393728  0.2431678367
## 988  0 -0.246792425  0.8951456095
## 989  0 -1.122393762  1.5183013499
## 990  0 -0.349712494  0.6299440486
## 991  2  0.769647541  1.5214332009
## 992  2  0.853623410 -0.7824225989
## 993  3  0.869706597  0.0348252137
## 994  3  0.238187741 -2.0374751209
## 995  0  1.596331886  0.2549414068
## 996  0 -1.343612777  1.4167703485
## 997  2 -2.378763537 -2.0870849166
## 998  1 -0.012977238 -0.4160263901
## 999  2 -0.433525509 -1.0128290023
## 1000 3 -0.175659012 -0.1728509895
## 1001 1  1.033255346  1.1925899552
## 1002 3  2.237911316 -1.8685030149
## 1003 2  0.184350407  0.2304617179
## 1004 2 -1.093967820 -1.8279800092
## 1005 2 -0.699733503 -0.5963906704
## 1006 1 -0.618056791 -0.7830221613
## 1007 3  0.246816633 -1.6979689022
## 1008 0 -0.863239517  0.4664770894
## 1009 0  0.213778784  1.2516629209
## 1010 1 -0.804827200 -0.9980026644
## 1011 1  0.098477693  0.0688739692
## 1012 2  1.437040787 -0.6765505549
## 1013 0 -1.503153839  0.1866409522
## 1014 0  0.753934566  1.1069337414
## 1015 0 -0.620405947  1.4271549110
## 1016 2 -0.815256728 -1.1638306639
## 1017 0 -1.654510375 -0.0178147984
## 1018 3  1.406075136 -0.3401745642
## 1019 0 -0.510287248  0.7439581089
## 1020 3  0.543518905 -0.2960928539
## 1021 2  0.728010552 -1.0326480344
## 1022 0 -1.174739008  2.8391108449
## 1023 3 -0.404025225 -2.1301877889
## 1024 1 -0.037505498  0.2344802283
## 1025 0 -0.341988920 -0.1176968738
## 1026 1  0.112765992  0.3438832159
## 1027 1  0.058654496  1.5327663767
## 1028 3 -0.496644726 -1.9354295675
## 1029 2  1.124574861 -0.5424325508
## 1030 1  0.096877664  0.6871984243
## 1031 2  0.089966117  0.3752476135
## 1032 1  0.387691070  0.3915902020
## 1033 3  0.346484103 -1.0869303656
## 1034 0 -0.375088200  2.8401999178
## 1035 0  0.085023460  1.1292295411
## 1036 2  0.563859552  0.0490288226
## 1037 3 -0.419550211 -0.6532717523
## 1038 1  0.247958498  0.1667420173
## 1039 3  0.488841278 -1.4664145849
## 1040 0 -1.029121978  1.2462419153
## 1041 2  1.577485334  1.0539147601
## 1042 0 -0.181809676  0.5739916887
## 1043 1 -0.814404435  1.2268783512
## 1044 0 -1.105805431  1.7007533454
## 1045 1  1.115993723  1.6671441199
## 1046 2  0.676095218 -0.3187054461
## 1047 0 -2.383931699 -1.2600471059
## 1048 0 -1.178446955  1.3694946573
## 1049 2  0.474381549 -1.1507800909
## 1050 3 -0.958338221 -0.5112049181
## 1051 1 -1.737086726 -0.2800864680
## 1052 1 -0.344989589  0.9012416403
## 1053 1  0.805807118 -1.0177807587
## 1054 1 -0.918997222  0.2202480911
## 1055 0  0.065254315  0.9877796435
## 1056 1  0.425738494 -0.1365176777
## 1057 0 -1.091675631  0.5933529033
## 1058 2  0.739637446  0.9108602655
## 1059 2 -1.436276543 -1.0411326761
## 1060 2 -0.014889231 -0.2875248120
## 1061 2  0.416629077 -0.5057366755
## 1062 2  0.390359012  0.5202570590
## 1063 3  0.712559009 -0.8332292434
## 1064 2 -0.213194159 -0.5408986379
## 1065 2 -1.484463945 -1.4510827323
## 1066 2  0.467055320 -1.3020965631
## 1067 3 -0.037547760  1.0407475130
## 1068 1 -1.249119227 -0.1436028603
## 1069 2  0.490431748  0.1665537932
## 1070 1 -0.197686154  1.3047714906
## 1071 2 -0.156177699 -0.6616460847
## 1072 1 -0.647404453  0.9116021415
## 1073 2  0.531628761  0.3674620581
## 1074 2  1.251288098  1.4713647392
## 1075 0 -1.057969572  0.2354877621
## 1076 3 -1.190740460 -0.6101309898
## 1077 2 -0.704916780 -0.9495541610
## 1078 0 -0.928136281 -0.1605356101
## 1079 3 -0.268979295 -1.5771950191
## 1080 0  0.141798614  2.1572156823
## 1081 1 -0.309320677  1.0288387058
## 1082 2 -0.807582856 -0.8769025463
## 1083 0 -1.219641360  0.7726698152
## 1084 2 -0.842715369  0.9218003298
## 1085 1 -0.488128379 -0.1634772659
## 1086 2 -0.782618390 -0.7733652378
## 1087 3  0.863918366  0.2981779416
## 1088 2  1.120660624  0.0483794968
## 1089 3  1.513918034  0.9343701013
## 1090 0 -0.247458630  1.3106669295
## 1091 3 -0.130217476 -1.1257531802
## 1092 1 -1.373273679  0.7481460138
## 1093 3  1.542874732  0.9208264329
## 1094 3  0.112650898  0.0947317559
## 1095 2 -0.024797861  1.1344595835
## 1096 0 -0.870509734  1.1618334838
## 1097 2  0.574999215  1.4412278325
## 1098 1  0.666626803  0.0718198332
## 1099 1 -1.011784323  1.0539718150
## 1100 0  0.189082374  0.6035227265
## 1101 1 -1.995098397 -0.3361548905
## 1102 3 -0.120461213 -1.0380216647
## 1103 1  1.124723820  0.9874622041
## 1104 0 -1.304963923  0.1643969662
## 1105 2 -0.818913347 -1.5104630644
## 1106 2  0.994833793 -0.6784822990
## 1107 1  0.236312330  1.1900939894
## 1108 2  0.275994730 -0.3881767915
## 1109 3  0.357639645 -1.1582065103
## 1110 0  0.877070625  1.5646508781
## 1111 3  1.046684010 -1.0392294376
## 1112 0  0.526649383  1.2809018271
## 1113 0  0.268854626  0.5793624959
## 1114 3  0.187093511 -2.7562667204
## 1115 2  1.176797785 -0.6178154551
## 1116 3  0.831249079 -2.1765868822
## 1117 3  1.650208643 -0.0051897418
## 1118 1 -0.189570020 -0.4849941977
## 1119 2 -1.093042689 -1.1704859730
## 1120 0  0.797632300  0.9356919043
## 1121 1 -1.013773025 -0.4454483619
## 1122 2  0.816274528 -0.3878755982
## 1123 1 -0.019295344  0.0782150341
## 1124 3  2.902196326 -0.3221182806
## 1125 2 -1.287207818 -0.7425789537
## 1126 1 -0.262773504  0.1788771594
## 1127 1 -1.532863108  0.9285698251
## 1128 0 -1.368184750  0.7604005875
## 1129 1 -0.748453224  0.1281827873
## 1130 1  1.072104451  1.2496938068
## 1131 1 -1.067124492  1.1058247367
## 1132 1  0.377574876  0.9513435833
## 1133 2 -0.722790844  0.4679158263
## 1134 1  0.242826337  0.4234089983
## 1135 1  0.386840474  0.7330004097
## 1136 1  0.290514562  0.9503945056
## 1137 1  0.429121259  1.4823449611
## 1138 1 -1.811537508 -0.8573838733
## 1139 2  1.219314178 -0.9099923261
## 1140 1 -0.033793258  1.0655030373
## 1141 1  1.776463518 -0.6926878497
## 1142 1  1.066042806  0.4716844630
## 1143 1  1.762909739  0.7950145841
## 1144 1  0.220071680  0.0803348001
## 1145 1 -1.132075084  1.4246626112
## 1146 2 -0.584044104 -1.2537375871
## 1147 3  0.699992808 -0.9541023635
## 1148 1 -0.205606187  0.7901455410
## 1149 0 -1.146390470  0.4374360116
## 1150 2 -0.533017476  0.4594165331
## 1151 2 -1.388907046 -1.4538697452
## 1152 1 -0.833257722 -0.2276183609
## 1153 3  0.055287604 -0.7205411938
## 1154 1 -0.799119973  0.8378576872
## 1155 0 -0.676247083 -0.3320920851
## 1156 3  1.457123140 -0.5598629294
## 1157 0 -1.029659748  0.5010426435
## 1158 1  0.421605279  0.9876595830
## 1159 1 -0.307966437  0.2359250401
## 1160 1 -0.052695455 -0.9618698976
## 1161 1 -0.138704389  0.5822044137
## 1162 2  0.412559008 -0.3028710529
## 1163 1  0.031203439  0.7660225246
## 1164 2  0.594300168  1.4341229172
## 1165 1 -0.011326108 -0.7280204916
## 1166 2  0.235050839 -0.9217623117
## 1167 2 -0.498151258 -1.0449626023
## 1168 2 -0.098021041 -1.4637851875
## 1169 0 -1.351071049  1.6974844708
## 1170 3  0.204771079  1.0364744673
## 1171 1  1.071635105  1.2579366978
## 1172 0  0.688892738  1.5720535363
## 1173 1  0.880726454  0.0608400532
## 1174 2  1.756004286  0.2918198906
## 1175 3  1.014987550 -1.2353160699
## 1176 3  0.558852893 -1.4654086754
## 1177 1 -0.185769747  0.3600302369
## 1178 2  1.006642353 -0.3473243625
## 1179 0 -1.924681551 -0.2172761251
## 1180 2  0.042447009  0.4993091490
## 1181 3 -0.414132464 -1.0318524993
## 1182 0 -0.422669094  1.5786968764
## 1183 0 -0.999822505 -0.0759833095
## 1184 1 -0.646346059  0.6893846705
## 1185 0 -0.250155018  0.1287795952
## 1186 2 -0.284409757  0.0015070365
## 1187 1  1.577990966  0.6632436911
## 1188 1 -1.728609009 -0.2256764075
## 1189 3  0.412243217 -1.5037031950
## 1190 3 -0.128629130 -2.4151161944
## 1191 0 -0.739397017 -0.0060132833
## 1192 3  0.036973796 -0.3761525818
## 1193 1 -0.004807621  0.0880558568
## 1194 3 -0.387258260 -1.0516690862
## 1195 0 -1.667460026 -0.5338711959
## 1196 2  1.692248392  0.1332424298
## 1197 2 -0.078409517  0.3612912517
## 1198 2  1.353605002 -0.8442270694
## 1199 0  0.020129894  0.8054341945
## 1200 3  0.901556309 -0.5737272029
## 1201 1  0.009531527 -0.4716556460
## 1202 2  0.073759438  1.1033221331
## 1203 2 -1.272091875 -0.7900754710
## 1204 1 -2.113435428 -0.3589279696
## 1205 2  0.758660734 -0.1177612761
## 1206 2  1.885960859  1.4571099688
## 1207 2 -0.267780675  0.0208030324
## 1208 3  1.756585694  0.4413924209
## 1209 3 -0.768935043 -1.4395330175
## 1210 2  0.231063321  0.7477434693
## 1211 1  0.069846612 -0.2552172788
## 1212 2  0.145249822 -1.3334273975
## 1213 2  0.440480201  0.6295010077
## 1214 1 -0.076158379  1.5119923093
## 1215 0 -0.207042263 -0.3280984239
## 1216 1  0.223260718  0.1849314775
## 1217 1 -0.360450555 -0.0121813824
## 1218 1  0.609040412  0.4904435572
## 1219 0 -0.752642364  0.0855932682
## 1220 0 -1.658524513  0.3894319505
## 1221 1  1.377180574  1.3853600904
## 1222 2  0.900475708 -1.1554538039
## 1223 1 -1.723502318 -0.0775158301
## 1224 3  0.922482830 -1.0982976148
## 1225 2 -0.312165236 -0.8568457802
## 1226 3  0.575266309 -0.5479823031
## 1227 2  0.198714244 -0.5006612790
## 1228 1 -1.105535160  0.6254375808
## 1229 2  0.457261348 -0.4456255855
## 1230 1 -0.079479062 -0.4298214757
## 1231 1  0.902473288 -0.2319529861
## 1232 2 -0.107758392  0.7409936363
## 1233 1  0.262782693 -0.0819771660
## 1234 1 -1.826784151  0.6032743514
## 1235 2 -0.003109551 -0.3220883618
## 1236 2  1.392895388  0.8300149021
## 1237 3  0.225853482 -1.1620196555
## 1238 1 -0.870675116  0.8304443956
## 1239 1 -0.494490544  1.8807975099
## 1240 0  0.672028348  1.6620788294
## 1241 2  0.147024299  0.3797362387
## 1242 2 -0.503366143  0.3153958512
## 1243 3  0.540096623 -0.5525473719
## 1244 2 -0.101311226 -0.3842723003
## 1245 1  0.392438850  0.5593904549
## 1246 1 -0.296519930  0.3341519189
## 1247 2  0.604186494 -0.1928249315
## 1248 3  2.137236841  0.2617923611
## 1249 1 -1.163785799 -0.4115673961
## 1250 2 -0.618019862 -0.6618601968
## 1251 0 -0.930382191  0.9217642623
## 1252 2  0.622249851 -0.3948484533
## 1253 0 -1.499210753  0.2111170326
## 1254 1 -1.601724827  0.2020186520
## 1255 1 -1.230908565  0.2747426039
## 1256 2  0.699594035  0.2664689575
## 1257 1  0.578774949 -1.4107218134
## 1258 1 -0.610034192 -0.1203208798
## 1259 2 -1.335742092  0.2844103407
## 1260 2 -0.256398174 -1.4618638348
## 1261 1 -0.908698144  0.9195703543
## 1262 1 -1.189603477  0.7600650655
## 1263 3  0.868098109 -1.2636383501
## 1264 0 -1.401797388  1.3149768653
## 1265 0 -0.649755348  0.9120903565
## 1266 3  0.664895434 -0.5393399494
## 1267 3  1.484135135 -0.4474881396
## 1268 1  0.871282474  0.5199034682
## 1269 0  0.313841883  1.4874133606
## 1270 0 -1.947623310  0.3985180329
## 1271 2 -2.154178786 -0.3934870117
## 1272 1  0.588239048  0.1605000675
## 1273 2 -0.061329645  0.4655641612
## 1274 1 -0.434788483 -1.3754461791
## 1275 3  0.260598603 -1.7746466602
## 1276 0  0.973908492  0.8131840221
## 1277 1  1.090961771  1.4567577848
## 1278 2  0.221338605  0.4845690863
## 1279 0 -0.228324146  1.5795980187
## 1280 1 -0.952878151  1.1479176389
## 1281 2 -0.299045215 -0.4686921436
## 1282 2  0.723821144 -0.1173129322
## 1283 3  0.942871983 -0.7141992639
## 1284 2 -0.370683695  0.1961360075
## 1285 0  0.470180717  1.0525251563
## 1286 1  0.189986395  0.1703024451
## 1287 0  0.622374905  0.5087927901
## 1288 2  0.884655622 -0.4352939519
## 1289 3  0.263954582 -0.7459826308
## 1290 2 -0.537044701 -0.9384885064
## 1291 2  0.386950573  1.5810602007
## 1292 3  0.070915367 -1.8349412312
## 1293 2  1.031231699  0.0409764969
## 1294 0 -1.852999830  0.3179544489
## 1295 0  0.396362146  2.7390262939
## 1296 0 -0.320734084  1.3582904859
## 1297 2  0.253724334 -0.5715092264
## 1298 3  0.329066185  0.1762127443
## 1299 2 -0.785070596 -0.2153346371
## 1300 2  1.111933967 -0.2513057433
## 1301 1 -0.794100138 -0.1887376089
## 1302 2 -0.459658235 -0.1227716492
## 1303 2 -0.084482089  0.2864009813
## 1304 2  1.085551783  0.2735241003
## 1305 1  0.670085045  0.3834475853
## 1306 1 -0.170867546 -1.1982843777
## 1307 1  0.145794057 -1.4065441149
## 1308 0 -0.914917550  1.0052540090
## 1309 1  0.510395092 -0.3318396538
## 1310 0  0.340802734  2.9946117697
## 1311 1 -0.220489447 -1.4961822808
## 1312 2  0.864022534  1.2562510210
## 1313 2  0.049745656 -0.4439332032
## 1314 2 -0.187322657 -0.4005244179
## 1315 0 -0.546737224  0.5513587184
## 1316 2  0.473994626 -0.1263799960
## 1317 1 -0.003592222  0.0677915619
## 1318 0 -0.217059922 -1.1880355986
## 1319 3  3.213012326  0.8343551034
## 1320 3  0.501949451 -0.0362027462
## 1321 2  0.436867662 -0.2247065610
## 1322 2  2.301860500  0.3594631018
## 1323 0 -1.324032517 -0.1477019194
## 1324 1 -0.402778268 -0.2252406670
## 1325 1  0.716936829  0.6641509778
## 1326 2  0.499583183 -0.5574362888
## 1327 0  0.367049238  0.3106017223
## 1328 1 -0.024504981  0.4232455937
## 1329 2  0.081392342  0.1223518350
## 1330 2  1.610812243  2.1120116328
## 1331 2  0.474389215  0.0589202349
## 1332 0 -0.837645351  0.0133374844
## 1333 3 -0.260893509 -0.3762172172
## 1334 3  2.277218918  0.9425190609
## 1335 0 -1.497962830 -0.5113351851
## 1336 3  0.742294156 -1.3330200979
## 1337 2  1.263869641 -0.9276741379
## 1338 1 -0.769419951  0.2590544604
## 1339 2  0.566417297  0.0458824801
## 1340 2 -0.589574550 -1.9577210359
## 1341 0 -1.169368423  1.3083283535
## 1342 2  1.232306307 -0.0088180153
## 1343 2  0.844829778 -0.7819281177
## 1344 1 -0.641862561 -0.3346807444
## 1345 3 -0.035310123 -1.5014615889
## 1346 2 -0.446110451 -0.5344193459
## 1347 1 -1.139366042 -1.4034738030
## 1348 2  0.254754688 -0.0010532265
## 1349 2  0.056407365 -0.8426420385
## 1350 0 -1.145604583  1.7610353504
## 1351 3  1.563321794  0.8050398098
## 1352 1 -1.846525035 -0.9975334285
## 1353 2  0.901921171  1.3434042588
## 1354 2 -1.406773810 -1.5310857373
## 1355 1 -1.533981328 -1.1911130743
## 1356 1  0.599365832  0.5404240136
## 1357 0 -1.081594894 -0.0439514690
## 1358 1 -0.004761316  0.5770266727
## 1359 1  0.354098872 -0.8599336540
## 1360 1 -0.087360998 -0.9718647238
## 1361 2  0.638704290  0.8292534491
## 1362 3  0.186189226 -0.1830714125
## 1363 3  1.343197249 -0.1201415369
## 1364 2 -1.497458609 -0.3481401276
## 1365 3 -1.289265190 -2.3884627627
## 1366 2  0.009999936 -0.7613730882
## 1367 0 -0.598095192  0.3584584089
## 1368 1 -0.155793529 -0.2552399828
## 1369 3  2.449423242 -0.6732591104
## 1370 2  0.021196779  0.2427078650
## 1371 2  0.679142389 -0.0484535901
## 1372 1 -1.328481287 -0.0918274485
## 1373 2  1.044571393 -0.1118427989
## 1374 1 -0.672705126 -1.1497492403
## 1375 1 -1.342502553 -0.2776491869
## 1376 1 -1.910889701 -0.2096835020
## 1377 3  0.490413295 -1.0004170275
## 1378 1 -0.881567759  1.3297519291
## 1379 0 -1.202747559  1.1080852462
## 1380 2  1.970657077 -0.4956311092
## 1381 1 -0.818278515  0.5951454193
## 1382 3  0.765491862 -0.7829724720
## 1383 1 -0.988255114 -0.1187590584
## 1384 2  0.447965152  0.0043440108
## 1385 1 -1.409412909  0.0597529823
## 1386 2  1.324389832 -0.4821053701
## 1387 3  0.125809467 -0.7493081601
## 1388 2 -0.720129740 -0.8857247772
## 1389 3  1.019803097 -2.0278452774
## 1390 0  0.569714839  1.8567727435
## 1391 0  0.912148563  1.1020474512
## 1392 2  0.110735933 -0.1267436752
## 1393 1  0.277485981 -0.3105850341
## 1394 1  0.634291013  0.3744077675
## 1395 1 -0.498696813 -0.8041079531
## 1396 1  0.066883571  1.5393103219
## 1397 0  1.172497340 -0.0443462316
## 1398 2 -0.173793102  0.9716061135
## 1399 1 -0.170980660  0.4805711927
## 1400 1 -0.296345629  0.5787988547
## 1401 2 -0.773251206 -1.0320242437
## 1402 2  1.612058900 -0.6157702516
## 1403 2 -1.896031854 -1.1638025087
## 1404 0 -0.233815976  1.9801009331
## 1405 3 -0.233096951 -0.3378108381
## 1406 1  0.050429984  1.2059349052
## 1407 1 -0.069271201  0.2877551115
## 1408 3 -0.343768867 -1.3008581479
## 1409 3 -0.208983254 -0.6270827322
## 1410 0  0.590610135  1.2611776242
## 1411 2  1.444133083 -0.6377022048
## 1412 3  0.168547945 -0.7816141524
## 1413 2  0.717060112 -0.5574277445
## 1414 2  2.575207977 -1.6192689450
## 1415 1 -0.444165678 -0.4254134419
## 1416 1 -1.293528825  0.9327900423
## 1417 2  0.550156813  0.2012157205
## 1418 2  0.181720493  0.4942502030
## 1419 1 -0.716363287 -0.7195111749
## 1420 1 -0.810958232  0.8990431927
## 1421 2  1.082773074  0.5093923554
## 1422 3  0.975726836 -0.6926169107
## 1423 3  2.058323766 -0.5734929509
## 1424 1 -0.024430090 -0.1745260045
## 1425 0 -1.348246340 -0.6005586216
## 1426 0 -0.530673920  1.5451484475
## 1427 3 -0.034033994 -1.9529741834
## 1428 3  0.947717348 -1.4464156274
## 1429 2 -0.587795683 -0.3070121449
## 1430 1 -0.801457694  0.0190447645
## 1431 1 -0.398299628  0.8493391222
## 1432 3 -1.270343979 -2.4770009317
## 1433 1  1.125374402 -0.0361936343
## 1434 1 -1.280904302  1.0370021133
## 1435 1  0.285853306  0.6000457231
## 1436 0 -1.602392754 -0.4242292563
## 1437 2  0.292107282 -0.7866020591
## 1438 2  0.474445021  1.6280714558
## 1439 1  0.630895206 -0.4056544004
## 1440 0 -0.953577943  0.8566559455
## 1441 1 -1.408222709 -0.0559822104
## 1442 2 -0.631098053 -0.4325322188
## 1443 2 -1.227528819 -0.4688384031
## 1444 1 -0.677101667 -0.8109300654
## 1445 1  1.034135168 -1.0551484376
## 1446 2 -0.542842285 -0.1663122177
## 1447 3 -0.507525993 -0.1184501722
## 1448 2 -1.565881186 -1.5898461306
## 1449 2 -0.494507073 -0.7960667958
## 1450 2  0.332781557 -0.5706580322
## 1451 2 -0.355001094 -1.1453269716
## 1452 0  0.705420934  0.8672858326
## 1453 2 -1.274564278 -0.7990912413
## 1454 1  0.404387384  0.6528406632
## 1455 1 -1.360758697  0.0161845419
## 1456 2  0.942648651  0.7179250361
## 1457 2  0.743621680  0.3176585792
## 1458 0 -1.067639657  1.8018108719
## 1459 0 -0.122513209  0.2377350438
## 1460 2  0.410996338 -0.9910218724
## 1461 3  0.892908899 -0.4131050620
## 1462 1  0.717260667  1.2767495303
## 1463 3  0.908813326  0.2748236156
## 1464 3  1.022623737 -1.5040588581
## 1465 3  0.085113301 -1.7778355250
## 1466 3 -0.917446890 -2.8850580184
## 1467 2 -0.316335565 -1.3123341398
## 1468 3  1.434031559  1.3718754286
## 1469 3  0.157210007 -1.1085800662
## 1470 0 -0.456513339  0.5470505209
## 1471 1  0.044588572  0.1856535638
## 1472 3  1.900003260  1.0913630303
## 1473 3 -0.265609519 -0.1757117822
## 1474 1 -0.194010917  0.1252439195
## 1475 3 -0.173446803  0.5572662968
## 1476 3 -1.497730376 -2.1936378137
## 1477 1 -1.616447737 -0.6710801533
## 1478 3  0.620589595 -1.3598789048
## 1479 1 -0.332629550 -1.1618917401
## 1480 3  0.386054844 -1.3854654746
## 1481 2  0.001172281  0.2957350544
## 1482 3  1.484449075 -1.6012240888
## 1483 3  0.195329310 -1.0546952316
## 1484 1 -1.109701965  0.5881503499
## 1485 1 -0.603918196  0.5553913054
## 1486 1 -1.193801958  1.1689586778
## 1487 1 -0.502769735  0.1811678743
## 1488 0 -0.933967788  1.0751952764
## 1489 2 -0.207170372 -0.9777414283
## 1490 2  1.467421750  0.6035349733
## 1491 1 -0.264138215  2.3717557634
## 1492 1 -0.896575832 -1.1017417215
## 1493 1 -0.231190881 -0.6650946794
## 1494 2  1.426241555  1.3438522967
## 1495 3  0.648566560  0.4855087269
## 1496 1 -0.323085971 -0.9939368100
## 1497 0 -1.670660902  0.0312153267
## 1498 1  0.129893397  0.5193192839
## 1499 1  0.474195889 -1.4067473389
## 1500 0 -1.435217253 -0.8965098871
\end{verbatim}

LDA with 4 classes

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# assess the accuracy of the prediction}
\NormalTok{lda.pred.test.prob3 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\AttributeTok{x =} \FunctionTok{predict}\NormalTok{(lda.model.prob3, prob3.test)}\SpecialCharTok{$}\NormalTok{class) }\SpecialCharTok{{-}} \DecValTok{1}
\FunctionTok{length}\NormalTok{(}\AttributeTok{x =}\NormalTok{ lda.pred.test.prob3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1500
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# analysis}
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{Confusion Matrix:}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Confusion Matrix:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ct }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(prob3.test}\SpecialCharTok{$}\NormalTok{Y, lda.pred.test.prob3)}
\NormalTok{ct}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    lda.pred.test.prob3
##       0   1   2   3
##   0 149  70  36   0
##   1 128 156 193   9
##   2  30 151 282  15
##   3   4  38 166  73
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cm }\OtherTok{\textless{}{-}} \FunctionTok{confusionMatrix}\NormalTok{(}\AttributeTok{data =} \FunctionTok{factor}\NormalTok{(lda.pred.test.prob3), }\AttributeTok{reference =} \FunctionTok{factor}\NormalTok{(prob3.test}\SpecialCharTok{$}\NormalTok{Y))}
\NormalTok{cm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1   2   3
##          0 149 128  30   4
##          1  70 156 151  38
##          2  36 193 282 166
##          3   0   9  15  73
## 
## Overall Statistics
##                                           
##                Accuracy : 0.44            
##                  95% CI : (0.4147, 0.4656)
##     No Information Rate : 0.324           
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.2213          
##                                           
##  Mcnemar's Test P-Value : < 2.2e-16       
## 
## Statistics by Class:
## 
##                      Class: 0 Class: 1 Class: 2 Class: 3
## Sensitivity           0.58431   0.3210   0.5900  0.25979
## Specificity           0.86988   0.7446   0.6135  0.98031
## Pos Pred Value        0.47910   0.3759   0.4165  0.75258
## Neg Pred Value        0.91085   0.6959   0.7618  0.85175
## Prevalence            0.17000   0.3240   0.3187  0.18733
## Detection Rate        0.09933   0.1040   0.1880  0.04867
## Detection Prevalence  0.20733   0.2767   0.4513  0.06467
## Balanced Accuracy     0.72710   0.5328   0.6017  0.62005
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{Diagonal (Correct Predictions):}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Diagonal (Correct Predictions):
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{diag}\NormalTok{(}\FunctionTok{prop.table}\NormalTok{(ct, }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         0         1         2         3 
## 0.5843137 0.3209877 0.5899582 0.2597865
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{Accuracy and Error Rate:}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Accuracy and Error Rate:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acc.prob3 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(}\FunctionTok{prop.table}\NormalTok{(ct)))}
\NormalTok{err.prob3 }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(}\FunctionTok{prop.table}\NormalTok{(ct)))}
\NormalTok{acc.prob3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.44
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{err.prob3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.56
\end{verbatim}

QDA with 4 classes

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# assess the accuracy of the prediction}
\NormalTok{qda.pred.test.prob3 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\AttributeTok{x =} \FunctionTok{predict}\NormalTok{(qda.model.prob3, prob3.test)}\SpecialCharTok{$}\NormalTok{class) }\SpecialCharTok{{-}} \DecValTok{1}
\FunctionTok{length}\NormalTok{(}\AttributeTok{x =}\NormalTok{ qda.pred.test.prob3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1500
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# assess}
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{Confusion Matrix:}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Confusion Matrix:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ct }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(prob3.test}\SpecialCharTok{$}\NormalTok{Y, qda.pred.test.prob3)}
\NormalTok{ct}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    qda.pred.test.prob3
##       0   1   2   3
##   0 124  62  66   3
##   1 116 125 229  16
##   2  27 110 288  53
##   3  15  37 159  70
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cm.prob3 }\OtherTok{\textless{}{-}} \FunctionTok{confusionMatrix}\NormalTok{(}\AttributeTok{data =} \FunctionTok{factor}\NormalTok{(qda.pred.test.prob3), }\AttributeTok{reference =} \FunctionTok{factor}\NormalTok{(prob3.test}\SpecialCharTok{$}\NormalTok{Y))}
\NormalTok{cm.prob3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1   2   3
##          0 124 116  27  15
##          1  62 125 110  37
##          2  66 229 288 159
##          3   3  16  53  70
## 
## Overall Statistics
##                                         
##                Accuracy : 0.4047        
##                  95% CI : (0.3797, 0.43)
##     No Information Rate : 0.324         
##     P-Value [Acc > NIR] : 3.298e-11     
##                                         
##                   Kappa : 0.1738        
##                                         
##  Mcnemar's Test P-Value : < 2.2e-16     
## 
## Statistics by Class:
## 
##                      Class: 0 Class: 1 Class: 2 Class: 3
## Sensitivity           0.48627  0.25720   0.6025  0.24911
## Specificity           0.87309  0.79389   0.5558  0.94094
## Pos Pred Value        0.43972  0.37425   0.3881  0.49296
## Neg Pred Value        0.89245  0.69039   0.7493  0.84462
## Prevalence            0.17000  0.32400   0.3187  0.18733
## Detection Rate        0.08267  0.08333   0.1920  0.04667
## Detection Prevalence  0.18800  0.22267   0.4947  0.09467
## Balanced Accuracy     0.67968  0.52554   0.5791  0.59502
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{Diagonal (Correct Predictions):}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Diagonal (Correct Predictions):
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{diag}\NormalTok{(}\FunctionTok{prop.table}\NormalTok{(ct, }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         0         1         2         3 
## 0.4862745 0.2572016 0.6025105 0.2491103
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{Accuracy and Error Rate:}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Accuracy and Error Rate:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acc.prob3 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(}\FunctionTok{prop.table}\NormalTok{(ct)))}
\NormalTok{err.prob3 }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(}\FunctionTok{prop.table}\NormalTok{(ct)))}
\NormalTok{acc.prob3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4046667
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{err.prob3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5953333
\end{verbatim}

For both LDA and QDA, the error rates are a small margin above half
(0.56 and 0.5953, correspondingly). QDA actually performed slightly less
accurately than LDA. The dataset may have linearly separable classes,
and/or the dataset size may be too small to alleviate noticeable effects
of variance for QDA. But these points are only speculation. As expected
with a higher error rate, QDA had lower sensitivity or recall for all
classes except slightly better with the 3rd class when comparing to LDA
via confusion matrices.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# class sizes}
\ControlFlowTok{for}\NormalTok{ (class }\ControlFlowTok{in} \DecValTok{0}\SpecialCharTok{:}\DecValTok{3}\NormalTok{) \{}
  \FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{Class \textquotesingle{}}\NormalTok{, class, }\StringTok{\textquotesingle{} size: \textquotesingle{}}\NormalTok{, }\FunctionTok{length}\NormalTok{(}\AttributeTok{x =}\NormalTok{ prob3.test[prob3.test}\SpecialCharTok{$}\NormalTok{Y }\SpecialCharTok{==}\NormalTok{ class, ]}\SpecialCharTok{$}\NormalTok{Y), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Class  0  size:  255 
## 
## Class  1  size:  486 
## 
## Class  2  size:  478 
## 
## Class  3  size:  281
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'dplyr' was built under R version 4.1.1
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'dplyr'
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:plyr':
## 
##     arrange, count, desc, failwith, id, mutate, rename, summarise,
##     summarize
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:MASS':
## 
##     select
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:stats':
## 
##     filter, lag
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:base':
## 
##     intersect, setdiff, setequal, union
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{class.size }\OtherTok{\textless{}{-}}\NormalTok{ prob3.test }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{count}\NormalTok{(Y)}
\NormalTok{class.size}\SpecialCharTok{$}\NormalTok{prop }\OtherTok{\textless{}{-}} \FunctionTok{prop.table}\NormalTok{(}\AttributeTok{x =}\NormalTok{ class.size)}\SpecialCharTok{$}\NormalTok{n}
\NormalTok{class.size}\SpecialCharTok{$}\NormalTok{err }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ class.size}\SpecialCharTok{$}\NormalTok{prop}
\NormalTok{class.size}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Y   n      prop       err
## 1 0 255 0.1693227 0.8306773
## 2 1 486 0.3227092 0.6772908
## 3 2 478 0.3173971 0.6826029
## 4 3 281 0.1865870 0.8134130
\end{verbatim}

Question 3 iii

Simply having an error rate \textgreater{} 0.50 should not be
immediately interpreted as being awful performance. From the cell above,
each class has a much higher chance of misclassification than 0.50 based
on random guessing with account of class sizes. Keep in mind that
\textgreater{} 0.50 can be interpreted as worse than random guessing for
binary cases of equal class sizes, but the problem at hand has 4 classes
with no class consuming over half of all data points.

Question 3 iv

Assuming that class sizes are all equal; that is, each class holds 0.25
of all data points. By randomly guessing, there would still be a 0.75 to
0.25 odds of misclassifying as opposed to correctly classifying an
instance regardless of class. With an error rate of \textasciitilde60\%,
QDA still outperforms random guessing with equal class sizes by a
substantial margin.

Question 4

Question 4i

Steps to run simulation study, with code right after.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Determine the number of simulations to run and the dataset sizes per
  simulation. Each simulation will iterate through the same sequence of
  monotonically increasing data sizes. For this case, there will be 100
  simulations, and the dataset sizes generated per simulation goes from
  100 to 2000 in increments of 100, visually as 100, 200, 300, \ldots,
  2000.
\item
  Create two 100 x 20 matrices initialized with zeros (0), one for LDA
  error rates (\texttt{\textquotesingle{}lda.err}) and another for QDA
  error rates (\texttt{qda.err}). Rows correspond to simulations, and
  columns correspond to increasing dataset sizes.
\item
  For simulation \texttt{i} and dataset \texttt{size}:
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Generate \texttt{size} independent random values using the standard
  normal distribution for the \texttt{X1} covariate. Repeat the same
  procedure for the \texttt{X2} covariate. Bind both vectors of
  generated values as columns inside the same dataframe (called
  \texttt{data}).
\item
  For all corresponding pairs of (\texttt{X1}, \texttt{X2}), compute a
  probability \texttt{p} = e\^{}X1 + e\^{}(0.5 * X2). Generate a label
  (either 0 or 1) as a Bernoulli trial with \texttt{p}. This should
  produce a vector \texttt{Y} containing \texttt{size} corresponding
  labels. Bind \texttt{Y} as a new column in \texttt{data}.
\item
  Split \texttt{data} into 2 equally sized halves with into train and
  test sets with no overlapping indexes from the original dataframe. The
  rows of \texttt{data} are randomly dispersed into the 2 datasets. Let
  the new datasets be \texttt{train.set} and \texttt{test.set}.
\item
  Instantiate LDA model with \texttt{train.set} and follow the same
  formula used in part b. Note the nonlinear combination of both
  covariates. Predict with \texttt{test.set} and compute a confusion
  matrix from the predicted labels and originally generated labels for
  rows in \texttt{test.set}. Compute the error rate from confusion
  matrix. Save the error rate as entry indexed (\texttt{i}, \texttt{j})
  in \texttt{lda.err} for i\^{}th simulation \texttt{i} and j\^{}th
  dataset size.
\item
  Repeat part d but replace the instantiated model with QDA and the
  matrix of saved error rates with with \texttt{qda.err}.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Find the mean of error rates columnwise for \texttt{lda.err} and
  \texttt{qda.err}. This leaves a vector of 20 averaged error rates,
  aligned with each dataset size, by compacting across all 100
  simulations for both types of discriminant analyses.
\item
  Plot both mean vectors from part 4 in the same figure.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# metadata}
\NormalTok{n }\OtherTok{=} \DecValTok{1}\SpecialCharTok{:}\DecValTok{20} \SpecialCharTok{*} \DecValTok{100} \CommentTok{\# monotonically increasing dataset size}
\NormalTok{n.sim }\OtherTok{=} \DecValTok{100} \CommentTok{\# number of simulations (trials)}
\NormalTok{model.formula }\OtherTok{\textless{}{-}}\NormalTok{ Y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{exp}\NormalTok{(}\AttributeTok{x =}\NormalTok{ X1) }\SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(}\AttributeTok{x =} \FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ X2)}
\NormalTok{model.formula.func }\OtherTok{\textless{}{-}} \DecValTok{0}

\NormalTok{expit }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(x) \{}
  \FunctionTok{exp}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x))}
\NormalTok{\}}

\NormalTok{lda.err }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\AttributeTok{data =} \DecValTok{0}\NormalTok{, }\AttributeTok{nrow =}\NormalTok{ n.sim, }\AttributeTok{ncol =} \FunctionTok{length}\NormalTok{(}\AttributeTok{x =}\NormalTok{ n))}
\NormalTok{qda.err }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\AttributeTok{data =} \DecValTok{0}\NormalTok{, }\AttributeTok{nrow =}\NormalTok{ n.sim, }\AttributeTok{ncol =} \FunctionTok{length}\NormalTok{(}\AttributeTok{x =}\NormalTok{ n))}

\CommentTok{\# each simulation or trial}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n.sim) \{}
  
  \CommentTok{\# each differently sized dataset per simulation}
  \ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(}\AttributeTok{x =}\NormalTok{ n)) \{}
    
    \CommentTok{\# Setup for both LDA and QDA}
    \CommentTok{\# generating data}
\NormalTok{    size }\OtherTok{\textless{}{-}}\NormalTok{ n[j] }\CommentTok{\# size of dataset}
\NormalTok{    data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{X1 =} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =}\NormalTok{ size), }\AttributeTok{X2 =} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =}\NormalTok{ size))}
\NormalTok{    p }\OtherTok{\textless{}{-}} \FunctionTok{expit}\NormalTok{(}\AttributeTok{x =} \FunctionTok{exp}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{X1) }\SpecialCharTok{{-}} \FunctionTok{exp}\NormalTok{(}\FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{X2))}
\NormalTok{    data}\SpecialCharTok{$}\NormalTok{Y }\OtherTok{=} \FunctionTok{rbinom}\NormalTok{(}\AttributeTok{n =}\NormalTok{ n, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{p =}\NormalTok{ p)}

    \CommentTok{\# train test split}
\NormalTok{    train.index }\OtherTok{\textless{}{-}}\NormalTok{ caret}\SpecialCharTok{::}\FunctionTok{createDataPartition}\NormalTok{(}\AttributeTok{y =}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{Y, }\AttributeTok{list =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{    train.set }\OtherTok{\textless{}{-}}\NormalTok{ data[train.index,]}
\NormalTok{    test.set }\OtherTok{\textless{}{-}}\NormalTok{ data[}\SpecialCharTok{{-}}\NormalTok{train.index,]}

    \CommentTok{\# LDA}
    \CommentTok{\# model and prediction}
\NormalTok{    lda.model.prob4 }\OtherTok{\textless{}{-}}\NormalTok{ MASS}\SpecialCharTok{::}\FunctionTok{lda}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ model.formula, }\AttributeTok{data =}\NormalTok{ train.set)}
\NormalTok{    lda.pred.test.prob4 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\AttributeTok{x =} \FunctionTok{predict}\NormalTok{(lda.model.prob4, test.set)}\SpecialCharTok{$}\NormalTok{class) }\SpecialCharTok{{-}} \DecValTok{1}
    \CommentTok{\# evaluation}
\NormalTok{    ct }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(test.set}\SpecialCharTok{$}\NormalTok{Y, lda.pred.test.prob4)}
\NormalTok{    acc.prob4 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(}\FunctionTok{prop.table}\NormalTok{(ct)))}
\NormalTok{    err.prob4 }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(}\FunctionTok{prop.table}\NormalTok{(ct)))}
\NormalTok{    lda.err[i, j] }\OtherTok{\textless{}{-}}\NormalTok{ err.prob4}

    \CommentTok{\# QDA}
    \CommentTok{\# model and prediction}
\NormalTok{    qda.model.prob4 }\OtherTok{\textless{}{-}}\NormalTok{ MASS}\SpecialCharTok{::}\FunctionTok{qda}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ model.formula, }\AttributeTok{data =}\NormalTok{ train.set)}
\NormalTok{    qda.pred.test.prob4 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\AttributeTok{x =} \FunctionTok{predict}\NormalTok{(qda.model.prob4, test.set)}\SpecialCharTok{$}\NormalTok{class) }\SpecialCharTok{{-}} \DecValTok{1}
    \CommentTok{\# evaluation}
\NormalTok{    ct }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(test.set}\SpecialCharTok{$}\NormalTok{Y, qda.pred.test.prob4)}
\NormalTok{    acc.prob4 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(}\FunctionTok{prop.table}\NormalTok{(ct)))}
\NormalTok{    err.prob4 }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(}\FunctionTok{prop.table}\NormalTok{(ct)))}
\NormalTok{    qda.err[i, j] }\OtherTok{\textless{}{-}}\NormalTok{ err.prob4}
\NormalTok{  \}}
  
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# verifying and averaging error rate data over all simulations}
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{Model error rates:}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Model error rates:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as.data.frame}\NormalTok{(}\AttributeTok{x =}\NormalTok{ lda.err)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           V1   V2        V3    V4       V5        V6        V7     V8        V9
## 1   0.380000 0.50 0.3800000 0.415 0.508000 0.3633333 0.7507163 0.5200 0.7933333
## 2   0.700000 0.46 0.4333333 0.455 0.712000 0.5166667 0.4857143 0.4825 0.3000000
## 3   0.540000 0.45 0.4066667 0.335 0.464000 0.4600000 0.3800000 0.2900 0.6111111
## 4   0.755102 0.54 0.6600000 0.595 0.660000 0.5166667 0.4285714 0.4100 0.4533333
## 5   0.460000 0.43 0.4266667 0.420 0.556000 0.5000000 0.7507163 0.3475 0.6000000
## 6   0.440000 0.49 0.4266667 0.540 0.428000 0.4900000 0.4485714 0.7500 0.6333333
## 7   0.400000 0.19 0.6933333 0.335 0.388000 0.4133333 0.5400000 0.4900 0.7222222
## 8   0.280000 0.55 0.6133333 0.385 0.424000 0.4666667 0.4485714 0.6300 0.5066667
## 9   0.340000 0.31 0.3266667 0.705 0.428000 0.4533333 0.3371429 0.5150 0.4088889
## 10  0.380000 0.45 0.2483221 0.480 0.516000 0.5133333 0.4285714 0.3750 0.6400000
## 11  0.420000 0.67 0.5200000 0.690 0.428000 0.4100000 0.6228571 0.4325 0.3755556
## 12  0.660000 0.44 0.6400000 0.410 0.420000 0.4466667 0.4000000 0.4750 0.5000000
## 13  0.480000 0.41 0.4400000 0.440 0.496000 0.4433333 0.8228571 0.5000 0.3377778
## 14  0.380000 0.49 0.5266667 0.365 0.500000 0.4800000 0.7507163 0.3950 0.4488889
## 15  0.400000 0.46 0.4400000 0.500 0.348000 0.4966667 0.5000000 0.5125 0.7505568
## 16  0.540000 0.58 0.4133333 0.605 0.432000 0.5400000 0.5685714 0.4575 0.3822222
## 17  0.460000 0.47 0.3533333 0.480 0.428000 0.6466667 0.4657143 0.4375 0.4888889
## 18  0.320000 0.56 0.4866667 0.470 0.416000 0.7500000 0.4571429 0.5150 0.3400000
## 19  0.320000 0.46 0.3733333 0.690 0.576000 0.5433333 0.2857143 0.4325 0.5955556
## 20  0.580000 0.40 0.8000000 0.355 0.808000 0.4900000 0.8028571 0.5575 0.4044444
## 21  0.500000 0.58 0.4666667 0.680 0.492000 0.7000000 0.5514286 0.5225 0.8533333
## 22  0.280000 0.44 0.7200000 0.615 0.640000 0.6866667 0.4457143 0.6025 0.4600000
## 23  0.500000 0.60 0.3000000 0.445 0.456000 0.4933333 0.4885714 0.5000 0.4000000
## 24  0.560000 0.56 0.4400000 0.450 0.408000 0.4766667 0.7507163 0.4625 0.8511111
## 25  0.440000 0.39 0.4466667 0.550 0.412000 0.3933333 0.5000000 0.6475 0.6133333
## 26  0.755102 0.50 0.8666667 0.620 0.436000 0.4100000 0.5942857 0.4825 0.5066667
## 27  0.600000 0.55 0.4800000 0.535 0.440000 0.7766667 0.3857143 0.6975 0.4088889
## 28  0.300000 0.38 0.4400000 0.470 0.600000 0.6700000 0.5000000 0.3650 0.4600000
## 29  0.460000 0.52 0.4000000 0.465 0.360000 0.4233333 0.4285714 0.2475 0.6155556
## 30  0.320000 0.45 0.4466667 0.430 0.424000 0.4333333 0.3828571 0.8900 0.2516704
## 31  0.380000 0.43 0.6933333 0.750 0.428000 0.3133333 0.4371429 0.6725 0.3488889
## 32  0.440000 0.38 0.5133333 0.625 0.488000 0.5966667 0.4142857 0.2525 0.4244444
## 33  0.520000 0.39 0.4866667 0.480 0.408000 0.3533333 0.4885714 0.7075 0.4755556
## 34  0.580000 0.44 0.5000000 0.475 0.352000 0.4266667 0.2914286 0.4675 0.6555556
## 35  0.620000 0.48 0.5400000 0.710 0.384000 0.3833333 0.4914286 0.4875 0.5911111
## 36  0.340000 0.26 0.4066667 0.340 0.436000 0.3966667 0.5628571 0.4400 0.5866667
## 37  0.420000 0.49 0.4666667 0.495 0.380000 0.3900000 0.6028571 0.3100 0.4022222
## 38  0.480000 0.43 0.5066667 0.450 0.332000 0.4766667 0.2521490 0.2825 0.3400000
## 39  0.440000 0.44 0.4533333 0.565 0.652000 0.3833333 0.2914286 0.5775 0.5888889
## 40  0.440000 0.40 0.4666667 0.315 0.652000 0.5100000 0.4000000 0.4025 0.6755556
## 41  0.440000 0.39 0.6133333 0.665 0.684000 0.4566667 0.4714286 0.5075 0.7933333
## 42  0.420000 0.61 0.3600000 0.475 0.404000 0.4666667 0.7171429 0.4075 0.4533333
## 43  0.380000 0.36 0.4666667 0.435 0.624000 0.4933333 0.5028571 0.4425 0.6822222
## 44  0.400000 0.44 0.4733333 0.400 0.512000 0.4733333 0.4857143 0.2875 0.6600000
## 45  0.320000 0.50 0.4733333 0.535 0.356000 0.5233333 0.6342857 0.4575 0.3488889
## 46  0.640000 0.51 0.3533333 0.345 0.480000 0.5200000 0.5914286 0.2100 0.4444444
## 47  0.400000 0.42 0.7400000 0.515 0.520000 0.4466667 0.5914286 0.4325 0.3622222
## 48  0.280000 0.47 0.4933333 0.360 0.560000 0.5033333 0.4742857 0.7500 0.4177778
## 49  0.380000 0.52 0.2066667 0.395 0.368000 0.5366667 0.5314286 0.1950 0.3933333
## 50  0.480000 0.80 0.4800000 0.400 0.440000 0.4100000 0.3971429 0.3650 0.3844444
## 51  0.460000 0.22 0.4000000 0.375 0.472000 0.4433333 0.6257143 0.6600 0.4466667
## 52  0.520000 0.47 0.5000000 0.590 0.560000 0.5100000 0.3342857 0.5425 0.4688889
## 53  0.560000 0.65 0.3000000 0.445 0.504000 0.5966667 0.4085714 0.5175 0.6577778
## 54  0.520000 0.69 0.4466667 0.345 0.360000 0.2533333 0.3085714 0.4600 0.7222222
## 55  0.400000 0.46 0.4800000 0.660 0.404000 0.6000000 0.6657143 0.4575 0.5400000
## 56  0.560000 0.67 0.3933333 0.550 0.464000 0.3900000 0.3000000 0.4650 0.4866667
## 57  0.560000 0.46 0.4666667 0.425 0.528000 0.5700000 0.4685714 0.3600 0.4422222
## 58  0.380000 0.53 0.3333333 0.365 0.648000 0.5266667 0.4542857 0.4900 0.4488889
## 59  0.580000 0.64 0.4866667 0.620 0.380000 0.4466667 0.4771429 0.4675 0.3644444
## 60  0.460000 0.45 0.2550336 0.515 0.332000 0.4333333 0.4657143 0.5575 0.2711111
## 61  0.480000 0.47 0.5200000 0.460 0.404000 0.4600000 0.4057143 0.4175 0.5622222
## 62  0.420000 0.49 0.4000000 0.495 0.388000 0.6500000 0.3828571 0.5350 0.4644444
## 63  0.400000 0.38 0.3866667 0.400 0.676000 0.4433333 0.3857143 0.5525 0.4488889
## 64  0.400000 0.42 0.4400000 0.470 0.376000 0.7500000 0.4771429 0.7075 0.4933333
## 65  0.580000 0.51 0.5800000 0.360 0.488000 0.3033333 0.4685714 0.4775 0.2866667
## 66  0.420000 0.45 0.4933333 0.455 0.396000 0.4966667 0.6857143 0.7500 0.4111111
## 67  0.520000 0.48 0.5133333 0.625 0.532000 0.6633333 0.4714286 0.3275 0.3266667
## 68  0.440000 0.58 0.6333333 0.490 0.516000 0.5100000 0.6514286 0.4650 0.6400000
## 69  0.520000 0.65 0.5866667 0.330 0.872000 0.5666667 0.6085714 0.4650 0.4822222
## 70  0.500000 0.33 0.3666667 0.420 0.464000 0.5133333 0.4114286 0.3825 0.4088889
## 71  0.620000 0.46 0.5933333 0.610 0.552000 0.7500000 0.2228571 0.4175 0.6355556
## 72  0.600000 0.45 0.4000000 0.470 0.552000 0.3766667 0.3914286 0.5925 0.6977778
## 73  0.300000 0.40 0.4933333 0.475 0.464000 0.5633333 0.5942857 0.4850 0.5088889
## 74  0.720000 0.43 0.5133333 0.550 0.568000 0.6400000 0.4914286 0.5050 0.4822222
## 75  0.480000 0.47 0.3666667 0.465 0.656000 0.4633333 0.3714286 0.2975 0.2866667
## 76  0.620000 0.52 0.4333333 0.345 0.448000 0.5733333 0.7507163 0.4750 0.3888889
## 77  0.560000 0.41 0.4466667 0.635 0.384000 0.5233333 0.4600000 0.6600 0.5577778
## 78  0.380000 0.40 0.4400000 0.330 0.712000 0.4200000 0.4628571 0.4800 0.4044444
## 79  0.460000 0.62 0.2733333 0.360 0.416000 0.4600000 0.6971429 0.4300 0.5377778
## 80  0.420000 0.30 0.6866667 0.490 0.396000 0.4366667 0.3771429 0.4100 0.4066667
## 81  0.700000 0.52 0.4066667 0.410 0.512000 0.3433333 0.6657143 0.3875 0.6288889
## 82  0.755102 0.44 0.3266667 0.460 0.572000 0.4433333 0.3857143 0.4825 0.5088889
## 83  0.640000 0.42 0.1733333 0.495 0.668000 0.5100000 0.6514286 0.4875 0.4666667
## 84  0.640000 0.37 0.4266667 0.455 0.564000 0.6366667 0.4171429 0.6575 0.2955556
## 85  0.340000 0.54 0.3866667 0.380 0.388000 0.4633333 0.1885714 0.4525 0.4844444
## 86  0.500000 0.65 0.3133333 0.390 0.376000 0.4466667 0.5657143 0.4500 0.6511111
## 87  0.300000 0.41 0.7516779 0.750 0.404000 0.6166667 0.5285714 0.5250 0.2866667
## 88  0.540000 0.47 0.5133333 0.390 0.432000 0.4133333 0.4914286 0.4100 0.6933333
## 89  0.520000 0.29 0.5133333 0.645 0.540000 0.4000000 0.4685714 0.5000 0.4088889
## 90  0.300000 0.65 0.4733333 0.510 0.592000 0.6466667 0.4171429 0.5350 0.4488889
## 91  0.500000 0.41 0.4600000 0.510 0.253012 0.7500000 0.5828571 0.6275 0.3444444
## 92  0.460000 0.49 0.6466667 0.475 0.588000 0.4800000 0.5457143 0.5550 0.6088889
## 93  0.500000 0.71 0.4800000 0.405 0.512000 0.6533333 0.6171429 0.4650 0.4577778
## 94  0.580000 0.50 0.6200000 0.370 0.452000 0.4366667 0.3457143 0.4600 0.5911111
## 95  0.240000 0.27 0.2266667 0.405 0.751004 0.4633333 0.5142857 0.3300 0.3977778
## 96  0.500000 0.40 0.5866667 0.505 0.396000 0.5500000 0.5228571 0.2450 0.7066667
## 97  0.480000 0.52 0.4933333 0.405 0.440000 0.3933333 0.4142857 0.7000 0.6177778
## 98  0.460000 0.54 0.4733333 0.465 0.496000 0.3800000 0.4942857 0.4475 0.2911111
## 99  0.360000 0.40 0.6266667 0.640 0.452000 0.4700000 0.4485714 0.5400 0.4466667
## 100 0.460000 0.41 0.6200000 0.585 0.564000 0.5033333 0.4914286 0.6600 0.6377778
##       V10       V11       V12       V13       V14       V15     V16       V17
## 1   0.438 0.4745455 0.8233333 0.3938462 0.4557143 0.5600000 0.75000 0.4788235
## 2   0.520 0.5127273 0.6383333 0.3015385 0.6557143 0.7013333 0.45625 0.5105882
## 3   0.464 0.6854545 0.2833333 0.7503852 0.4957143 0.5906667 0.75000 0.6917647
## 4   0.558 0.4527273 0.6033333 0.4184615 0.7500000 0.7026667 0.56250 0.5788235
## 5   0.448 0.7504554 0.4200000 0.5353846 0.4642857 0.6426667 0.80875 0.4447059
## 6   0.470 0.7163636 0.3483333 0.5369231 0.5442857 0.4680000 0.50250 0.3705882
## 7   0.500 0.4563636 0.5233333 0.4553846 0.1971429 0.6426667 0.48625 0.5811765
## 8   0.474 0.3600000 0.4683333 0.2923077 0.6357143 0.4586667 0.51500 0.4894118
## 9   0.512 0.3872727 0.4850000 0.4338462 0.5300000 0.3666667 0.30875 0.4623529
## 10  0.582 0.8036364 0.7500000 0.5369231 0.4971429 0.6586667 0.46125 0.3470588
## 11  0.412 0.5127273 0.4616667 0.5861538 0.6200000 0.6600000 0.44125 0.3976471
## 12  0.652 0.4909091 0.4116667 0.2969231 0.4042857 0.4840000 0.64250 0.5529412
## 13  0.644 0.5890909 0.4133333 0.4123077 0.3928571 0.4506667 0.54750 0.3964706
## 14  0.656 0.4400000 0.4433333 0.3584615 0.7000000 0.4613333 0.46125 0.4576471
## 15  0.466 0.4436364 0.3883333 0.4707692 0.4457143 0.5520000 0.48750 0.6011765
## 16  0.372 0.5054545 0.6533333 0.4353846 0.4514286 0.6493333 0.45625 0.5447059
## 17  0.644 0.4054545 0.5866667 0.5430769 0.5228571 0.6880000 0.75000 0.4094118
## 18  0.344 0.4490909 0.3116667 0.4584615 0.4300000 0.4640000 0.51125 0.6564706
## 19  0.588 0.5018182 0.7500000 0.3446154 0.5085714 0.3506667 0.61750 0.5411765
## 20  0.622 0.4727273 0.4416667 0.4600000 0.7500000 0.4573333 0.43375 0.6505882
## 21  0.604 0.3927273 0.3000000 0.4523077 0.6042857 0.6293333 0.40125 0.5047059
## 22  0.438 0.4418182 0.4583333 0.6123077 0.7942857 0.6520000 0.44500 0.5070588
## 23  0.552 0.6436364 0.4300000 0.5107692 0.6014286 0.5906667 0.46125 0.4682353
## 24  0.410 0.4400000 0.3916667 0.3446154 0.6000000 0.3906667 0.35250 0.6105882
## 25  0.496 0.5109091 0.4633333 0.2511556 0.4157143 0.4186667 0.34500 0.5564706
## 26  0.470 0.6163636 0.4650000 0.5907692 0.6085714 0.6386667 0.69500 0.4376471
## 27  0.750 0.5127273 0.6933333 0.3461538 0.4528571 0.4440000 0.49750 0.4364706
## 28  0.412 0.3672727 0.4550000 0.6692308 0.5471429 0.4640000 0.51250 0.6494118
## 29  0.418 0.4581818 0.4566667 0.5015385 0.7028571 0.7066667 0.50375 0.4576471
## 30  0.426 0.4545455 0.7500000 0.6984615 0.4400000 0.4573333 0.38250 0.4470588
## 31  0.590 0.2036364 0.4800000 0.4476923 0.4585714 0.5013333 0.45500 0.4023529
## 32  0.608 0.6254545 0.5100000 0.3584615 0.4085714 0.3306667 0.40000 0.4494118
## 33  0.660 0.4945455 0.3400000 0.4507692 0.3985714 0.4680000 0.34250 0.4505882
## 34  0.316 0.6800000 0.4250000 0.3923077 0.6142857 0.4253333 0.50500 0.6635294
## 35  0.500 0.3345455 0.5033333 0.8384615 0.5428571 0.3826667 0.53625 0.5905882
## 36  0.626 0.4636364 0.6950000 0.5030769 0.6514286 0.3626667 0.59375 0.4694118
## 37  0.436 0.6345455 0.4716667 0.5092308 0.6171429 0.7106667 0.25000 0.4800000
## 38  0.606 0.4454545 0.3500000 0.5676923 0.3957143 0.4066667 0.33875 0.4341176
## 39  0.590 0.4472727 0.5066667 0.5815385 0.4414286 0.4720000 0.44875 0.5341176
## 40  0.908 0.4690909 0.7066667 0.4538462 0.3614286 0.3533333 0.69750 0.6929412
## 41  0.662 0.3418182 0.3633333 0.4476923 0.5100000 0.4706667 0.53750 0.4447059
## 42  0.456 0.3036364 0.4216667 0.3969231 0.4742857 0.4600000 0.39875 0.4364706
## 43  0.696 0.4236364 0.4966667 0.6369231 0.5914286 0.3893333 0.72000 0.4517647
## 44  0.352 0.4090909 0.3916667 0.4615385 0.6042857 0.4360000 0.49750 0.5847059
## 45  0.384 0.4363636 0.3933333 0.4353846 0.6128571 0.3920000 0.50375 0.4482353
## 46  0.720 0.5981818 0.4383333 0.4569231 0.4457143 0.4560000 0.62375 0.3388235
## 47  0.390 0.6854545 0.5883333 0.6615385 0.4071429 0.3560000 0.80750 0.6505882
## 48  0.424 0.2495446 0.4550000 0.4969231 0.4657143 0.4893333 0.41750 0.3870588
## 49  0.434 0.7504554 0.3633333 0.3646154 0.6128571 0.3333333 0.63875 0.7247059
## 50  0.612 0.4745455 0.3950000 0.7503852 0.4271429 0.4493333 0.47500 0.5223529
## 51  0.450 0.6581818 0.4050000 0.5861538 0.4028571 0.4493333 0.64750 0.5070588
## 52  0.398 0.4345455 0.4133333 0.4107692 0.3857143 0.3426667 0.46500 0.4047059
## 53  0.556 0.3563636 0.4766667 0.4707692 0.5471429 0.4960000 0.38375 0.5541176
## 54  0.350 0.2495446 0.4683333 0.4984615 0.4685714 0.4120000 0.51125 0.4270588
## 55  0.438 0.6200000 0.5933333 0.3830769 0.4914286 0.5880000 0.53750 0.6764706
## 56  0.356 0.5036364 0.4500000 0.4707692 0.4442857 0.3946667 0.48125 0.3058824
## 57  0.458 0.7504554 0.4600000 0.3784615 0.4628571 0.3880000 0.46875 0.6482353
## 58  0.412 0.4600000 0.4800000 0.4876923 0.5971429 0.3560000 0.44000 0.4682353
## 59  0.460 0.4454545 0.6133333 0.4553846 0.6228571 0.4146667 0.51625 0.4905882
## 60  0.538 0.4000000 0.4600000 0.3907692 0.4257143 0.4466667 0.44750 0.4329412
## 61  0.450 0.6709091 0.4550000 0.3938462 0.5328571 0.3506667 0.45500 0.6988235
## 62  0.416 0.4054545 0.7233333 0.5000000 0.4485714 0.4866667 0.45250 0.5917647
## 63  0.390 0.6200000 0.8100000 0.4307692 0.6085714 0.7503338 0.45875 0.4729412
## 64  0.626 0.6709091 0.4883333 0.3953846 0.5885714 0.3746667 0.70625 0.5329412
## 65  0.646 0.6454545 0.3900000 0.5723077 0.4585714 0.4413333 0.68875 0.4588235
## 66  0.406 0.7504554 0.4500000 0.4569231 0.4300000 0.6186667 0.40875 0.4576471
## 67  0.572 0.4581818 0.4133333 0.3230769 0.6042857 0.5040000 0.48625 0.4552941
## 68  0.452 0.6054545 0.4766667 0.4661538 0.4028571 0.4866667 0.48625 0.5129412
## 69  0.360 0.5400000 0.4150000 0.5676923 0.3357143 0.3960000 0.70500 0.4470588
## 70  0.538 0.7504554 0.6933333 0.6015385 0.4385714 0.6560000 0.40875 0.4505882
## 71  0.470 0.3254545 0.4883333 0.4030769 0.4700000 0.5533333 0.46500 0.4823529
## 72  0.394 0.4545455 0.4650000 0.5784615 0.4042857 0.4240000 0.35875 0.4752941
## 73  0.458 0.6218182 0.6466667 0.6061538 0.3671429 0.4720000 0.40500 0.4705882
## 74  0.614 0.4563636 0.4316667 0.4661538 0.6900000 0.6133333 0.52500 0.3435294
## 75  0.480 0.3581818 0.4083333 0.4200000 0.3957143 0.5853333 0.57125 0.4929412
## 76  0.532 0.7018182 0.6366667 0.7107692 0.3171429 0.4493333 0.59125 0.5505882
## 77  0.376 0.4018182 0.2883333 0.5215385 0.3285714 0.4426667 0.75000 0.6670588
## 78  0.448 0.3981818 0.3483333 0.4676923 0.3942857 0.7503338 0.75000 0.4470588
## 79  0.470 0.4690909 0.6133333 0.5969231 0.5142857 0.4133333 0.44500 0.4576471
## 80  0.326 0.6927273 0.4933333 0.4476923 0.4342857 0.5000000 0.53625 0.6882353
## 81  0.460 0.6927273 0.4266667 0.4538462 0.4600000 0.4000000 0.37875 0.6447059
## 82  0.614 0.6872727 0.5050000 0.6169231 0.3557143 0.5440000 0.63125 0.4729412
## 83  0.660 0.4909091 0.4850000 0.4630769 0.4900000 0.4013333 0.45250 0.5105882
## 84  0.308 0.4054545 0.4000000 0.3476923 0.3428571 0.8066667 0.60500 0.5882353
## 85  0.480 0.7504554 0.4216667 0.4984615 0.4485714 0.5133333 0.49375 0.6258824
## 86  0.484 0.4581818 0.4900000 0.4030769 0.3528571 0.5906667 0.39875 0.4482353
## 87  0.340 0.4345455 0.5916667 0.4492308 0.5557143 0.6560000 0.58625 0.4000000
## 88  0.696 0.4200000 0.4600000 0.3492308 0.5071429 0.4106667 0.71500 0.3482353
## 89  0.450 0.4854545 0.6333333 0.5153846 0.4400000 0.6026667 0.47875 0.4529412
## 90  0.462 0.5709091 0.4266667 0.4123077 0.4614286 0.4653333 0.60125 0.4741176
## 91  0.516 0.7504554 0.4383333 0.6200000 0.4500000 0.7013333 0.65125 0.6411765
## 92  0.464 0.4545455 0.4950000 0.3861538 0.4157143 0.5426667 0.47000 0.6141176
## 93  0.414 0.4036364 0.3883333 0.3984615 0.5128571 0.4586667 0.46500 0.6364706
## 94  0.518 0.6436364 0.5533333 0.3815385 0.5114286 0.7503338 0.40000 0.5235294
## 95  0.458 0.4054545 0.6116667 0.4523077 0.3957143 0.7503338 0.40125 0.6388235
## 96  0.750 0.6018182 0.7083333 0.4338462 0.4114286 0.4986667 0.64500 0.5882353
## 97  0.408 0.4145455 0.3466667 0.2984615 0.5014286 0.6600000 0.33000 0.4870588
## 98  0.388 0.4181818 0.4250000 0.5369231 0.3985714 0.3586667 0.40125 0.4988235
## 99  0.676 0.4436364 0.4983333 0.5000000 0.4500000 0.7503338 0.53375 0.3164706
## 100 0.456 0.6181818 0.3183333 0.5446154 0.3414286 0.5040000 0.78625 0.4482353
##           V18       V19   V20
## 1   0.7066667 0.6536842 0.596
## 2   0.6611111 0.5957895 0.397
## 3   0.6022222 0.4431579 0.394
## 4   0.5044444 0.4736842 0.499
## 5   0.3977778 0.4968421 0.445
## 6   0.4077778 0.6421053 0.614
## 7   0.5777778 0.2518440 0.455
## 8   0.4877778 0.3515789 0.493
## 9   0.5066667 0.6073684 0.457
## 10  0.3100000 0.3957895 0.543
## 11  0.3400000 0.4526316 0.447
## 12  0.4522222 0.5926316 0.489
## 13  0.3311111 0.6273684 0.448
## 14  0.5077778 0.6031579 0.709
## 15  0.4500000 0.4515789 0.504
## 16  0.4155556 0.5947368 0.646
## 17  0.4755556 0.6442105 0.417
## 18  0.3455556 0.4947368 0.417
## 19  0.4511111 0.5200000 0.348
## 20  0.3977778 0.4978947 0.411
## 21  0.7500000 0.2926316 0.354
## 22  0.5955556 0.5252632 0.647
## 23  0.3588889 0.6778947 0.704
## 24  0.6000000 0.6968421 0.486
## 25  0.4566667 0.4431579 0.514
## 26  0.6444444 0.7168421 0.616
## 27  0.6544444 0.5568421 0.452
## 28  0.4388889 0.6705263 0.467
## 29  0.5977778 0.4031579 0.337
## 30  0.4400000 0.3473684 0.519
## 31  0.6088889 0.3073684 0.488
## 32  0.4011111 0.8105263 0.458
## 33  0.4988889 0.4568421 0.455
## 34  0.6877778 0.2497366 0.647
## 35  0.5266667 0.4663158 0.602
## 36  0.6188889 0.6863158 0.611
## 37  0.6466667 0.7502634 0.465
## 38  0.5044444 0.6547368 0.560
## 39  0.5411111 0.4494737 0.659
## 40  0.4155556 0.6989474 0.445
## 41  0.4611111 0.3494737 0.651
## 42  0.4955556 0.6357895 0.592
## 43  0.3611111 0.3168421 0.462
## 44  0.4477778 0.2947368 0.362
## 45  0.6477778 0.3926316 0.446
## 46  0.4088889 0.7502634 0.348
## 47  0.6011111 0.3957895 0.459
## 48  0.4766667 0.6305263 0.640
## 49  0.3711111 0.4452632 0.442
## 50  0.6011111 0.4757895 0.448
## 51  0.2877778 0.4663158 0.361
## 52  0.6977778 0.4631579 0.460
## 53  0.4733333 0.7115789 0.702
## 54  0.3555556 0.4368421 0.346
## 55  0.7500000 0.5894737 0.442
## 56  0.3388889 0.4831579 0.647
## 57  0.6722222 0.5936842 0.750
## 58  0.6922222 0.5926316 0.394
## 59  0.3966667 0.3536842 0.676
## 60  0.4444444 0.4126316 0.493
## 61  0.5955556 0.6031579 0.520
## 62  0.3811111 0.7042105 0.518
## 63  0.3500000 0.3652632 0.574
## 64  0.6977778 0.5431579 0.542
## 65  0.7500000 0.7105263 0.493
## 66  0.5111111 0.4042105 0.520
## 67  0.4822222 0.6800000 0.462
## 68  0.6988889 0.3673684 0.408
## 69  0.4011111 0.4442105 0.359
## 70  0.4588889 0.6421053 0.659
## 71  0.4577778 0.4873684 0.750
## 72  0.5533333 0.4557895 0.420
## 73  0.4855556 0.5947368 0.412
## 74  0.4566667 0.6021053 0.294
## 75  0.4622222 0.6021053 0.448
## 76  0.5811111 0.6800000 0.513
## 77  0.5188889 0.5589474 0.500
## 78  0.4055556 0.4400000 0.352
## 79  0.4500000 0.6410526 0.582
## 80  0.7500000 0.4747368 0.457
## 81  0.4477778 0.5094737 0.497
## 82  0.4433333 0.4042105 0.606
## 83  0.4566667 0.6452632 0.544
## 84  0.4966667 0.4621053 0.400
## 85  0.4622222 0.3568421 0.444
## 86  0.4811111 0.4378947 0.462
## 87  0.3988889 0.4684211 0.339
## 88  0.4411111 0.6484211 0.641
## 89  0.6166667 0.5178947 0.375
## 90  0.4422222 0.6084211 0.450
## 91  0.4933333 0.4884211 0.376
## 92  0.4022222 0.5157895 0.810
## 93  0.4855556 0.4505263 0.404
## 94  0.5944444 0.4547368 0.446
## 95  0.4977778 0.4052632 0.366
## 96  0.6300000 0.5442105 0.413
## 97  0.3433333 0.4368421 0.405
## 98  0.3788889 0.5094737 0.634
## 99  0.4600000 0.4494737 0.593
## 100 0.2955556 0.6094737 0.621
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as.data.frame}\NormalTok{(}\AttributeTok{x =}\NormalTok{ qda.err)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            V1   V2        V3    V4        V5        V6        V7     V8
## 1   0.4000000 0.54 0.3866667 0.435 0.4880000 0.3733333 0.2808023 0.4900
## 2   0.5600000 0.46 0.4066667 0.450 0.3000000 0.5166667 0.4800000 0.4800
## 3   0.5000000 0.43 0.4066667 0.365 0.4480000 0.4533333 0.5571429 0.3200
## 4   0.7551020 0.55 0.3466667 0.420 0.3360000 0.4833333 0.4457143 0.4150
## 5   0.5200000 0.43 0.5133333 0.410 0.5040000 0.4800000 0.2693410 0.3675
## 6   0.4400000 0.47 0.4600000 0.525 0.4880000 0.4766667 0.4285714 0.2750
## 7   0.3200000 0.20 0.6933333 0.390 0.3880000 0.4133333 0.5257143 0.5075
## 8   0.3200000 0.44 0.4333333 0.605 0.5760000 0.4633333 0.4600000 0.4125
## 9   0.3600000 0.38 0.3133333 0.335 0.4440000 0.4466667 0.3542857 0.4825
## 10  0.5600000 0.48 0.2483221 0.525 0.5000000 0.5000000 0.5657143 0.4000
## 11  0.4600000 0.67 0.5200000 0.315 0.5560000 0.4300000 0.3828571 0.4400
## 12  0.3800000 0.40 0.3800000 0.510 0.4200000 0.4300000 0.4200000 0.5100
## 13  0.4800000 0.64 0.4333333 0.445 0.4680000 0.4700000 0.8228571 0.5075
## 14  0.4200000 0.50 0.4800000 0.375 0.4760000 0.5566667 0.2521490 0.3950
## 15  0.5200000 0.52 0.4466667 0.475 0.3960000 0.4533333 0.5000000 0.4725
## 16  0.4200000 0.46 0.4200000 0.565 0.4280000 0.5566667 0.4371429 0.4650
## 17  0.3200000 0.57 0.3733333 0.490 0.5600000 0.6466667 0.4714286 0.4425
## 18  0.3000000 0.54 0.4600000 0.490 0.4440000 0.7500000 0.4200000 0.5200
## 19  0.3600000 0.56 0.4466667 0.690 0.4280000 0.4400000 0.3057143 0.4325
## 20  0.5000000 0.40 0.8000000 0.370 0.2200000 0.4833333 0.8028571 0.5200
## 21  0.5200000 0.44 0.4466667 0.335 0.5240000 0.7000000 0.4857143 0.4800
## 22  0.7000000 0.47 0.2733333 0.615 0.4480000 0.4700000 0.4428571 0.6025
## 23  0.6000000 0.65 0.3200000 0.440 0.4760000 0.5000000 0.4942857 0.4825
## 24  0.5800000 0.44 0.4733333 0.455 0.4120000 0.5033333 0.2578797 0.4725
## 25  0.3800000 0.43 0.5000000 0.485 0.4800000 0.4600000 0.5085714 0.3800
## 26  0.3061224 0.45 0.1466667 0.405 0.4480000 0.4933333 0.5942857 0.4850
## 27  0.5000000 0.54 0.5000000 0.465 0.4720000 0.2533333 0.4114286 0.3025
## 28  0.2400000 0.45 0.4733333 0.485 0.4080000 0.6700000 0.5257143 0.4425
## 29  0.3800000 0.48 0.6133333 0.550 0.3840000 0.4133333 0.4457143 0.2525
## 30  0.3200000 0.52 0.4866667 0.485 0.4240000 0.4500000 0.3942857 0.8900
## 31  0.4000000 0.44 0.3200000 0.280 0.4000000 0.3133333 0.4942857 0.3750
## 32  0.3800000 0.40 0.5333333 0.380 0.5000000 0.5666667 0.4285714 0.2650
## 33  0.4200000 0.38 0.4800000 0.480 0.4480000 0.3766667 0.4914286 0.3050
## 34  0.5600000 0.42 0.5066667 0.430 0.3520000 0.4333333 0.3057143 0.5050
## 35  0.4000000 0.56 0.5266667 0.640 0.4360000 0.3833333 0.4714286 0.5275
## 36  0.5000000 0.24 0.4066667 0.375 0.4440000 0.4000000 0.5428571 0.4550
## 37  0.4600000 0.48 0.4933333 0.585 0.4240000 0.5800000 0.4228571 0.3350
## 38  0.4800000 0.40 0.4600000 0.440 0.3400000 0.5100000 0.3037249 0.3075
## 39  0.5200000 0.47 0.4266667 0.565 0.4080000 0.4266667 0.3114286 0.5300
## 40  0.3800000 0.35 0.4733333 0.350 0.6520000 0.5166667 0.4000000 0.4150
## 41  0.6000000 0.52 0.5666667 0.665 0.3360000 0.4400000 0.4685714 0.4875
## 42  0.4800000 0.45 0.3866667 0.480 0.4000000 0.4533333 0.2971429 0.4450
## 43  0.5800000 0.34 0.4933333 0.500 0.6240000 0.5300000 0.4800000 0.5575
## 44  0.4400000 0.56 0.5066667 0.400 0.4880000 0.4700000 0.4800000 0.3150
## 45  0.6600000 0.55 0.4600000 0.535 0.3600000 0.5066667 0.3685714 0.4375
## 46  0.5000000 0.50 0.4333333 0.350 0.5680000 0.4933333 0.5428571 0.2275
## 47  0.5200000 0.48 0.7400000 0.470 0.4800000 0.5200000 0.5057143 0.4325
## 48  0.3000000 0.49 0.5200000 0.350 0.4400000 0.4700000 0.4857143 0.2550
## 49  0.4800000 0.47 0.2066667 0.515 0.3680000 0.5233333 0.5057143 0.2325
## 50  0.6400000 0.80 0.4866667 0.420 0.4720000 0.4233333 0.4171429 0.3650
## 51  0.4800000 0.24 0.4200000 0.390 0.4560000 0.4666667 0.3828571 0.3600
## 52  0.4600000 0.53 0.4866667 0.415 0.4400000 0.4966667 0.3400000 0.5500
## 53  0.5000000 0.36 0.3000000 0.545 0.5400000 0.4066667 0.4057143 0.5125
## 54  0.5600000 0.69 0.5266667 0.350 0.3600000 0.2800000 0.3085714 0.4700
## 55  0.4000000 0.46 0.4533333 0.350 0.4280000 0.3833333 0.3342857 0.4400
## 56  0.5800000 0.47 0.4733333 0.545 0.4920000 0.4000000 0.2914286 0.4675
## 57  0.7200000 0.45 0.4400000 0.435 0.4720000 0.4333333 0.5171429 0.3700
## 58  0.3000000 0.48 0.3600000 0.375 0.3640000 0.4666667 0.5257143 0.4650
## 59  0.5000000 0.55 0.5066667 0.410 0.4040000 0.5300000 0.4828571 0.4700
## 60  0.3400000 0.45 0.3020134 0.480 0.5880000 0.4333333 0.4714286 0.5100
## 61  0.4400000 0.56 0.5600000 0.475 0.4160000 0.4800000 0.4485714 0.4250
## 62  0.4400000 0.64 0.3733333 0.500 0.3840000 0.3466667 0.3857143 0.5150
## 63  0.4400000 0.39 0.4066667 0.435 0.3200000 0.5166667 0.3971429 0.4750
## 64  0.3800000 0.51 0.4933333 0.475 0.5640000 0.7500000 0.4828571 0.3400
## 65  0.5400000 0.50 0.4600000 0.380 0.4360000 0.3100000 0.4914286 0.4900
## 66  0.5600000 0.52 0.4533333 0.455 0.6080000 0.4733333 0.3457143 0.2650
## 67  0.5400000 0.54 0.4733333 0.460 0.5240000 0.6633333 0.5142857 0.3275
## 68  0.4800000 0.57 0.4000000 0.500 0.4960000 0.4900000 0.3542857 0.4725
## 69  0.5200000 0.56 0.4933333 0.340 0.8720000 0.4100000 0.4371429 0.4775
## 70  0.5200000 0.37 0.3800000 0.435 0.4880000 0.5033333 0.4371429 0.4150
## 71  0.3600000 0.54 0.3866667 0.610 0.4600000 0.7500000 0.2400000 0.4525
## 72  0.6000000 0.44 0.4466667 0.495 0.4720000 0.3833333 0.4171429 0.5925
## 73  0.3800000 0.49 0.4933333 0.475 0.4640000 0.5100000 0.4171429 0.5150
## 74  0.7200000 0.58 0.5333333 0.580 0.4280000 0.3833333 0.5228571 0.4850
## 75  0.4600000 0.40 0.5866667 0.485 0.6560000 0.5066667 0.4000000 0.2975
## 76  0.6000000 0.51 0.4266667 0.375 0.4560000 0.5200000 0.2607450 0.5000
## 77  0.6000000 0.41 0.4533333 0.370 0.4160000 0.4800000 0.5200000 0.3625
## 78  0.3400000 0.53 0.4400000 0.375 0.7120000 0.4300000 0.4628571 0.5025
## 79  0.5200000 0.62 0.3133333 0.380 0.4200000 0.4800000 0.3114286 0.5150
## 80  0.6000000 0.32 0.5266667 0.515 0.4120000 0.4533333 0.4057143 0.4075
## 81  0.3200000 0.55 0.4333333 0.520 0.4760000 0.3433333 0.3885714 0.3850
## 82  0.2857143 0.46 0.3400000 0.445 0.5720000 0.5266667 0.3942857 0.4925
## 83  0.4400000 0.42 0.2266667 0.470 0.3280000 0.4966667 0.5400000 0.5125
## 84  0.4400000 0.37 0.4066667 0.440 0.5320000 0.3666667 0.4200000 0.3475
## 85  0.4000000 0.47 0.4866667 0.415 0.4440000 0.5033333 0.1914286 0.4700
## 86  0.5600000 0.38 0.3266667 0.585 0.3760000 0.4466667 0.4371429 0.4775
## 87  0.3600000 0.40 0.7516779 0.750 0.4040000 0.3900000 0.5342857 0.4775
## 88  0.5400000 0.44 0.5000000 0.415 0.4480000 0.5566667 0.4742857 0.4275
## 89  0.4200000 0.32 0.5333333 0.350 0.5320000 0.4333333 0.4742857 0.4925
## 90  0.4800000 0.52 0.4933333 0.535 0.4280000 0.6466667 0.4200000 0.4800
## 91  0.4600000 0.42 0.4866667 0.500 0.2851406 0.3166667 0.4171429 0.4225
## 92  0.4600000 0.48 0.3666667 0.445 0.5880000 0.5533333 0.5457143 0.5300
## 93  0.5200000 0.34 0.4800000 0.395 0.5040000 0.3733333 0.4085714 0.5175
## 94  0.5200000 0.46 0.5533333 0.405 0.5800000 0.4533333 0.3457143 0.4600
## 95  0.2600000 0.31 0.2266667 0.400 0.2530120 0.4900000 0.5400000 0.3625
## 96  0.5400000 0.41 0.5466667 0.505 0.4680000 0.5000000 0.5057143 0.2550
## 97  0.4000000 0.48 0.5066667 0.415 0.4760000 0.4033333 0.4085714 0.7000
## 98  0.4800000 0.45 0.4666667 0.435 0.5080000 0.3866667 0.5085714 0.5400
## 99  0.6600000 0.44 0.6266667 0.640 0.4560000 0.4566667 0.4428571 0.5525
## 100 0.5600000 0.42 0.6200000 0.400 0.5000000 0.5133333 0.4914286 0.6600
##            V9   V10       V11       V12       V13       V14       V15     V16
## 1   0.2111111 0.438 0.4836364 0.8233333 0.4107692 0.5500000 0.4360000 0.25125
## 2   0.3022222 0.520 0.4981818 0.3783333 0.3015385 0.3628571 0.7013333 0.46500
## 3   0.6111111 0.482 0.6854545 0.2966667 0.2557781 0.4971429 0.4160000 0.75000
## 4   0.4733333 0.558 0.4600000 0.6033333 0.4353846 0.2614286 0.3080000 0.44000
## 5   0.5088889 0.450 0.7504554 0.4233333 0.5184615 0.5228571 0.6426667 0.21125
## 6   0.3822222 0.522 0.7163636 0.3733333 0.5200000 0.4571429 0.5133333 0.53375
## 7   0.2933333 0.494 0.4709091 0.5066667 0.4615385 0.2042857 0.6426667 0.55750
## 8   0.4644444 0.498 0.3672727 0.4666667 0.3153846 0.3671429 0.5080000 0.50500
## 9   0.4088889 0.498 0.4036364 0.5083333 0.5553846 0.5014286 0.5466667 0.30750
## 10  0.3644444 0.408 0.2036364 0.2850000 0.5369231 0.4842857 0.3453333 0.46625
## 11  0.3977778 0.422 0.5218182 0.4483333 0.5861538 0.5357143 0.3453333 0.44250
## 12  0.5088889 0.348 0.5000000 0.4150000 0.3046154 0.4071429 0.4906667 0.35125
## 13  0.3511111 0.374 0.4236364 0.4283333 0.4230769 0.4114286 0.4440000 0.47500
## 14  0.5044444 0.656 0.4600000 0.4516667 0.3584615 0.3028571 0.4733333 0.48000
## 15  0.2427617 0.472 0.5145455 0.3966667 0.4846154 0.5328571 0.4480000 0.47375
## 16  0.3911111 0.586 0.5127273 0.3500000 0.4369231 0.4514286 0.6493333 0.50875
## 17  0.5088889 0.644 0.4109091 0.4166667 0.4784615 0.5100000 0.3146667 0.25625
## 18  0.4244444 0.356 0.5600000 0.3316667 0.5446154 0.4457143 0.4906667 0.51375
## 19  0.4022222 0.588 0.4945455 0.2533333 0.3630769 0.5000000 0.3600000 0.39250
## 20  0.4044444 0.382 0.4909091 0.4433333 0.4692308 0.7500000 0.4680000 0.44875
## 21  0.1488889 0.456 0.4018182 0.3100000 0.4615385 0.4057143 0.3640000 0.40625
## 22  0.5266667 0.440 0.5090909 0.4816667 0.6123077 0.2200000 0.3733333 0.44500
## 23  0.4000000 0.452 0.3927273 0.4400000 0.4984615 0.4442857 0.4266667 0.47750
## 24  0.8511111 0.410 0.5800000 0.4100000 0.3630769 0.4014286 0.3906667 0.37250
## 25  0.3844444 0.490 0.5127273 0.4550000 0.2727273 0.4485714 0.4266667 0.34500
## 26  0.5133333 0.488 0.6163636 0.4750000 0.4169231 0.6085714 0.6386667 0.30375
## 27  0.4355556 0.750 0.5236364 0.3150000 0.3630769 0.4314286 0.4453333 0.51750
## 28  0.5177778 0.432 0.3836364 0.4600000 0.3415385 0.5357143 0.4733333 0.50625
## 29  0.6155556 0.418 0.4981818 0.4683333 0.4784615 0.3085714 0.7066667 0.48500
## 30  0.2739421 0.458 0.4600000 0.7500000 0.3092308 0.4857143 0.4680000 0.38875
## 31  0.3711111 0.406 0.2200000 0.5066667 0.4600000 0.5614286 0.5106667 0.46750
## 32  0.4977778 0.396 0.6254545 0.4833333 0.3584615 0.4157143 0.3493333 0.40000
## 33  0.4777778 0.354 0.4836364 0.3550000 0.4492308 0.4085714 0.4720000 0.34750
## 34  0.3577778 0.340 0.3236364 0.4400000 0.3907692 0.5142857 0.5213333 0.50250
## 35  0.4222222 0.494 0.3490909 0.4800000 0.8384615 0.5157143 0.3826667 0.46875
## 36  0.4666667 0.626 0.4981818 0.3100000 0.5107692 0.3700000 0.3626667 0.40125
## 37  0.4177778 0.462 0.5290909 0.4783333 0.4846154 0.3842857 0.2906667 0.29375
## 38  0.4933333 0.540 0.4563636 0.3966667 0.5430769 0.4128571 0.4066667 0.37125
## 39  0.4088889 0.526 0.4490909 0.5250000 0.4246154 0.4400000 0.4666667 0.45500
## 40  0.3222222 0.908 0.4727273 0.3033333 0.5276923 0.3585714 0.3733333 0.30500
## 41  0.7933333 0.336 0.3418182 0.3866667 0.4800000 0.5214286 0.4666667 0.47500
## 42  0.4533333 0.544 0.3272727 0.4400000 0.4015385 0.4700000 0.5013333 0.41875
## 43  0.3288889 0.324 0.4236364 0.4983333 0.3846154 0.4100000 0.4266667 0.72000
## 44  0.6600000 0.382 0.5927273 0.4300000 0.4600000 0.4828571 0.5160000 0.47750
## 45  0.3644444 0.388 0.5200000 0.4033333 0.4538462 0.4000000 0.3933333 0.49625
## 46  0.4400000 0.310 0.4400000 0.4850000 0.4630769 0.4457143 0.4746667 0.37500
## 47  0.3555556 0.418 0.3163636 0.4116667 0.3615385 0.4128571 0.3733333 0.80750
## 48  0.4244444 0.440 0.2513661 0.4666667 0.4892308 0.4628571 0.4933333 0.45000
## 49  0.6044444 0.444 0.7504554 0.4016667 0.3861538 0.4057143 0.3480000 0.37000
## 50  0.4333333 0.612 0.4709091 0.3966667 0.7503852 0.4357143 0.4573333 0.47625
## 51  0.5088889 0.456 0.3600000 0.4500000 0.4169231 0.4028571 0.4613333 0.35375
## 52  0.4711111 0.414 0.4490909 0.4183333 0.4215385 0.4000000 0.3533333 0.51250
## 53  0.3466667 0.528 0.3727273 0.4800000 0.4846154 0.5242857 0.5080000 0.41000
## 54  0.7222222 0.400 0.2622951 0.4700000 0.4984615 0.5328571 0.4480000 0.50625
## 55  0.4955556 0.444 0.3909091 0.4183333 0.5076923 0.4857143 0.5880000 0.47375
## 56  0.5111111 0.374 0.5090909 0.4500000 0.4661538 0.4585714 0.4186667 0.50625
## 57  0.4333333 0.462 0.7504554 0.4650000 0.3815385 0.4885714 0.3933333 0.45875
## 58  0.4466667 0.426 0.4654545 0.4816667 0.5723077 0.5971429 0.3573333 0.53375
## 59  0.3666667 0.478 0.4545455 0.4700000 0.5276923 0.4042857 0.4280000 0.49125
## 60  0.3022222 0.506 0.4181818 0.4533333 0.4246154 0.5471429 0.4533333 0.44375
## 61  0.5244444 0.456 0.3490909 0.4483333 0.4046154 0.5042857 0.3520000 0.55250
## 62  0.4644444 0.590 0.4054545 0.2833333 0.4969231 0.4542857 0.4840000 0.45125
## 63  0.5577778 0.392 0.4236364 0.2066667 0.4446154 0.4014286 0.2817089 0.45750
## 64  0.4977778 0.374 0.3363636 0.5100000 0.4046154 0.4214286 0.3826667 0.70625
## 65  0.3000000 0.360 0.6454545 0.4016667 0.5000000 0.4671429 0.4386667 0.68875
## 66  0.4311111 0.454 0.2495446 0.4633333 0.4538462 0.5157143 0.4053333 0.41000
## 67  0.3244444 0.514 0.4800000 0.5083333 0.3276923 0.4100000 0.5053333 0.48750
## 68  0.3844444 0.434 0.6054545 0.5133333 0.4738462 0.4028571 0.5040000 0.50125
## 69  0.5000000 0.372 0.4636364 0.4133333 0.5353846 0.3742857 0.4666667 0.70500
## 70  0.4022222 0.482 0.2622951 0.6933333 0.4384615 0.4414286 0.3533333 0.44750
## 71  0.3955556 0.470 0.3709091 0.5316667 0.4184615 0.4685714 0.5666667 0.47375
## 72  0.6977778 0.390 0.5581818 0.5383333 0.5784615 0.5985714 0.4360000 0.36250
## 73  0.4977778 0.438 0.3927273 0.3683333 0.4969231 0.3757143 0.4653333 0.40500
## 74  0.4711111 0.614 0.5018182 0.4366667 0.4953846 0.3142857 0.4013333 0.51750
## 75  0.3000000 0.478 0.3836364 0.4366667 0.4338462 0.3971429 0.4160000 0.42000
## 76  0.3977778 0.452 0.7018182 0.6366667 0.2953846 0.3242857 0.4546667 0.42625
## 77  0.4444444 0.390 0.3927273 0.3383333 0.5323077 0.3400000 0.4520000 0.26250
## 78  0.4066667 0.444 0.4000000 0.3533333 0.5000000 0.4014286 0.2576769 0.75000
## 79  0.4555556 0.496 0.4727273 0.3900000 0.5969231 0.5214286 0.5706667 0.45250
## 80  0.4133333 0.316 0.6927273 0.4733333 0.4461538 0.4785714 0.5146667 0.46250
## 81  0.3888889 0.472 0.6927273 0.4350000 0.4584615 0.4471429 0.4213333 0.38250
## 82  0.5022222 0.452 0.3272727 0.5550000 0.3969231 0.3771429 0.5440000 0.37000
## 83  0.4622222 0.348 0.4836364 0.4883333 0.5076923 0.5028571 0.4186667 0.45375
## 84  0.3044444 0.320 0.3963636 0.4050000 0.3630769 0.3557143 0.2093333 0.39875
## 85  0.4644444 0.500 0.2677596 0.4216667 0.4938462 0.4528571 0.5186667 0.47750
## 86  0.6511111 0.506 0.4854545 0.4816667 0.4030769 0.3528571 0.5906667 0.41750
## 87  0.3044444 0.352 0.4545455 0.5383333 0.5369231 0.4971429 0.6560000 0.41750
## 88  0.6933333 0.696 0.4527273 0.4550000 0.3584615 0.5171429 0.4266667 0.28875
## 89  0.4088889 0.464 0.5000000 0.3733333 0.4907692 0.4542857 0.5720000 0.49875
## 90  0.4888889 0.572 0.5709091 0.4283333 0.4123077 0.4471429 0.4560000 0.51000
## 91  0.3422222 0.510 0.2495446 0.4583333 0.3969231 0.4557143 0.3040000 0.65125
## 92  0.4177778 0.544 0.4563636 0.4816667 0.3861538 0.4242857 0.4680000 0.47125
## 93  0.4888889 0.414 0.4290909 0.3983333 0.4030769 0.5100000 0.4640000 0.45625
## 94  0.4777778 0.492 0.3563636 0.4483333 0.3892308 0.5400000 0.2777036 0.39500
## 95  0.4000000 0.452 0.4163636 0.6116667 0.4676923 0.4357143 0.2857143 0.40125
## 96  0.2933333 0.750 0.4527273 0.2916667 0.4615385 0.4057143 0.4920000 0.35625
## 97  0.4044444 0.406 0.4145455 0.3783333 0.3000000 0.4700000 0.3546667 0.34000
## 98  0.2911111 0.414 0.4290909 0.5483333 0.5138462 0.5257143 0.3640000 0.41125
## 99  0.4466667 0.376 0.4418182 0.5000000 0.5138462 0.4442857 0.2643525 0.52250
## 100 0.6377778 0.464 0.4127273 0.3266667 0.5153846 0.3614286 0.4866667 0.21750
##           V17       V18       V19   V20
## 1   0.4823529 0.2944444 0.6536842 0.401
## 2   0.5258824 0.3411111 0.4084211 0.398
## 3   0.6917647 0.4311111 0.4368421 0.401
## 4   0.5788235 0.4988889 0.4905263 0.512
## 5   0.5541176 0.4122222 0.5021053 0.505
## 6   0.5364706 0.4222222 0.3621053 0.402
## 7   0.4317647 0.4466667 0.2528978 0.477
## 8   0.4964706 0.4933333 0.3557895 0.484
## 9   0.4623529 0.4900000 0.4031579 0.489
## 10  0.6517647 0.3200000 0.3957895 0.467
## 11  0.4094118 0.3500000 0.4494737 0.439
## 12  0.5388235 0.4755556 0.5926316 0.467
## 13  0.4047059 0.3311111 0.6273684 0.510
## 14  0.4682353 0.5100000 0.3957895 0.709
## 15  0.4094118 0.4544444 0.4547368 0.494
## 16  0.4552941 0.4277778 0.5947368 0.354
## 17  0.4164706 0.4700000 0.3894737 0.417
## 18  0.3482353 0.3600000 0.4968421 0.426
## 19  0.4764706 0.4611111 0.4915789 0.348
## 20  0.3576471 0.4244444 0.4905263 0.419
## 21  0.5235294 0.2577778 0.3000000 0.381
## 22  0.5117647 0.5955556 0.5189474 0.367
## 23  0.4694118 0.3588889 0.3242105 0.704
## 24  0.4470588 0.4122222 0.3052632 0.514
## 25  0.4400000 0.5455556 0.5094737 0.473
## 26  0.4541176 0.6444444 0.7168421 0.616
## 27  0.5223529 0.6544444 0.4536842 0.452
## 28  0.3752941 0.4622222 0.6705263 0.467
## 29  0.4729412 0.4544444 0.4315789 0.350
## 30  0.4635294 0.4466667 0.3484211 0.501
## 31  0.4082353 0.4255556 0.3147368 0.500
## 32  0.4682353 0.4033333 0.8105263 0.459
## 33  0.4647059 0.4944444 0.4578947 0.543
## 34  0.3329412 0.6877778 0.2507903 0.373
## 35  0.4152941 0.5155556 0.4631579 0.602
## 36  0.5400000 0.4155556 0.3147368 0.398
## 37  0.5035294 0.6466667 0.7502634 0.471
## 38  0.4258824 0.5177778 0.3557895 0.453
## 39  0.5341176 0.4655556 0.4421053 0.359
## 40  0.6929412 0.4222222 0.2989474 0.462
## 41  0.4423529 0.4577778 0.3494737 0.651
## 42  0.4423529 0.5066667 0.6357895 0.592
## 43  0.4588235 0.3766667 0.3305263 0.469
## 44  0.4141176 0.4466667 0.3084211 0.370
## 45  0.4611765 0.6477778 0.3989474 0.460
## 46  0.3423529 0.4044444 0.7502634 0.348
## 47  0.3564706 0.4011111 0.4021053 0.474
## 48  0.3988235 0.4766667 0.3705263 0.640
## 49  0.7247059 0.3733333 0.4515789 0.554
## 50  0.5235294 0.4055556 0.4684211 0.453
## 51  0.5082353 0.2811111 0.4578947 0.361
## 52  0.4317647 0.6977778 0.4852632 0.460
## 53  0.4576471 0.4711111 0.2936842 0.702
## 54  0.4341176 0.3555556 0.4410526 0.345
## 55  0.3400000 0.2544444 0.5894737 0.463
## 56  0.3058824 0.3377778 0.5052632 0.364
## 57  0.6482353 0.3288889 0.5936842 0.750
## 58  0.4964706 0.6922222 0.5926316 0.418
## 59  0.5047059 0.3911111 0.3536842 0.327
## 60  0.4317647 0.4355556 0.4347368 0.492
## 61  0.6988235 0.5955556 0.4042105 0.513
## 62  0.4282353 0.3822222 0.7042105 0.514
## 63  0.4905882 0.3500000 0.3652632 0.428
## 64  0.4717647 0.3011111 0.4547368 0.454
## 65  0.4682353 0.7500000 0.7105263 0.515
## 66  0.5117647 0.4933333 0.4052632 0.495
## 67  0.4494118 0.4966667 0.6800000 0.463
## 68  0.5035294 0.6988889 0.3673684 0.427
## 69  0.4505882 0.4044444 0.4747368 0.362
## 70  0.5388235 0.4766667 0.6421053 0.345
## 71  0.4905882 0.4633333 0.5021053 0.750
## 72  0.4788235 0.4777778 0.4589474 0.438
## 73  0.4835294 0.4922222 0.4073684 0.423
## 74  0.3564706 0.4422222 0.4221053 0.305
## 75  0.4870588 0.4622222 0.3968421 0.460
## 76  0.5094118 0.4266667 0.6800000 0.495
## 77  0.3400000 0.5033333 0.4568421 0.503
## 78  0.5000000 0.4055556 0.4557895 0.353
## 79  0.4611765 0.4600000 0.3568421 0.422
## 80  0.6882353 0.2522222 0.4789474 0.474
## 81  0.6447059 0.4544444 0.5073684 0.491
## 82  0.4729412 0.5411111 0.4105263 0.405
## 83  0.4905882 0.4577778 0.3526316 0.528
## 84  0.4200000 0.5133333 0.4600000 0.418
## 85  0.3835294 0.4966667 0.3568421 0.538
## 86  0.5223529 0.4611111 0.5021053 0.455
## 87  0.4000000 0.6011111 0.5336842 0.339
## 88  0.3517647 0.5411111 0.6484211 0.357
## 89  0.4647059 0.4000000 0.5000000 0.381
## 90  0.4729412 0.4500000 0.4010526 0.552
## 91  0.3600000 0.5011111 0.5147368 0.395
## 92  0.3882353 0.4355556 0.5052632 0.810
## 93  0.4835294 0.4988889 0.5421053 0.410
## 94  0.5200000 0.5944444 0.4705263 0.452
## 95  0.6388235 0.4900000 0.4052632 0.368
## 96  0.4047059 0.3766667 0.5168421 0.415
## 97  0.4882353 0.3511111 0.4368421 0.408
## 98  0.4741176 0.4055556 0.4947368 0.370
## 99  0.3258824 0.4600000 0.5515789 0.420
## 100 0.5317647 0.3200000 0.4042105 0.380
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{Model error rates means across dataset sizes:}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Model error rates means across dataset sizes:
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}{-}\textquotesingle{}}\NormalTok{, }\AttributeTok{times =} \DecValTok{20}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  - - - - - - - - - - - - - - - - - - - -
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lda.colMeans }\OtherTok{\textless{}{-}} \FunctionTok{colMeans}\NormalTok{(}\AttributeTok{x =}\NormalTok{ lda.err)}
\NormalTok{qda.colMeans }\OtherTok{\textless{}{-}} \FunctionTok{colMeans}\NormalTok{(}\AttributeTok{x =}\NormalTok{ qda.err)}
\NormalTok{prob4.err }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\AttributeTok{x =} \FunctionTok{cbind}\NormalTok{(}\StringTok{\textquotesingle{}lda.err.means\textquotesingle{}} \OtherTok{=}\NormalTok{ lda.colMeans, }\StringTok{\textquotesingle{}qda.err.means\textquotesingle{}} \OtherTok{=}\NormalTok{ qda.colMeans, }\StringTok{\textquotesingle{}size\textquotesingle{}} \OtherTok{=}\NormalTok{ n))}
\NormalTok{prob4.err}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    lda.err.means qda.err.means size
## 1      0.4736531     0.4710694  100
## 2      0.4731000     0.4703000  200
## 3      0.4745503     0.4537535  300
## 4      0.4843500     0.4599000  400
## 5      0.4892802     0.4599015  500
## 6      0.4979667     0.4736667  600
## 7      0.4936573     0.4366750  700
## 8      0.4832500     0.4443000  800
## 9      0.4999556     0.4425670  900
## 10     0.5015400     0.4664800 1000
## 11     0.5114500     0.4585963 1100
## 12     0.4894167     0.4429000 1200
## 13     0.4773039     0.4524043 1300
## 14     0.4907571     0.4460571 1400
## 15     0.5096900     0.4453649 1500
## 16     0.5171125     0.4522375 1600
## 17     0.5082353     0.4733176 1700
## 18     0.5001222     0.4566778 1800
## 19     0.5196632     0.4683474 1900
## 20     0.4994500     0.4628800 2000
\end{verbatim}

Question 4ii

QDA outperformed LDA by an increasing difference as the dataset size
increased. QDA is more susceptible to variance than LDA. However, this
is mended more and more as the amount of data points increases. For very
large datasets, QDA can be preferred over LDA. LDA suffers more from
bias and nonlinear truth. On the other hand, QDA relaxes the assumption
of equal variances between classes and fares better at discriminating
between classes that truly share nonlinear boundaries. The formula
plugged into the \texttt{expit} function to generate Y includes
exponential terms. This naturally gives edge to QDA, which can create
curved boundaries to accomodate nonlinearity from the formula.

Question 4iii

Please refer to dataframe generated by \texttt{prob4.err} in the most
recent cell to compare QDA and LDA error rates side by side along
increasing dataset size.

Question 4iv

As dataset size increases, QDA performs with increasing accuracy (or
decreasing error rate) relative to LDA. Having more data points also
makes QDA less susceptible to new data points added, noting that
variance is its weakness. A plot of error rates for both LDA and QDA can
illustrate this trend.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ prob4.err, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ size)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ lda.err.means, }\AttributeTok{color =} \StringTok{\textquotesingle{}green\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ qda.err.means, }\AttributeTok{color =} \StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}
    \AttributeTok{method =}\NormalTok{ lm,}
    \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{fullrange =} \ConstantTok{TRUE}\NormalTok{,}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ lda.err.means, }\AttributeTok{color =} \StringTok{\textquotesingle{}green\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}
    \AttributeTok{method =}\NormalTok{ lm,}
    \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{fullrange =} \ConstantTok{TRUE}\NormalTok{,}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ qda.err.means, }\AttributeTok{color =} \StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}QDA Error Rate\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}LDA Error Rate\textquotesingle{}}\NormalTok{), }\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}green\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using formula 'y ~ x'
## `geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{Chapter-2-HW_files/figure-latex/unnamed-chunk-21-1.pdf}

\end{document}
