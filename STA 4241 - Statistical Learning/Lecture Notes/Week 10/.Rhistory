which(pca.sum$importance['Cumulative Proportion', ] < 0.95 == FALSE)
which(as.vector(x = pca.sum$importance['Cumulative Proportion', ] < 0.95) == FALSE)
component.index <- which(as.vector(x = pca.sum$importance['Cumulative Proportion', ] < 0.95) == FALSE)[1]
component.index
pca.results$x
pca.results$x[, component.index]
dim(pca.results$x[, component.index])
typeof(pca.results$x[, component.index])
typeof(pca.results$x[, 1:component.index])
dim(pca.results$x[, 1:component.index])
x.train.pca <- pca.results$x[, 1:component.index]
x.train.pca
pca.results$x
pca.results$rotation
dim(pca.results$rotation)
pcr.95.model <- pls::pcr(formula = y.train ~ x.train, center = TRUE, scale = TRUE)
pcr.95.pred <- predict(object = pcr.95.model, newdata = x.test, ncomp = 125)
pcr.95.pred <- predict(object = pcr.95.model, newdata = x.test, ncomp = component.index)
Metrics::mse(actual = y.test, predicted = as.vector(x = pcr.95.pred))
# PLS regression with components chosen by CV
pls.model <- pls::plsr(y.train ~ x.train, center = TRUE)
# PLS regression with components chosen by CV
pls.model <- pls::plsr(y.train ~ x.train, center = TRUE, scale = TRUE)
# PLS regression with components chosen by CV
pls.model <- pls::plsr(y.train ~ x.train, center = TRUE, scale = TRUE, validation = 'xcvx)
# PLS regression with components chosen by CV
pls.model <- pls::plsr(y.train ~ x.train, center = TRUE, scale = TRUE, validation = 'xcvx')
# PLS regression with components chosen by CV
pls.model <- pls::plsr(y.train ~ x.train, center = TRUE, scale = TRUE, validation = 'CV')
summary(object = pls.model) # 125 components recommended by CV
summary(object = pls.model) # 125 components recommended by CV
Metrics::mse(actual = y.test, predicted = pls.pred)
Metrics::mse(actual = y.test, predicted = as.vector(x = pls.pred))
pls.pred <- predict(object = pls.model, newdata = x.test, ncomp = 125)
Metrics::mse(actual = y.test, predicted = as.vector(x = pls.pred))
Metrics::mse(actual = y.test, predicted = pls.pred)
pls.pred
Metrics::mse(actual = y.test, predicted = as.vector(x = pls.pred))
max(abs(x = x) - lambda) * as.numeric(x = x > 0)
# soft thresholding function
soft.thresh <- function(x, lambda) {
max(abs(x = x) - lambda) * as.numeric(x = x > 0)
}
soft.thresh(1, 1)
soft.thresh(2, 1)
soft.thresh(4, 1)
soft.thresh(-4, 1)
# soft thresholding function
soft.thresh <- function(x, lambda) {
max(abs(x = x) - lambda) * sign(x = x)
}
soft.thresh(-4, 1)
beta.tilde <- rep(x = 1, ncol(x = X))
lasso <- function(X, y, lambda, tol = 1e-3) {
beta.tilde <- rep(x = 1, ncol(x = X))
}
beta.tilde
# soft thresholding function
soft <- function(x, lambda) {
max(abs(x = x) - lambda) * sign(x = x)
}
x <- t(x = r_j) %*% X[, j] / (t(x = X[], j) %*% X[, j])
lasso.cd <- function(X, y, lambda, tol = 1e-3) {
p <- ncol(x = X)
beta.tilde <- rep(x = 1, times = p)
r <- Y -X %*% beta.tilde
for (j in 1:p) {
r_j <- r + X[, j] %*% beta.tilde[j]
x <- t(x = r_j) %*% X[, j] / (t(x = X[], j) %*% X[, j])
beta.plus <- soft(x = x, lambda = lambda)
beta.tilde[j] <- beta.plus
r <- r_j - X[, j] * beta.tilde[j]
}
}
lasso.cd(X = x, y = y, lambda = 0.5)
lasso.cd <- function(X, Y, lambda, tol = 1e-3) {
p <- ncol(x = X)
beta.tilde <- rep(x = 1, times = p)
r <- Y - X %*% beta.tilde
for (j in 1:p) {
r_j <- r + X[, j] %*% beta.tilde[j]
x <- t(x = r_j) %*% X[, j] / (t(x = X[], j) %*% X[, j])
beta.plus <- soft(x = x, lambda = lambda)
beta.tilde[j] <- beta.plus
r <- r_j - X[, j] * beta.tilde[j]
}
}
lasso.cd(X = x, y = y, lambda = 0.5)
lasso.cd(X = x, Y = y, lambda = 0.5)
p <- ncol(x = X)
lasso.cd(X = x, Y = y, lambda = 0.5)
lasso.cd <- function(X, Y, lambda, tol = 1e-3) {
p <- ncol(x = X)
beta.tilde <- rep(x = 1, times = p)
print(beta.tilde)
r <- Y - X %*% beta.tilde
print(r)
for (j in 1:p) {
r_j <- r + X[, j] %*% beta.tilde[j]
x <- t(x = r_j) %*% X[, j] / (t(x = X[], j) %*% X[, j])
beta.plus <- soft(x = x, lambda = lambda)
beta.tilde[j] <- beta.plus
r <- r_j - X[, j] * beta.tilde[j]
}
}
lasso.cd(X = x, Y = y, lambda = 0.5)
dim(X)
dim(x)
lasso.cd <- function(X, Y, lambda, tol = 1e-3) {
p <- ncol(x = X)
beta.tilde <- rep(x = 1, times = p)
# print(beta.tilde)
r <- Y - X %*% beta.tilde
# print(r)
for (j in 1:p) {
r_j <- r + X[, j] * beta.tilde[j]
x <- t(x = r_j) %*% X[, j] / (t(x = X[], j) %*% X[, j])
beta.plus <- soft(x = x, lambda = lambda)
beta.tilde[j] <- beta.plus
r <- r_j - X[, j] * beta.tilde[j]
}
}
lasso.cd(X = x, Y = y, lambda = 0.5)
lasso.cd <- function(X, Y, lambda, tol = 1e-3) {
p <- ncol(x = X)
beta.tilde <- rep(x = 1, times = p)
# print(beta.tilde)
r <- Y - X %*% beta.tilde
# print(r)
for (j in 1:p) {
r_j <- r + X[, j] * beta.tilde[j]
x <- t(x = r_j) %*% X[, j] / (t(x = X[, j]) %*% X[, j])
beta.plus <- soft(x = x, lambda = lambda)
beta.tilde[j] <- beta.plus
r <- r_j - X[, j] * beta.tilde[j]
}
}
lasso.cd(X = x, Y = y, lambda = 0.5)
lasso.cd <- function(X, Y, lambda, tol = 1e-3) {
p <- ncol(x = X)
beta.tilde <- rep(x = 1, times = p)
# print(beta.tilde)
r <- Y - X %*% beta.tilde
# print(r)
for (j in 1:p) {
r_j <- r + X[, j] * beta.tilde[j]
x <- t(x = r_j) %*% X[, j] / (t(x = X[, j]) %*% X[, j])
beta.plus <- soft(x = x, lambda = lambda)
beta.tilde[j] <- beta.plus
r <- r_j - X[, j] * beta.tilde[j]
}
beta.tilde
}
lasso.cd(X = x, Y = y, lambda = 0.5)
lasso.cd <- function(X, Y, lambda, tol = 1e-3) {
p <- ncol(x = X)
beta.tilde <- rep(x = 1, times = p)
# print(beta.tilde)
r <- Y - X %*% beta.tilde
# print(r)
tol.curr <- 1000
beta.tilde.old <- beta.tilde
while (tol.curr > tol) {
for (j in 1:p) {
r_j <- r + X[, j] * beta.tilde[j]
x <- t(x = r_j) %*% X[, j] / (t(x = X[, j]) %*% X[, j])
beta.plus <- soft(x = x, lambda = lambda)
beta.tilde[j] <- beta.plus
r <- r_j - X[, j] * beta.tilde[j]
}
tol.curr <- crossprod(x = beta.tilde - beta.tilde.old)
print(tol.curr)
}
beta.tilde
}
lasso.cd(X = x, Y = y, lambda = 0.5)
lasso.cd(X = x, Y = y, lambda = 0.5)
beta.delta <- 1000
lasso.cd <- function(X, Y, lambda, tol = 1e-3) {
p <- ncol(x = X)
beta.tilde <- rep(x = 1, times = p)
# print(beta.tilde)
r <- Y - X %*% beta.tilde
# print(r)
beta.delta <- 1000
beta.tilde.old <- beta.tilde
while (beta.delta > tol) {
for (j in 1:p) {
r_j <- r + X[, j] * beta.tilde[j]
x <- t(x = r_j) %*% X[, j] / (t(x = X[, j]) %*% X[, j])
beta.plus <- soft(x = x, lambda = lambda)
beta.tilde[j] <- beta.plus
r <- r_j - X[, j] * beta.tilde[j]
}
# change in estimated beta in this iteration compared to previous iteration
beta.delta <- abs(x = beta.tilde - beta.tilde.old)
print(tol.curr)
}
beta.tilde
}
lasso.cd(X = x, Y = y, lambda = 0.5)
lasso.cd <- function(X, Y, lambda, tol = 1e-3) {
p <- ncol(x = X)
beta.tilde <- rep(x = 1, times = p)
# print(beta.tilde)
r <- Y - X %*% beta.tilde
# print(r)
beta.delta <- 1000
beta.tilde.old <- beta.tilde
while (beta.delta > tol) {
for (j in 1:p) {
r_j <- r + X[, j] * beta.tilde[j]
x <- t(x = r_j) %*% X[, j] / (t(x = X[, j]) %*% X[, j])
beta.plus <- soft(x = x, lambda = lambda)
beta.tilde[j] <- beta.plus
r <- r_j - X[, j] * beta.tilde[j]
}
# change in estimated beta in this iteration compared to previous iteration
beta.delta <- abs(x = beta.tilde - beta.tilde.old)
print(beta.delta)
}
beta.tilde
}
lasso.cd(X = x, Y = y, lambda = 0.5)
lasso.cd <- function(X, Y, lambda, tol = 1e-3) {
p <- ncol(x = X)
beta.tilde <- rep(x = 1, times = p)
# print(beta.tilde)
r <- Y - X %*% beta.tilde
# print(r)
beta.delta <- 1000
beta.tilde.old <- beta.tilde
while (beta.delta > tol) {
for (j in 1:p) {
r_j <- r + X[, j] * beta.tilde[j]
x <- t(x = r_j) %*% X[, j] / (t(x = X[, j]) %*% X[, j])
beta.plus <- soft(x = x, lambda = lambda)
beta.tilde[j] <- beta.plus
r <- r_j - X[, j] * beta.tilde[j]
}
# change in estimated beta in this iteration compared to previous iteration
beta.delta <- sum(abs(x = beta.tilde - beta.tilde.old))
print(beta.delta)
}
beta.tilde
}
lasso.cd <- function(X, Y, lambda, tol = 1e-3) {
p <- ncol(x = X)
beta.tilde <- rep(x = 1, times = p)
# print(beta.tilde)
r <- Y - X %*% beta.tilde
# print(r)
beta.delta <- 1.0
beta.tilde.old <- beta.tilde
while (beta.delta > tol) {
for (j in 1:p) {
r_j <- r + X[, j] * beta.tilde[j]
x <- t(x = r_j) %*% X[, j] / (t(x = X[, j]) %*% X[, j])
beta.plus <- soft(x = x, lambda = lambda)
beta.tilde[j] <- beta.plus
r <- r_j - X[, j] * beta.tilde[j]
}
# change in estimated beta in this iteration compared to previous iteration
beta.delta <- sum(abs(x = beta.tilde - beta.tilde.old))
print(beta.delta)
}
beta.tilde
}
lasso.cd(X = x, Y = y, lambda = 0.5)
# soft thresholding function
soft <- function(x, lambda) {
max(abs(x = x) - lambda) * sign(x = x)
}
lasso.cd <- function(X, Y, lambda, tol = 1e-3) {
p <- ncol(x = X)
beta.tilde <- rep(x = 1, times = p)
# print(beta.tilde)
r <- Y - X %*% beta.tilde
# print(r)
beta.delta <- 1.0
beta.tilde.old <- beta.tilde
while (beta.delta > tol) {
for (j in 1:p) {
r_j <- r + X[, j] * beta.tilde[j]
x <- t(x = r_j) %*% X[, j] / (t(x = X[, j]) %*% X[, j])
beta.plus <- soft(x = x, lambda = lambda)
beta.tilde[j] <- beta.plus
r <- r_j - X[, j] * beta.tilde[j]
}
# change in estimated beta in this iteration compared to previous iteration
beta.delta <- sum(abs(x = beta.tilde - beta.tilde.old))
print(beta.delta)
}
beta.tilde
}
lasso.cd(X = x, Y = y, lambda = 0.5, tol = 50)
lasso.cd(X = x, Y = y, lambda = 0.5, tol = 47)
lasso.cd(X = x, Y = y, lambda = 0.5, tol = 40)
lasso.cd(X = x, Y = y, lambda = 0.5, tol = 4)
# soft thresholding function
soft <- function(x, lambda) {
max(abs(x = x) - lambda) * sign(x = x)
}
lasso.cd <- function(X, Y, lambda, tol = 1e-3) {
p <- ncol(x = X)
beta.tilde <- rep(x = 1, times = p)
# print(beta.tilde)
r <- Y - X %*% beta.tilde
# print(r)
beta.delta <- 1e3
beta.tilde.old <- beta.tilde
while (beta.delta > tol) {
for (j in 1:p) {
r_j <- r + X[, j] * beta.tilde[j]
x <- t(x = r_j) %*% X[, j] / (t(x = X[, j]) %*% X[, j])
beta.plus <- soft(x = x, lambda = lambda)
beta.tilde[j] <- beta.plus
r <- r_j - X[, j] * beta.tilde[j]
}
# change in estimated beta in this iteration compared to previous iteration
beta.delta <- sum(abs(x = beta.tilde - beta.tilde.old))
print(beta.delta)
}
beta.tilde
}
lasso.cd(X = x, Y = y, lambda = 0.5, tol = 50)
glmnet::glmnet(x = x, y = y, intercept = FALSE)
glmnet::glmnet(x = x, y = y, intercept = FALSE, lambda = 0.5)
glmnet.model <- glmnet::glmnet(x = x, y = y, intercept = FALSE, lambda = 0.5)
summary(glmnet.model)
glmnet.model$beta
glmnet.model
coef(object = glmnet.model)
lasso.cd(X = x, Y = y, lambda = 0.5, tol = 10)
# soft thresholding function
soft <- function(x, lambda) {
max(abs(x = x) - lambda) * sign(x = x)
}
lasso.cd <- function(X, Y, lambda, tol = 1e-3) {
p <- ncol(x = X)
beta.tilde <- rep(x = 1, times = p)
# print(beta.tilde)
r <- Y - X %*% beta.tilde
# print(r)
beta.delta <- 1e3
beta.tilde.old <- beta.tilde
while (beta.delta > tol) {
for (j in 1:p) {
r_j <- r + X[, j] * beta.tilde[j]
x <- t(x = r_j) %*% X[, j] / (t(x = X[, j]) %*% X[, j])
beta.plus <- soft(x = x, lambda = lambda)
beta.tilde[j] <- beta.plus
r <- r_j - X[, j] * beta.tilde[j]
}
# change in estimated beta in this iteration compared to previous iteration
beta.delta <- sum(abs(x = beta.tilde - beta.tilde.old))
# print(beta.delta)
}
beta.tilde
}
lasso.cd(X = x, Y = y, lambda = 0.5, tol = 10)
lasso.cd(X = x, Y = y, lambda = 0.5, tol = 45)
stats::prcomp(formula = y ~ x)
stats::prcomp(formula = y ~ x)
stats::prcomp(y ~ x)
# ElasticNet regression
en.cv <- glmnet::cv.glmnet(x = x.train, y = y.train, alpha = 0.5, lambda = 1:1000)
en.model <- glmnet::glmnet(x = x.train, y = y.train, alpha = 0.5, lambda = en.cv$lambda.min)
en.pred <- glmnet::predict.glmnet(object = en.model, newx = x.test, type = 'response')
Metrics::mse(actual = y.test, predicted = en.pred)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(
error = TRUE, # do not interrupt in case of errors
)
knitr::opts_chunk$set(
echo = TRUE,
error = TRUE, # do not interrupt in case of errors
)
knitr::opts_chunk$set(
echo = TRUE,
error = TRUE # do not interrupt in case of errors
)
cor.matrix <- stats::cor(x = x, y = x)
stats::heatmap(x = cor.matrix)
prcomp(x = x)
pca.obj <- prcomp(x = x)
fviz_eig(pca.obj)
packages <- c('factoextra', 'glmnet', 'Metrics', 'pls')
install.packages(packages)
install.packages(packages)
lapply(packages, library, character.only = TRUE)
# reading in data
data <- read.csv(file = './Data/Problem1.csv', header = TRUE)
x <- as.matrix(x = data[,1:52])
y <- as.numeric(x = data[,53])
load(file = './Data/Problem2train.dat')
load(file = 'Data/Problem2test.dat')
x.train <- as.matrix(x = dataTrain[, 1:204])
y.train <- as.matrix(x = dataTrain[, 205])
x.test <- as.matrix(x = dataTest[, 1:204])
y.test <- as.matrix(x = dataTest[, 205])
lapply(packages, library, character.only = TRUE)
factoextra::fviz_eig(pca.obj)
pca.obj <- prcomp(x = x)
factoextra::fviz_eig(pca.obj)
cor.matrix <- stats::cor(x = x, y = x)
stats::heatmap(x = cor.matrix)
# reading in data
data <- read.csv(file = './Data/Problem1.csv', header = TRUE)
x <- as.matrix(x = data[,1:52])
y <- as.numeric(x = data[,53])
load(file = './Data/Problem2train.dat')
load(file = 'Data/Problem2test.dat')
x.train <- as.matrix(x = dataTrain[, 1:204])
y.train <- as.matrix(x = dataTrain[, 205])
x.test <- as.matrix(x = dataTest[, 1:204])
y.test <- as.matrix(x = dataTest[, 205])
# LASSO regression
lasso.cv <- glmnet::cv.glmnet(x = x.train, y = y.train, alpha = 0, lambda = 1:1000)
lasso.model <- glmnet::glmnet(x = x.train, y = y.train, alpha = 0, lambda = lasso.cv$lambda.min)
lasso.pred <- glmnet::predict.glmnet(object = lasso.model, newx = x.test, type = 'response')
Metrics::mse(actual = y.test, predicted = lasso.pred)
# ridge regression
ridge.cv <- glmnet::cv.glmnet(x = x.train, y = y.train, alpha = 1, lambda = seq(0.0, 1.0, by = 0.0001))
ridge.model <- glmnet::glmnet(x = x.train, y = y.train, alpha = 1, lambda = ridge.cv$lambda.min)
ridge.pred <- glmnet::predict.glmnet(object = ridge.model, newx = x.test, type = 'response')
Metrics::mse(actual = y.test, predicted = ridge.pred)
pcr.model$residuals
# PCA regression with no. of components chosen by CV
pcr.model <- pls::pcr(formula = y.train ~ x.train, center = TRUE, scale = TRUE, validation = 'CV')
summary(object = pcr.model) # 125 components recommended by CV
pcr.pred <- predict(object = pcr.model, newdata = x.test, ncomp = 125)
Metrics::mse(actual = y.test, predicted = as.vector(x = pcr.pred))
# PCA regression with minimum no. of components to contain 95% explained variance
pca.results <- stats::prcomp(x = x.train, center = TRUE, scale. = TRUE)
stats::heatmap(x = stats::cor(x = pca.results$x, y = pca.results$x))
pca.sum <- summary(object = pca.results)
component.index <- which(as.vector(x = pca.sum$importance['Cumulative Proportion', ] < 0.95) == FALSE)[1] # number of top components with cumulative explained variance >= 0.95
pcr.95.model <- pls::pcr(formula = y.train ~ x.train, center = TRUE, scale = TRUE)
pcr.95.pred <- predict(object = pcr.95.model, newdata = x.test, ncomp = component.index)
Metrics::mse(actual = y.test, predicted = as.vector(x = pcr.95.pred))
# PLS regression with components chosen by CV
pls.model <- pls::plsr(y.train ~ x.train, center = TRUE, scale = TRUE, validation = 'CV')
summary(object = pls.model) # 125 components recommended by CV
pls.pred <- predict(object = pls.model, newdata = x.test, ncomp = 125)
Metrics::mse(actual = y.test, predicted = as.vector(x = pls.pred))
# ElasticNet regression
en.cv <- glmnet::cv.glmnet(x = x.train, y = y.train, alpha = 0.5, lambda = 1:1000)
en.model <- glmnet::glmnet(x = x.train, y = y.train, alpha = 0.5, lambda = en.cv$lambda.min)
en.pred <- glmnet::predict.glmnet(object = en.model, newx = x.test, type = 'response')
Metrics::mse(actual = y.test, predicted = en.pred)
# LASSO regression
lasso.cv <- glmnet::cv.glmnet(x = x.train, y = y.train, alpha = 0, lambda = 1:1000)
lasso.model <- glmnet::glmnet(x = x.train, y = y.train, alpha = 0, lambda = lasso.cv$lambda.min)
lasso.pred <- glmnet::predict.glmnet(object = lasso.model, newx = x.test, type = 'response')
Metrics::mse(actual = y.test, predicted = lasso.pred)
# ridge regression
ridge.cv <- glmnet::cv.glmnet(x = x.train, y = y.train, alpha = 1, lambda = seq(0.0, 1.0, by = 0.0001))
ridge.model <- glmnet::glmnet(x = x.train, y = y.train, alpha = 1, lambda = ridge.cv$lambda.min)
ridge.pred <- glmnet::predict.glmnet(object = ridge.model, newx = x.test, type = 'response')
Metrics::mse(actual = y.test, predicted = ridge.pred)
pcr.model$residuals
# PCA regression with no. of components chosen by CV
pcr.model <- pls::pcr(formula = y.train ~ x.train, center = TRUE, scale = TRUE, validation = 'CV')
# summary(object = pcr.model) # 125 components recommended by CV
pcr.pred <- predict(object = pcr.model, newdata = x.test, ncomp = 125)
Metrics::mse(actual = y.test, predicted = as.vector(x = pcr.pred))
# PCA regression with minimum no. of components to contain 95% explained variance
pca.results <- stats::prcomp(x = x.train, center = TRUE, scale. = TRUE)
stats::heatmap(x = stats::cor(x = pca.results$x, y = pca.results$x))
pca.sum <- summary(object = pca.results)
component.index <- which(as.vector(x = pca.sum$importance['Cumulative Proportion', ] < 0.95) == FALSE)[1] # number of top components with cumulative explained variance >= 0.95
pcr.95.model <- pls::pcr(formula = y.train ~ x.train, center = TRUE, scale = TRUE)
pcr.95.pred <- predict(object = pcr.95.model, newdata = x.test, ncomp = component.index)
Metrics::mse(actual = y.test, predicted = as.vector(x = pcr.95.pred))
# PLS regression with components chosen by CV
pls.model <- pls::plsr(y.train ~ x.train, center = TRUE, scale = TRUE, validation = 'CV')
# summary(object = pls.model) # 125 components recommended by CV
pls.pred <- predict(object = pls.model, newdata = x.test, ncomp = 125)
Metrics::mse(actual = y.test, predicted = as.vector(x = pls.pred))
# ElasticNet regression
en.cv <- glmnet::cv.glmnet(x = x.train, y = y.train, alpha = 0.5, lambda = 1:1000)
en.model <- glmnet::glmnet(x = x.train, y = y.train, alpha = 0.5, lambda = en.cv$lambda.min)
en.pred <- glmnet::predict.glmnet(object = en.model, newx = x.test, type = 'response')
Metrics::mse(actual = y.test, predicted = en.pred)
# soft thresholding function
soft <- function(x, lambda) {
max(abs(x = x) - lambda) * sign(x = x)
}
lasso.cd <- function(X, Y, lambda, tol = 1e-3) {
p <- ncol(x = X)
beta.tilde <- rep(x = 1, times = p)
# print(beta.tilde)
r <- Y - X %*% beta.tilde
# print(r)
beta.delta <- 1e3
beta.tilde.old <- beta.tilde
while (beta.delta > tol) {
for (j in 1:p) {
r_j <- r + X[, j] * beta.tilde[j]
x <- t(x = r_j) %*% X[, j] / (t(x = X[, j]) %*% X[, j])
beta.plus <- soft(x = x, lambda = lambda)
beta.tilde[j] <- beta.plus
r <- r_j - X[, j] * beta.tilde[j]
}
# change in estimated beta in this iteration compared to previous iteration
beta.delta <- sum(abs(x = beta.tilde - beta.tilde.old))
# print(beta.delta)
}
beta.tilde
}
lasso.cd(X = x, Y = y, lambda = 0.5, tol = 45)
